{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd \"C:\\Git\\BI0730\"\n",
    "import os\n",
    "import sys\n",
    "import duckdb\n",
    "import re\n",
    "sys.path.append('../../')\n",
    "sys.path.append(rf\"C:\\Git\\BI0730\")\n",
    "from libs.geral.utils import * \n",
    "from libs.denodo import conexaoDenodo as cnn\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = None\n",
    "user = os.getlogin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8293f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicador_nova_principalidade_historico\n",
    "query = '''SELECT * FROM indicador_nova_principalidade_historico'''\n",
    "principalidade = cnn.baixar_consulta_manual(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "path = rf\"C:\\Users\\{user}\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\\cia_pcp_indicador_principalidade_historico\\\\2025*.parquet\"\n",
    "query = rf'''\n",
    "    CREATE TABLE dados AS\n",
    "    SELECT * FROM read_parquet('{path}')\n",
    "    \n",
    "   \n",
    "'''\n",
    "\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = f''' \n",
    "      WITH tratados AS \n",
    "        (\n",
    "            SELECT *,\n",
    "            ROW_NUMBER() OVER (PARTITION BY cpf_cnpj ORDER BY ano_mes DESC) AS rn,\n",
    "            pontos_principalidade - LAG(pontos_principalidade)  OVER (PARTITION BY cpf_cnpj ORDER BY ano_mes ASC) AS var_pontos\n",
    "\n",
    "            FROM dados\n",
    "            QUALIFY rn <= 4\n",
    "            ORDER BY ano_Mes\n",
    "        ),\n",
    "        \n",
    "        marcacoes AS (\n",
    "            SELECT  \n",
    "            *,\n",
    "            CASE WHEN var_pontos < 0 THEN 1 ELSE 0 END AS queda_flag\n",
    "            FROM tratados \n",
    "            WHERE rn <= 3\n",
    "            \n",
    "        ),\n",
    "        checagem AS (\n",
    "            SELECT *,\n",
    "            SUM(queda_flag) OVER (PARTITION BY cpf_cnpj ORDER BY ano_mes) AS soma_quedas\n",
    "            FROM marcacoes\n",
    "        )\n",
    "        \n",
    "        SELECT  * FROM checagem WHERE soma_quedas >= 3 and ano_mes = (SELECT MAX(ano_mes) FROM checagem)\n",
    "        --SELECT  * FROM checagem WHERE cpf_cnpj = '00362741948'\n",
    "        \n",
    "\n",
    "        '''\n",
    "con.sql(query).to_df()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saida = rf\"C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira\\associados_queda_principalidade.parquet\"\n",
    "tmp = saida + \".tmp\"\n",
    "\n",
    "# Executar a CTE (usa a variável 'query' já definida acima) e materializar\n",
    "df_quedas = con.sql(query).fetchdf()\n",
    "\n",
    "# Validar presença das colunas calculadas\n",
    "esperadas_calc = ['var_pontos','queda_flag','soma_quedas']\n",
    "faltan = [c for c in esperadas_calc if c not in df_quedas.columns]\n",
    "if faltan:\n",
    "    raise RuntimeError(f\"Faltan columnas calculadas: {faltan}\")\n",
    "\n",
    "# Guardar atomicamente\n",
    "df_quedas.to_parquet(tmp, index=False)\n",
    "os.replace(tmp, saida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(rf\"C:\\\\Users\\\\{USER}\\\\Sicredi\\\\TimeBI_0730 - Documentos\\\\01_Rotineiros\\\\33_GiroCarteira\\\\giro_de_carteira\\\\associados_queda_principalidade.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assodia =  carregar_associados_completo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assodia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e22317",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste prévio de ano_atual para evitar NameError e definir marca de data (AAAAMM)\n",
    "from datetime import date\n",
    "\n",
    "# Garante que ANO exista\n",
    "try:\n",
    "    ANO\n",
    "except NameError:\n",
    "    ANO = date.today().year\n",
    "\n",
    "ano_atual = ANO\n",
    "\n",
    "# Marca de data para uso em nomes de arquivos (AAAAMM)\n",
    "data = date.today().strftime('%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANO = 2025\n",
    "USER = os.getlogin()\n",
    "\n",
    "def localizar_arquivo_principalidade(ano:int, base_parquet_dir:str,\n",
    "                                     nome_base:str='cia_pcp_indicador_principalidade_historico'):\n",
    "    \"\"\"Retorna (caminho_escolhido, lista_arquivos_avaliados).\n",
    "    Regras:\n",
    "      1. Se existir diretório <base>/<nome_base>/ com vários parquets:\n",
    "         - Considera arquivos *.parquet que contenham nome_base.\n",
    "         - Extrai prefixo inicial AAAAMM antes do primeiro '_'. \n",
    "         - Primeiro tenta só os do ano solicitado (AAAAMM começa com str(ano)).\n",
    "         - Se não houver para o ano, considera todos e pega o maior AAAAMM.\n",
    "      2. Se não houver diretório, tenta arquivo único <base>/<nome_base>.parquet\n",
    "    \"\"\"\n",
    "    dir_candidatos = os.path.join(base_parquet_dir, nome_base)\n",
    "    avaliados = []\n",
    "    selecionaveis = []\n",
    "    if os.path.isdir(dir_candidatos):\n",
    "        for f in os.listdir(dir_candidatos):\n",
    "            if f.lower().endswith('.parquet') and nome_base in f:\n",
    "                avaliados.append(f)\n",
    "                m = re.match(r'^(\\d{6})_', f)\n",
    "                if m:\n",
    "                    aaaamm = m.group(1)\n",
    "                    try:\n",
    "                        selecionaveis.append((int(aaaamm), f))\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "        # Filtrar por ano solicitado\n",
    "        ano_prefix = str(ano)\n",
    "        por_ano = [t for t in selecionaveis if str(t[0]).startswith(ano_prefix)]\n",
    "        if por_ano:\n",
    "            por_ano.sort()\n",
    "            escolhido = por_ano[-1][1]\n",
    "        elif selecionaveis:\n",
    "            selecionaveis.sort()\n",
    "            escolhido = selecionaveis[-1][1]\n",
    "        else:\n",
    "            # Nenhum com padrão AAAAMM_ => fallback arquivo único\n",
    "            escolhido = None\n",
    "        if escolhido:\n",
    "            return os.path.join(dir_candidatos, escolhido), avaliados\n",
    "        else:\n",
    "            # fallback arquivo único na raiz\n",
    "            return os.path.join(base_parquet_dir, f'{nome_base}.parquet'), avaliados\n",
    "    else:\n",
    "        # Diretório não existe => assumir arquivo único\n",
    "        return os.path.join(base_parquet_dir, f'{nome_base}.parquet'), avaliados\n",
    "\n",
    "principalidade_path, lista_avaliados = localizar_arquivo_principalidade(ANO, PATH_BASES_PARQUET)\n",
    "print('[principalidade] candidatos avaliados:', lista_avaliados[:8], '...' if len(lista_avaliados) > 8 else '')\n",
    "print('[principalidade] selecionado:', principalidade_path)\n",
    "\n",
    "RUTAS = {\n",
    "    'inadimplentes': os.path.join(PATH_BASES_PARQUET, 'base_inadimplentes.parquet'),\n",
    "    'associados_dir': os.path.join(PATH_BASES_PARQUET, 'associados_total_diario'),\n",
    "    'saida_dir': fr'C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira',\n",
    "    'saida_dir_giro': fr'C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira',\n",
    "    'principalidade': principalidade_path\n",
    "}\n",
    "\n",
    "os.makedirs(RUTAS['saida_dir_giro'], exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'ano': ano_atual,\n",
    "    'pasta_saida_duckdb': rf\"C:\\\\Users\\\\{USER}\\\\Sicredi\\\\TimeBI_0730 - Documentos\\\\01_Rotineiros\\\\33_GiroCarteira\\\\giro_de_carteira\\\\associados_queda_principalidade.parquet\",\n",
    "    'pasta_isa_historico': rf\"C:\\Users\\carola_luco\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira\",\n",
    "    'arquivo_saida': rf\"C:\\Users\\carola_luco\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira_2025.parquet\",\n",
    "    'filtros_risco_bbm': [\"BAIXÍSSIMO\", \"BAIXO 1\", \"BAIXO 2\", \"MÉDIO 1\", \"MÉDIO 2\"],\n",
    "    'dias_sem_movimentacao': (20, 45)\n",
    "}\n",
    "PARAMS = {\n",
    "    'filtros_risco_bbm': [\"BAIXÍSSIMO\", \"BAIXO 1\", \"BAIXO 2\", \"MÉDIO 1\", \"MÉDIO 2\"],\n",
    "    'dias_sem_movimentacao': (20, 45),\n",
    "    'principalidade': ['sow']\n",
    "}\n",
    "\n",
    "for k, v in RUTAS.items():\n",
    "    print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a827c",
   "metadata": {},
   "source": [
    "el archivo que sustenta es el archivo de principalidade, los merge serán realizados a este archivo para poder filtrar por associados sin inadimplencia y que no tengan uso en el campo 'mobi_transacionou_30d' ó ultimo movimento entre 20 - 45 días\n",
    "\n",
    "serán generados 2 parquet uno con toda la información y un dataframe construido con la siguientes campos e informaciones\n",
    "\n",
    "{  \n",
    "  \"titulo\": \"string\",    \n",
    "  \"usuarioResponsavel\": \"string\", ----> GESTOR\n",
    "  \"usuarioSolicitante\": \"string\", -----> \n",
    "  \"numeroAgencia\": 0, -----> cod_agencia\n",
    "  \"descricao\": \"string\",  --------> descricao texto com detalhes e indices do associado + o texto da oferta de produtos a entregar\n",
    "  \"statusChamado\": \"RASCUNHO\", \n",
    "  \"departamentoId\": 0,\n",
    "  \"nomeDepartamento\": \"string\", --------->  _nom_agencia\n",
    "  \"dataNecessidade\": \"2025-08-14T11:45:45.991Z\",\n",
    "  \"categoriaChamado\": {\n",
    "    \"categoriaChamadoId\": 0,  ---- > 1 ou 2\n",
    "    \"nomeCategoria\": \"string\", ------> \n",
    "    \"descricaoCategoria\": \"string\", \n",
    "    \"dataCriacao\": \"2025-08-14\",\n",
    "    \"departamentoId\": 0}\n",
    "}\n",
    "\n",
    "Titulo () + nome associado + cpf \n",
    "\n",
    "Descrição:\n",
    "PF OU PJ\n",
    "ISA atual \n",
    "ISA 3M\n",
    "MC TOTAL ATUAL\n",
    "MC 6M\n",
    "INDICES PRINCIPALIDADE\n",
    "\n",
    "OFERTA : TEXTO conformado por  categoria + descriçao categoria + produto  1 texto : \n",
    "\n",
    "  Oferta Acesso  ----->  Categoria 1 \n",
    "(descriçao categoria) Ofertar Produtos/Serviços Básicos : {} ou [] {add. Produto}-------> Titulo + nome associado + cpf  \n",
    ". Chave PIX ativa \n",
    "· Débito automático\n",
    "· Cartão de débito\n",
    "· Cartão de crédito\n",
    ". Canais digitais\n",
    ". Credenciamento ativo\n",
    "· Cobrança\n",
    ". Folha de Pagamento\n",
    ". Domicílio\n",
    ". Open Finance (receptor)\n",
    "\n",
    " Oferta Fluxo de Caixa   ----->  Categoria 2 \n",
    "if % produto in range \n",
    "(descriçao categoria) Ofertar Movimentações : [] ou {} {add. Crédito &| cartão}-------> Titulo + nome associado + cpf\n",
    ". Somatório do total de movimentações:\n",
    "- Receber (Cash-in)\n",
    "- Pagar (Cash-out)\n",
    "\n",
    "Crédito - cartão\n",
    ". SOW (Share of Wallet)\n",
    "referente à cartão\n",
    "\n",
    "Credito - outros\n",
    "SOW (Share of Wallet)\n",
    "referente à créditos\n",
    "\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "el campo categoria puede tener dos contenidos acesso y fluxo de caixa será utilizado como una flag de la condicion de los productos que no tiene \n",
    "\n",
    "el campo producto deberá conter todos los productos que no tiene haciendo un array para luego ser utilizado en la construccion del texto del campo descripcion tendremos 2 campos productos categoria I y productos categoria II\n",
    "\n",
    "\n",
    "la categoria fluxo de caixa tiene percentuais que deberán ser utilizados con rangos para saber si existe posibilidad de un upgrade, se mantiene o que producto ofertar, esto deberá ser gravado en el campo produto ahora colocaremos solo los porcentages que existem en el sow, indicando a que corresponden, para esso utilizaremos el titulo del campo e la cifra contenido en el valor del campo\n",
    "\n",
    "al final de todo concatenaremos todos los textos de los campos \n",
    "\n",
    "\n",
    "Titulo () + nome associado + cpf \n",
    "\n",
    "Descrição:\n",
    "PF OU PJ\n",
    "ISA atual \n",
    "ISA 3M\n",
    "MC TOTAL ATUAL\n",
    "MC 6M\n",
    "INDICES PRINCIPALIDADE\n",
    "\n",
    "OFERTA : TEXTO conformado por  categoria + descriçao categoria + produto  1 texto \n",
    "\n",
    "\n",
    "y crearemos el campo Descripcion \n",
    "\n",
    "\n",
    "deberá ser generado un codigo que implemente filtros de indimplencia, risco bbm y que la ultima movimentacion dejó de entre 20 y 45\n",
    "será construida el campo descripcion con las caracteristia de los datos existentes en los parquets utilizados según el modelo, si el associado NO tiene el producto será addicionado como producto en la descripción con el titulo del campo como catgroria \n",
    "\n",
    "\n",
    "\n",
    "las columnas para este último parquet o dataframe serán \n",
    "{  \n",
    "  \"titulo\": \"string\",    \n",
    "  \"usuarioResponsavel\": \"string\", ----> GESTOR\n",
    "  \"usuarioSolicitante\": \"string\", -----> \n",
    "  \"numeroAgencia\": 0, -----> cod_agencia\n",
    "  \"descricao\": \"string\",  --------> descricao texto com detalhes e indices do associado + o texto da oferta de produtos a entregar\n",
    "  \"statusChamado\": \"RASCUNHO\", \n",
    "  \"departamentoId\": 0,\n",
    "  \"nomeDepartamento\": \"string\", --------->  _nom_agencia\n",
    "  \"dataNecessidade\": \"2025-08-14T11:45:45.991Z\", ----> exemplo\n",
    "  \"categoriaChamado\": {\n",
    "    \"categoriaChamadoId\": 0,  ---- > 1 ou 2\n",
    "    \"nomeCategoria\": \"string\", ------> \n",
    "    \"descricaoCategoria\": \"string\", \n",
    "    \"dataCriacao\": \"2025-08-14\",\n",
    "    \"departamentoId\": 0}\n",
    "}\n",
    "\n",
    "\n",
    "las bases son conformadas mensualmente por lo que utilizaremos una data base de mes pero dividremos el total en 4 partes, para que sea usado por semana, colocando los mas antiguos para ser contactados la primera semana y así sucesivamente hasta nuevamente ser generado un nuevo archivo para eso generaremos la informacion en el campo dataNecessidade\n",
    "\n",
    "\n",
    "\n",
    "del dashboard associados total dashboard\n",
    "utilizaremos cpf_cnpj e nome_agencia,mc_total_1,gestor\n",
    "C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\\associados_totais_tratados_dashboard.parquet\n",
    "\n",
    "de isa historico\n",
    "\n",
    "C:\\Users\\carola_luco\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira\\isa_historico_analise_20250814.parquet utilizaremos \n",
    "gestor,\n",
    "isa_status\n",
    "isa_media\n",
    "isa_202503\n",
    "isa_202504\n",
    "isa_202505\n",
    "isa_202506  siempre utilizando los ultimos 3 \n",
    "\n",
    "de acuerdo a todo lo dicho necesito que generes un ipynb de debug con las rutas y las condiciones generadas,  utilizaremos pandas numpy re os sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas arquivo principalidade\n",
    "colunas_principalidade = ['ano_mes', 'cpf_cnpj', 'segmento', 'cod_cooperativa', 'cod_central',\n",
    "       'cod_agencia', 'cod_carteira', 'status_associado', 'porte_padrao',\n",
    "       'meses_desde_associacao', 'assoc_desde', 'nivel_risco', \n",
    "       'vlr_capital_social',  'pix_trans_30d', 'cad_pix',\n",
    "       'sum_titulo_investimento', 'mobi_transacionou_30d', 'sum_previdencia',\n",
    "       'sum_investimentos', 'total_valor_cartao_mercado_scr',\n",
    "       'total_valor_cartao_sicredi', 'total_valor_outros_sicredi',  'sow_cartao',\n",
    "       'possui_cartao_credito', 'debito_conta_ativo', 'possui_domicilio',\n",
    "       'possui_cartao_debito', 'possui_folha_pagamento', 'possui_cobranca',\n",
    "       'possui_adquirencia', 'transacao_app', 'flg_ativo', 'flg_novo_assoc',  'cad_pix_ativo',\n",
    "       'ativou_open_finance', 'cash_in', 'cash_out', 'cash_total_fator',\n",
    "       'cash_total', 'pontos_produtos_basicos',  'flag_principalidade',\n",
    "       'pontos_principalidade', 'faixa_atingimento_meta',\n",
    "       'percentual_atingimento',  'faixa_categoria',   'var_pontos',\n",
    "       'queda_flag', 'soma_quedas']\n",
    "colunas_principalidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar parquet base gerado pela célula DuckDB e restringir ESTRITAMENTE às colunas definidas\n",
    "import os, pandas as pd\n",
    "ruta_base = rf\"C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira\\associados_queda_principalidade.parquet\"\n",
    "print('Lendo parquet base (fonte única principalidade):', ruta_base)\n",
    "if not os.path.exists(ruta_base):\n",
    "    raise FileNotFoundError('Parquet de principalidade não encontrado. Execute antes a célula DuckDB que o gera.')\n",
    "_tmp = pd.read_parquet(ruta_base)\n",
    "# Validar presença integral das colunas obrigatórias\n",
    "faltantes = [c for c in colunas_principalidade if c not in _tmp.columns]\n",
    "if faltantes:\n",
    "    raise ValueError(f'Colunas obrigatórias ausentes no parquet: {faltantes}')\n",
    "# Manter somente as colunas definidas (ordem garantida)\n",
    "base_princ = _tmp[colunas_principalidade].copy()\n",
    "print('Linhas:', len(base_princ), '| Colunas mantidas:', len(base_princ.columns))\n",
    "base_princ.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Removido salvamento antigo redundante) Placeholder para evitar erro.\n",
    "print('Utilize a célula de montagem final para salvar os parquets.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivar produtos faltantes (executar após filtros e merges)\n",
    "# Diferenciar PF vs PJ: assumir segmento ou comprimento do documento (11=PF, >11=PJ) se não houver coluna específica\n",
    "\n",
    "prod_pf = {\n",
    "    'Chave PIX ativa': 'cad_pix_ativo',\n",
    "    'Débito automático': 'debito_conta_ativo',\n",
    "    'Conta salário': 'possui_folha_pagamento',\n",
    "    'Cartão de débito': 'possui_cartao_debito',\n",
    "    'Cartão de crédito': 'possui_cartao_credito',\n",
    "    'Canais digitais': 'transacao_app',\n",
    "    'Open Finance (receptor)': 'ativou_open_finance'\n",
    "}\n",
    "prod_pj = {\n",
    "    'Chave PIX ativa': 'cad_pix_ativo',\n",
    "    'Débito automático': 'debito_conta_ativo',\n",
    "    'Cartão de débito': 'possui_cartao_debito',\n",
    "    'Cartão de crédito': 'possui_cartao_credito',\n",
    "    'Canais digitais': 'transacao_app',\n",
    "    'Open Finance (receptor)': 'ativou_open_finance',\n",
    "    'Domicílio': 'possui_domicilio',\n",
    "    'Credenciamento ativo': 'possui_adquirencia',\n",
    "    'Cobrança': 'possui_cobranca',\n",
    "    'Folha de Pagamento': 'possui_folha_pagamento'\n",
    "}\n",
    "\n",
    "# Métricas fluxo / SOW (iguais para ambos)\n",
    "prod_cat2_metricas = {\n",
    "    'Cash-in': 'cash_in',\n",
    "    'Cash-out': 'cash_out',\n",
    "    'Total movimentações': 'cash_total',\n",
    "    'SOW Cartão': 'sow_cartao'\n",
    "}\n",
    "\n",
    "# Inferir tipo_pessoa\n",
    "base_princ['tipo_pessoa'] = np.where(base_princ['cpf_cnpj'].str.len()==11, 'PF','PJ')\n",
    "\n",
    "# Normalizar booleans\n",
    "for col in set(list(prod_pf.values()) + list(prod_pj.values())):\n",
    "    if col in base_princ.columns:\n",
    "        base_princ[col] = base_princ[col].replace({'S':1,'N':0}).fillna(0).astype(float)\n",
    "\n",
    "# Soma de chaves PIX fortes (se existisse colunas detalhadas, placeholder; usar cad_pix_ativo como 1 ou 0)\n",
    "base_princ['qtde_chaves_pix_fortes'] = base_princ.get('cad_pix_ativo', pd.Series([0]*len(base_princ))).fillna(0)\n",
    "\n",
    "# Produtos faltantes conforme tipo\n",
    "\n",
    "def faltantes_prod(r):\n",
    "    mapping = prod_pf if r['tipo_pessoa']=='PF' else prod_pj\n",
    "    return [n for n,c in mapping.items() if c in base_princ.columns and r.get(c,0)==0]\n",
    "\n",
    "base_princ['prod_cat1_faltantes'] = base_princ.apply(faltantes_prod, axis=1)\n",
    "\n",
    "# Métricas categoria 2\n",
    "base_princ['prod_cat2_metricas'] = base_princ.apply(\n",
    "    lambda r: [f\"{n}: {r.get(c)}\" for n,c in prod_cat2_metricas.items() if c in base_princ.columns], axis=1)\n",
    "\n",
    "base_princ.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_princ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe787ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar y enriquecer la base principal: produtos, métricas, descripciones y salida\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ não carregada. Execute a célula de leitura do parquet base antes desta.')\n",
    "\n",
    "# Mapeo de productos (clave descriptiva -> columna en base_princ)\n",
    "prod_cols_map = {\n",
    "    'Chave PIX ativa': 'cad_pix_ativo',\n",
    "    'Débito automático': 'debito_conta_ativo',\n",
    "    'Conta salário': 'possui_folha_pagamento',\n",
    "    'Cartão de débito': 'possui_cartao_debito',\n",
    "    'Cartão de crédito': 'possui_cartao_credito',\n",
    "    'Canais digitais': 'transacao_app',\n",
    "    'Open Finance (receptor)': 'ativou_open_finance',\n",
    "    'Domicílio': 'possui_domicilio',\n",
    "    'Credenciamento ativo': 'possui_adquirencia',\n",
    "    'Cobrança': 'possui_cobranca'\n",
    "}\n",
    "\n",
    "# Normalizar columnas de productos (S/N -> 1/0) si existen\n",
    "for col in set(prod_cols_map.values()):\n",
    "    if col in base_princ.columns:\n",
    "        base_princ[col] = base_princ[col].replace({'S': 1, 'N': 0}).fillna(0).astype(float)\n",
    "\n",
    "# Inferir tipo_pessoa si no existe\n",
    "if 'tipo_pessoa' not in base_princ.columns and 'cpf_cnpj' in base_princ.columns:\n",
    "    base_princ['tipo_pessoa'] = np.where(base_princ['cpf_cnpj'].astype(str).str.len() == 11, 'PF', 'PJ')\n",
    "\n",
    "# Función para listar productos faltantes por cliente\n",
    "def _falt(r):\n",
    "    tipo = r.get('tipo_pessoa', 'PF')\n",
    "    # Para PF excluimos productos que no aplican típicamente\n",
    "    excluidos_pf = {'Domicílio', 'Credenciamento ativo', 'Cobrança'}\n",
    "    activos = {k: v for k, v in prod_cols_map.items() if not (tipo == 'PF' and k in excluidos_pf)}\n",
    "    faltantes = []\n",
    "    for nome, col in activos.items():\n",
    "        if col in r.index and pd.notnull(r.get(col)):\n",
    "            try:\n",
    "                if float(r.get(col)) == 0:\n",
    "                    faltantes.append(nome)\n",
    "            except Exception:\n",
    "                # si no es numérico, considerarlo faltante si es falsy\n",
    "                if not r.get(col):\n",
    "                    faltantes.append(nome)\n",
    "        else:\n",
    "            # si la columna no existe en el registro, lo ignoramos\n",
    "            continue\n",
    "    return faltantes\n",
    "\n",
    "base_princ['prod_cat1_faltantes'] = base_princ.apply(_falt, axis=1)\n",
    "\n",
    "# Métricas de fluxo / SOW: etiquetas -> columnas esperadas en la base\n",
    "prod_cat2_metricas = {\n",
    "    'Cash-in': 'cash_in',\n",
    "    'Cash-out': 'cash_out',\n",
    "    'Cash-total movimentações': 'cash_total',\n",
    "    'SOW Total Cartão': 'sow_cartao'\n",
    "}\n",
    "\n",
    "def _fmt_val(v):\n",
    "    try:\n",
    "        return f\"{float(v):.2f}\"\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "def _monta_metricas(r):\n",
    "    itens = []\n",
    "    for nome, col in prod_cat2_metricas.items():\n",
    "        if col in r.index and pd.notnull(r.get(col)):\n",
    "            itens.append(f\"{nome}: {_fmt_val(r.get(col))}\")\n",
    "    return itens\n",
    "\n",
    "base_princ['prod_cat2_metricas'] = base_princ.apply(_monta_metricas, axis=1)\n",
    "\n",
    "# Título para o chamado\n",
    "base_princ['titulo'] = base_princ.apply(lambda r: f\"Oferta Principalidade - {r.get('cpf_cnpj', '')}\", axis=1)\n",
    "\n",
    "# Determinar columna ISA atual\n",
    "col_isa_atual = None\n",
    "if 'isa_atual_col' in globals() and isinstance(isa_atual_col, str) and isa_atual_col in base_princ.columns:\n",
    "    col_isa_atual = isa_atual_col\n",
    "elif 'isa_media' in base_princ.columns:\n",
    "    col_isa_atual = 'isa_media'\n",
    "\n",
    "def _fmt_isa_media3m(v):\n",
    "    try:\n",
    "        return f\"{float(v):.2f}\"\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "if col_isa_atual:\n",
    "    def _bloco_isa(r):\n",
    "        atual = r.get(col_isa_atual, '')\n",
    "        media3m = _fmt_isa_media3m(r.get('isa_media_3m_calc')) if 'isa_media_3m_calc' in r.index else ''\n",
    "        partes = []\n",
    "        if pd.notnull(atual) and atual != '':\n",
    "            partes.append(f\"ISA atual: {atual}\")\n",
    "        if media3m:\n",
    "            partes.append(f\"ISA média (3M): {media3m}\")\n",
    "        return ' | '.join(partes)\n",
    "    base_princ['bloco_isa'] = base_princ.apply(_bloco_isa, axis=1)\n",
    "else:\n",
    "    base_princ['bloco_isa'] = ''\n",
    "\n",
    "# Bloques de productos e indicadores\n",
    "base_princ['bloco_cat1'] = base_princ['prod_cat1_faltantes'].apply(lambda lst: 'Produtos básicos completos' if not lst else 'Produtos a ofertar: ' + ', '.join(lst))\n",
    "base_princ['bloco_cat2'] = base_princ['prod_cat2_metricas'].apply(lambda lst: ' / '.join(lst) if lst else '')\n",
    "\n",
    "# Descripción combinada\n",
    "def _descricao(r):\n",
    "    parts = [f\"Segmento: {r.get('segmento','')}\", f\"MC | Total: {r.get('cash_total','')}\"]\n",
    "    if r.get('bloco_isa'): parts.append(r.get('bloco_isa'))\n",
    "    if r.get('bloco_cat1'): parts.append(r.get('bloco_cat1'))\n",
    "    if r.get('bloco_cat2'): parts.append(r.get('bloco_cat2'))\n",
    "    return ' | '.join([p for p in parts if p])\n",
    "\n",
    "base_princ['descricao'] = base_princ.apply(_descricao, axis=1)\n",
    "\n",
    "# Categoria do chamado\n",
    "base_princ['categoriaChamadoId'] = base_princ['prod_cat1_faltantes'].apply(lambda l: 1 if len(l) > 0 else 2)\n",
    "nome_map = {1: 'Acesso', 2: 'Fluxo de Caixa'}\n",
    "base_princ['nomeCategoria'] = base_princ['categoriaChamadoId'].map(nome_map)\n",
    "\n",
    "desc_map = {1: 'Ofertar Básicos', 2: 'Ofertar Serviços de Movimentações'}\n",
    "base_princ['descricaoCategoria'] = base_princ['categoriaChamadoId'].map(desc_map)\n",
    "\n",
    "# Ordenar por pontos de principalidade se existir\n",
    "if 'pontos_principalidade' in base_princ.columns:\n",
    "    base_princ = base_princ.sort_values('pontos_principalidade').reset_index(drop=True)\n",
    "else:\n",
    "    base_princ = base_princ.reset_index(drop=True)\n",
    "\n",
    "# Distribuir datas de necessidade ciclando por 4 semanas (0,7,14,21)\n",
    "qtd = len(base_princ)\n",
    "ini = datetime.today().date()\n",
    "semanas = [0, 7, 14, 21]\n",
    "if qtd > 0:\n",
    "    base_princ['dataNecessidade'] = base_princ.index.map(lambda i: (ini + timedelta(days=semanas[i % len(semanas)])).isoformat() + 'T08:00:00.000Z')\n",
    "else:\n",
    "    base_princ['dataNecessidade'] = []\n",
    "\n",
    "# Montar campo categoriaChamado (estrutura esperada)\n",
    "def _monta_categoria(r):\n",
    "    return {\n",
    "        'categoriaChamadoId': int(r['categoriaChamadoId']),\n",
    "        'nomeCategoria': r.get('nomeCategoria',''),\n",
    "        'descricaoCategoria': r.get('descricaoCategoria',''),\n",
    "        'dataCriacao': datetime.today().date().isoformat(),\n",
    "        'departamentoId': int(r.get('cod_agencia', 0))\n",
    "    }\n",
    "\n",
    "base_princ['categoriaChamado'] = base_princ.apply(_monta_categoria, axis=1)\n",
    "\n",
    "# Preparar base_final renombrando columnas esperadas\n",
    "base_final = base_princ.rename(columns={'cod_agencia': 'numeroAgencia', 'nome_agencia': 'nomeDepartamento'})\n",
    "\n",
    "# Responsable / solicitante\n",
    "usuario_default = globals().get('user', '') or globals().get('USER', '')\n",
    "base_final['usuarioResponsavel'] = base_final.get('gestor', usuario_default)\n",
    "base_final['usuarioSolicitante'] = base_final['usuarioResponsavel']\n",
    "base_final['statusChamado'] = 'RASCUNHO'\n",
    "base_final['departamentoId'] = base_final['numeroAgencia']\n",
    "\n",
    "# Selección columnas de salida\n",
    "cols_saida = ['titulo', 'usuarioResponsavel', 'usuarioSolicitante', 'numeroAgencia',\n",
    "             'descricao', 'statusChamado', 'departamentoId', 'nomeDepartamento',\n",
    "             'dataNecessidade', 'categoriaChamado']\n",
    "\n",
    "resultado_chamados = base_final.loc[:, [c for c in cols_saida if c in base_final.columns]].copy()\n",
    "\n",
    "# Salvar arquivos parquet\n",
    "saida_dir = RUTAS.get('saida_dir_giro') if isinstance(RUTAS, dict) else None\n",
    "if saida_dir:\n",
    "    parquet_completo = os.path.join(saida_dir, 'giro_completo_principalidade.parquet')\n",
    "    parquet_chamados = os.path.join(saida_dir, 'giro_chamados_principalidade.parquet')\n",
    "    try:\n",
    "        base_princ.to_parquet(parquet_completo, index=False)\n",
    "        resultado_chamados.to_parquet(parquet_chamados, index=False)\n",
    "        print({'completo': parquet_completo, 'chamados': parquet_chamados, 'linhas_chamados': len(resultado_chamados)})\n",
    "    except Exception as e:\n",
    "        print('Erro ao salvar parquet:', e)\n",
    "else:\n",
    "    print('RUTAS[\"saida_dir_giro\"] não encontrada; não foi possível salvar os arquivos.')\n",
    "\n",
    "resultado_chamados.to_parquet(rf\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22e1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar filtros e preparar base principal (usa base_princ já carregada do parquet associados_queda_principalidade)\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ não carregada. Execute a célula de leitura do parquet base antes desta.')\n",
    "\n",
    "if base_princ.empty:\n",
    "    raise ValueError('base_princ está vazia. Verifique o parquet base.')\n",
    "\n",
    "# Normalizar chave\n",
    "def normaliza_doc(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "    v = re.sub(r'\\D','', str(valor))\n",
    "    return v.lstrip('0')\n",
    "\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    possiveis_chaves = [c for c in base_princ.columns if 'cpf' in c.lower() or 'cnpj' in c.lower() or 'doc' in c.lower()]\n",
    "    if possiveis_chaves:\n",
    "        base_princ.rename(columns={possiveis_chaves[0]:'cpf_cnpj'}, inplace=True)\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    raise ValueError('Coluna cpf_cnpj ausente na base principalidade mês.')\n",
    "base_princ['cpf_cnpj'] = base_princ['cpf_cnpj'].map(normaliza_doc)\n",
    "\n",
    "# Inadimplentes\n",
    "path_inad = RUTAS['inadimplentes']\n",
    "if os.path.exists(path_inad):\n",
    "    inad = pd.read_parquet(path_inad)\n",
    "    for alt in ['cpf','cnpj','documento','doc','num_cpf_cnpj']:\n",
    "        if alt in inad.columns and 'cpf_cnpj' not in inad.columns:\n",
    "            inad.rename(columns={alt:'cpf_cnpj'}, inplace=True)\n",
    "    if 'cpf_cnpj' not in inad.columns:\n",
    "        inad['cpf_cnpj'] = inad.iloc[:,0]\n",
    "    inad['cpf_cnpj'] = inad['cpf_cnpj'].map(normaliza_doc)\n",
    "    inad = inad[['cpf_cnpj']].drop_duplicates()\n",
    "    inad['flag_inadimplente'] = 1\n",
    "    base_princ = base_princ.merge(inad, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['flag_inadimplente'] = np.nan\n",
    "\n",
    "# Risco BBM (normalizar caixa / acentuação mínima)\n",
    "if 'nivel_risco' in base_princ.columns:\n",
    "    base_princ['nivel_risco_norm'] = base_princ['nivel_risco'].astype(str).str.upper().str.strip()\n",
    "    riscos_ok = [r.upper() for r in PARAMS['filtros_risco_bbm']]\n",
    "    base_princ = base_princ[base_princ['nivel_risco_norm'].isin(riscos_ok)]\n",
    "else:\n",
    "    print('Aviso: coluna nivel_risco ausente.')\n",
    "\n",
    "# Sem inadimplência\n",
    "base_princ = base_princ[(base_princ['flag_inadimplente'].isna()) | (base_princ['flag_inadimplente']!=1)]\n",
    "\n",
    "# Merge dashboard (inclui ult_movimento)\n",
    "if 'assoc_dash' in globals():\n",
    "    assoc_dash['cpf_cnpj'] = assoc_dash['cpf_cnpj'].map(normaliza_doc)\n",
    "    base_princ = base_princ.merge(assoc_dash, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    print('Dashboard não carregado: execute a célula de carga do dashboard/ISA antes desta.')\n",
    "\n",
    "# Filtro movimento 20-45 dias usando ult_movimento\n",
    "if 'ult_movimento' in base_princ.columns:\n",
    "    hoje = pd.Timestamp.today().normalize()\n",
    "    base_princ['ult_movimento_dt'] = pd.to_datetime(base_princ['ult_movimento'], errors='coerce')\n",
    "    base_princ['dias_sem_mov'] = (hoje - base_princ['ult_movimento_dt']).dt.days\n",
    "    lim_inf, lim_sup = PARAMS['dias_sem_movimentacao']\n",
    "    base_princ = base_princ[(base_princ['dias_sem_mov']>=lim_inf) & (base_princ['dias_sem_mov']<=lim_sup)]\n",
    "else:\n",
    "    print('Coluna ult_movimento ausente no dashboard para filtro de movimento.')\n",
    "\n",
    "# Merge ISA se disponível\n",
    "if 'isa_sel' in globals() and isinstance(isa_sel, pd.DataFrame) and not isa_sel.empty and 'cpf_cnpj' in isa_sel.columns:\n",
    "    isa_sel['cpf_cnpj'] = isa_sel['cpf_cnpj'].map(normaliza_doc)\n",
    "    if 'gestor' in base_princ.columns and 'gestor' in isa_sel.columns:\n",
    "        base_princ = base_princ.merge(isa_sel, on=['cpf_cnpj','gestor'], how='left')\n",
    "    else:\n",
    "        base_princ = base_princ.merge(isa_sel, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['isa_media_3m_calc'] = np.nan\n",
    "\n",
    "# Corte ISA >= 2 (média atual ou média 3M). Remove quem está fora.\n",
    "if 'isa_media' in base_princ.columns:\n",
    "    base_princ = base_princ[(base_princ['isa_media'].fillna(0) >= 2) | (base_princ.get('isa_media_3m_calc', pd.Series(dtype=float)).fillna(0) >= 2)]\n",
    "\n",
    "print('Base após filtros (inclui corte ISA>=2):', base_princ.shape)\n",
    "base_princ.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar parquets: completo (dados filtrados) e card Kanban\n",
    "import json\n",
    "\n",
    "if 'base_princ' not in globals() or 'resultado_chamados' not in globals():\n",
    "    raise RuntimeError('Execute as células anteriores antes de gerar os parquets finais.')\n",
    "\n",
    "# 1. Parquet completo já salvo anteriormente, mas reforçamos aqui se desejar atualizar\n",
    "parquet_completo = os.path.join(RUTAS['saida_dir_giro'], 'giro_completo_principalidade.parquet')\n",
    "base_princ.to_parquet(parquet_completo, index=False)\n",
    "\n",
    "# 2. Parquet apenas com campos necessários para o card (Kanban)\n",
    "cols_card = [\n",
    "    'titulo',\n",
    "    'descricao',\n",
    "    'categoriaChamado',\n",
    "    'usuarioResponsavel',\n",
    "    'usuarioSolicitante',\n",
    "    'dataNecessidade',\n",
    "    'statusChamado',\n",
    "    'numeroAgencia',\n",
    "    'nomeDepartamento',\n",
    "    'departamentoId'\n",
    "]\n",
    "card_df = resultado_chamados[cols_card].copy()\n",
    "\n",
    "# Opcional: serializar categoriaChamado em JSON para consumo externo se necessário\n",
    "card_df['categoriaChamado_json'] = card_df['categoriaChamado'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "\n",
    "parquet_card = os.path.join(RUTAS['saida_dir_giro'], 'giro_cards_kanban.parquet')\n",
    "card_df.to_parquet(parquet_card, index=False)\n",
    "\n",
    "print({'parquet_completo': parquet_completo, 'parquet_card': parquet_card, 'linhas_card': len(card_df)})\n",
    "card_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE FINAL UNIFICADA\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ ausente. Execute leitura base.')\n",
    "\n",
    "# Garantir campos essenciais derivados\n",
    "if 'prod_cat1_faltantes' not in base_princ.columns:\n",
    "    raise RuntimeError('Execute a célula de derivação de produtos antes da pipeline final.')\n",
    "\n",
    "# Família pontuação por grupos (Acesso = produtos básicos, Fluxo = cash, SOW = sow_cartao)\n",
    "base_princ['score_acesso'] = base_princ.get('pontos_produtos_basicos', np.nan)\n",
    "base_princ['score_fluxo'] = base_princ.get('cash_total_fator', np.nan)\n",
    "base_princ['score_sow'] = base_princ.get('sow_cartao', np.nan)\n",
    "\n",
    "# Classificação novo associado (flg_novo_assoc ou meses_desde_associacao)\n",
    "base_princ['class_assoc'] = np.where(\n",
    "    (base_princ.get('flg_novo_assoc',0)==1) | (base_princ.get('meses_desde_associacao',999)<=6),\n",
    "    'Novo', 'Base Giro'\n",
    ")\n",
    "\n",
    "# Blocos texto adicionais\n",
    "base_princ['bloco_principalidade'] = base_princ.apply(lambda r: f\"Faixa: {r.get('faixa_categoria')} | Var pts: {r.get('var_pontos')} | Queda flag: {r.get('queda_flag')} | Soma quedas: {r.get('soma_quedas')}\", axis=1)\n",
    "base_princ['bloco_familias'] = base_princ.apply(lambda r: f\"Pontuações -> Acesso: {r.get('score_acesso')} | Fluxo: {r.get('score_fluxo')} | SOW Cartão: {r.get('score_sow')}\", axis=1)\n",
    "base_princ['bloco_tipo_pix'] = base_princ.apply(lambda r: f\"Tipo Pessoa: {r.get('tipo_pessoa')} | Chaves PIX fortes: {r.get('qtde_chaves_pix_fortes')}\", axis=1)\n",
    "base_princ['bloco_assoc'] = base_princ['class_assoc'].apply(lambda v: f\"Classificação Associação: {v}\")\n",
    "\n",
    "# Bloco ISA (recalcular se necessário)\n",
    "base_princ['bloco_isa'] = base_princ.apply(lambda r: (\n",
    "    f\"ISA atual: {r.get('isa_media')} | ISA média 3M: {r.get('isa_media_3m_calc'):.2f}\" if pd.notnull(r.get('isa_media_3m_calc')) else (\n",
    "        f\"ISA atual: {r.get('isa_media')}\" if pd.notnull(r.get('isa_media')) else ''\n",
    "    )\n",
    "), axis=1)\n",
    "\n",
    "base_princ['bloco_prod_basicos'] = base_princ['prod_cat1_faltantes'].apply(lambda lst: ('Produtos básicos a ofertar: ' + ', '.join(lst)) if lst else 'Produtos básicos completos')\n",
    "base_princ['bloco_fluxo'] = base_princ['prod_cat2_metricas'].apply(lambda m: 'Indicadores fluxo / SOW: ' + '; '.join(m))\n",
    "\n",
    "# Título e descrição (incluyendo nuevos blocos)\n",
    "base_princ['titulo'] = base_princ.apply(lambda r: f\"Oferta Principalidade - {r.get('cpf_cnpj')}\", axis=1)\n",
    "base_princ['descricao'] = base_princ.apply(lambda r: ' | '.join([\n",
    "    f\"Segmento: {r.get('segmento')}\",\n",
    "    r.get('bloco_tipo_pix',''),\n",
    "    r.get('bloco_assoc',''),\n",
    "    r.get('bloco_isa',''),\n",
    "    r.get('bloco_principalidade',''),\n",
    "    r.get('bloco_familias',''),\n",
    "    f\"MC Total Atual: {r.get('cash_total')}\",\n",
    "    r.get('bloco_prod_basicos',''),\n",
    "    r.get('bloco_fluxo','')\n",
    "]), axis=1)\n",
    "\n",
    "# Categoria (1 se há produto faltante, senão 2)\n",
    "base_princ['categoriaChamadoId'] = base_princ['prod_cat1_faltantes'].apply(lambda l: 1 if len(l)>0 else 2)\n",
    "base_princ['nomeCategoria'] = base_princ['categoriaChamadoId'].map({1:'Acesso',2:'Fluxo de Caixa'})\n",
    "base_princ['descricaoCategoria'] = base_princ['nomeCategoria'].map({'Acesso':'Ofertar Produtos e Serviços Básicos','Fluxo de Caixa':'Ofertar Movimentações e Uso de Crédito'})\n",
    "\n",
    "# Distribuir datas\n",
    "base_princ = base_princ.sort_values('pontos_principalidade').reset_index(drop=True)\n",
    "qtd = len(base_princ)\n",
    "ini_mes = datetime.today().replace(day=1).date()\n",
    "semanas = [0,7,14,21]\n",
    "base_princ['dataNecessidade'] = base_princ.index.map(lambda i: (ini_mes + timedelta(days=semanas[(i * 4 // (qtd if qtd>0 else 1)) % 4])).isoformat()+'T08:00:00.000Z')\n",
    "\n",
    "base_princ['categoriaChamado'] = base_princ.apply(lambda r: {\n",
    "    'categoriaChamadoId': int(r['categoriaChamadoId']),\n",
    "    'nomeCategoria': r['nomeCategoria'],\n",
    "    'descricaoCategoria': r['descricaoCategoria'],\n",
    "    'dataCriacao': datetime.today().date().isoformat(),\n",
    "    'departamentoId': int(r.get('cod_agencia',0))\n",
    "}, axis=1)\n",
    "\n",
    "# Preparar base_final renombrando columnas esperadas\n",
    "base_final = base_princ.rename(columns={'cod_agencia': 'numeroAgencia', 'nome_agencia': 'nomeDepartamento'})\n",
    "\n",
    "# Responsable / solicitante\n",
    "usuario_default = globals().get('user', '') or globals().get('USER', '')\n",
    "base_final['usuarioResponsavel'] = base_final.get('gestor', usuario_default)\n",
    "base_final['usuarioSolicitante'] = base_final['usuarioResponsavel']\n",
    "base_final['statusChamado'] = 'RASCUNHO'\n",
    "base_final['departamentoId'] = base_final['numeroAgencia']\n",
    "\n",
    "# Selección columnas de salida\n",
    "cols_saida = ['titulo', 'usuarioResponsavel', 'usuarioSolicitante', 'numeroAgencia',\n",
    "             'descricao', 'statusChamado', 'departamentoId', 'nomeDepartamento',\n",
    "             'dataNecessidade', 'categoriaChamado']\n",
    "\n",
    "resultado_chamados = base_final.loc[:, [c for c in cols_saida if c in base_final.columns]].copy()\n",
    "\n",
    "# Salvar arquivos parquet\n",
    "saida_dir = RUTAS.get('saida_dir_giro') if isinstance(RUTAS, dict) else None\n",
    "if saida_dir:\n",
    "    parquet_completo = os.path.join(saida_dir, 'giro_completo_principalidade.parquet')\n",
    "    parquet_chamados = os.path.join(saida_dir, 'giro_chamados_principalidade.parquet')\n",
    "    try:\n",
    "        base_princ.to_parquet(parquet_completo, index=False)\n",
    "        resultado_chamados.to_parquet(parquet_chamados, index=False)\n",
    "        print({'completo': parquet_completo, 'chamados': parquet_chamados, 'linhas_chamados': len(resultado_chamados)})\n",
    "    except Exception as e:\n",
    "        print('Erro ao salvar parquet:', e)\n",
    "else:\n",
    "    print('RUTAS[\"saida_dir_giro\"] não encontrada; não foi possível salvar os arquivos.')\n",
    "\n",
    "resultado_chamados.to_parquet(rf\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar filtros e preparar base principal (usa base_princ já carregada do parquet associados_queda_principalidade)\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ não carregada. Execute a célula de leitura do parquet base antes desta.')\n",
    "\n",
    "if base_princ.empty:\n",
    "    raise ValueError('base_princ está vazia. Verifique o parquet base.')\n",
    "\n",
    "# Normalizar chave\n",
    "def normaliza_doc(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "    v = re.sub(r'\\D','', str(valor))\n",
    "    return v.lstrip('0')\n",
    "\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    possiveis_chaves = [c for c in base_princ.columns if 'cpf' in c.lower() or 'cnpj' in c.lower() or 'doc' in c.lower()]\n",
    "    if possiveis_chaves:\n",
    "        base_princ.rename(columns={possiveis_chaves[0]:'cpf_cnpj'}, inplace=True)\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    raise ValueError('Coluna cpf_cnpj ausente na base principalidade mês.')\n",
    "base_princ['cpf_cnpj'] = base_princ['cpf_cnpj'].map(normaliza_doc)\n",
    "\n",
    "# Inadimplentes\n",
    "path_inad = RUTAS['inadimplentes']\n",
    "if os.path.exists(path_inad):\n",
    "    inad = pd.read_parquet(path_inad)\n",
    "    for alt in ['cpf','cnpj','documento','doc','num_cpf_cnpj']:\n",
    "        if alt in inad.columns and 'cpf_cnpj' not in inad.columns:\n",
    "            inad.rename(columns={alt:'cpf_cnpj'}, inplace=True)\n",
    "    if 'cpf_cnpj' not in inad.columns:\n",
    "        inad['cpf_cnpj'] = inad.iloc[:,0]\n",
    "    inad['cpf_cnpj'] = inad['cpf_cnpj'].map(normaliza_doc)\n",
    "    inad = inad[['cpf_cnpj']].drop_duplicates()\n",
    "    inad['flag_inadimplente'] = 1\n",
    "    base_princ = base_princ.merge(inad, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['flag_inadimplente'] = np.nan\n",
    "\n",
    "# Risco BBM (normalizar caixa / acentuação mínima)\n",
    "if 'nivel_risco' in base_princ.columns:\n",
    "    base_princ['nivel_risco_norm'] = base_princ['nivel_risco'].astype(str).str.upper().str.strip()\n",
    "    riscos_ok = [r.upper() for r in PARAMS['filtros_risco_bbm']]\n",
    "    base_princ = base_princ[base_princ['nivel_risco_norm'].isin(riscos_ok)]\n",
    "else:\n",
    "    print('Aviso: coluna nivel_risco ausente.')\n",
    "\n",
    "# Sem inadimplência\n",
    "base_princ = base_princ[(base_princ['flag_inadimplente'].isna()) | (base_princ['flag_inadimplente']!=1)]\n",
    "\n",
    "# Merge dashboard (inclui ult_movimento)\n",
    "if 'assoc_dash' in globals():\n",
    "    assoc_dash['cpf_cnpj'] = assoc_dash['cpf_cnpj'].map(normaliza_doc)\n",
    "    base_princ = base_princ.merge(assoc_dash, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    print('Dashboard não carregado: execute a célula de carga do dashboard/ISA antes desta.')\n",
    "\n",
    "# Filtro movimento 20-45 dias usando ult_movimento\n",
    "if 'ult_movimento' in base_princ.columns:\n",
    "    hoje = pd.Timestamp.today().normalize()\n",
    "    base_princ['ult_movimento_dt'] = pd.to_datetime(base_princ['ult_movimento'], errors='coerce')\n",
    "    base_princ['dias_sem_mov'] = (hoje - base_princ['ult_movimento_dt']).dt.days\n",
    "    lim_inf, lim_sup = PARAMS['dias_sem_movimentacao']\n",
    "    base_princ = base_princ[(base_princ['dias_sem_mov']>=lim_inf) & (base_princ['dias_sem_mov']<=lim_sup)]\n",
    "else:\n",
    "    print('Coluna ult_movimento ausente no dashboard para filtro de movimento.')\n",
    "\n",
    "# Merge ISA se disponível\n",
    "if 'isa_sel' in globals() and isinstance(isa_sel, pd.DataFrame) and not isa_sel.empty and 'cpf_cnpj' in isa_sel.columns:\n",
    "    isa_sel['cpf_cnpj'] = isa_sel['cpf_cnpj'].map(normaliza_doc)\n",
    "    if 'gestor' in base_princ.columns and 'gestor' in isa_sel.columns:\n",
    "        base_princ = base_princ.merge(isa_sel, on=['cpf_cnpj','gestor'], how='left')\n",
    "    else:\n",
    "        base_princ = base_princ.merge(isa_sel, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['isa_media_3m_calc'] = np.nan\n",
    "\n",
    "# Corte ISA >= 2 (média atual ou média 3M). Remove quem está fora.\n",
    "if 'isa_media' in base_princ.columns:\n",
    "    base_princ = base_princ[(base_princ['isa_media'].fillna(0) >= 2) | (base_princ.get('isa_media_3m_calc', pd.Series(dtype=float)).fillna(0) >= 2)]\n",
    "\n",
    "print('Base após filtros (inclui corte ISA>=2):', base_princ.shape)\n",
    "base_princ.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar parquets: completo (dados filtrados) e card Kanban\n",
    "import json\n",
    "\n",
    "if 'base_princ' not in globals() or 'resultado_chamados' not in globals():\n",
    "    raise RuntimeError('Execute as células anteriores antes de gerar os parquets finais.')\n",
    "\n",
    "# 1. Parquet completo já salvo anteriormente, mas reforçamos aqui se desejar atualizar\n",
    "parquet_completo = os.path.join(RUTAS['saida_dir_giro'], 'giro_completo_principalidade.parquet')\n",
    "base_princ.to_parquet(parquet_completo, index=False)\n",
    "\n",
    "# 2. Parquet apenas com campos necessários para o card (Kanban)\n",
    "cols_card = [\n",
    "    'titulo',\n",
    "    'descricao',\n",
    "    'categoriaChamado',\n",
    "    'usuarioResponsavel',\n",
    "    'usuarioSolicitante',\n",
    "    'dataNecessidade',\n",
    "    'statusChamado',\n",
    "    'numeroAgencia',\n",
    "    'nomeDepartamento',\n",
    "    'departamentoId'\n",
    "]\n",
    "card_df = resultado_chamados[cols_card].copy()\n",
    "\n",
    "# Opcional: serializar categoriaChamado em JSON para consumo externo se necessário\n",
    "card_df['categoriaChamado_json'] = card_df['categoriaChamado'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "\n",
    "parquet_card = os.path.join(RUTAS['saida_dir_giro'], 'giro_cards_kanban.parquet')\n",
    "card_df.to_parquet(parquet_card, index=False)\n",
    "\n",
    "print({'parquet_completo': parquet_completo, 'parquet_card': parquet_card, 'linhas_card': len(card_df)})\n",
    "card_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE FINAL UNIFICADA\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ ausente. Execute leitura base.')\n",
    "\n",
    "# Garantir campos essenciais derivados\n",
    "if 'prod_cat1_faltantes' not in base_princ.columns:\n",
    "    raise RuntimeError('Execute a célula de derivação de produtos antes da pipeline final.')\n",
    "\n",
    "# Família pontuação por grupos (Acesso = produtos básicos, Fluxo = cash, SOW = sow_cartao)\n",
    "base_princ['score_acesso'] = base_princ.get('pontos_produtos_basicos', np.nan)\n",
    "base_princ['score_fluxo'] = base_princ.get('cash_total_fator', np.nan)\n",
    "base_princ['score_sow'] = base_princ.get('sow_cartao', np.nan)\n",
    "\n",
    "# Classificação novo associado (flg_novo_assoc ou meses_desde_associacao)\n",
    "base_princ['class_assoc'] = np.where(\n",
    "    (base_princ.get('flg_novo_assoc',0)==1) | (base_princ.get('meses_desde_associacao',999)<=6),\n",
    "    'Novo', 'Base Giro'\n",
    ")\n",
    "\n",
    "# Blocos texto adicionais\n",
    "base_princ['bloco_principalidade'] = base_princ.apply(lambda r: f\"Faixa: {r.get('faixa_categoria')} | Var pts: {r.get('var_pontos')} | Queda flag: {r.get('queda_flag')} | Soma quedas: {r.get('soma_quedas')}\", axis=1)\n",
    "base_princ['bloco_familias'] = base_princ.apply(lambda r: f\"Pontuações -> Acesso: {r.get('score_acesso')} | Fluxo: {r.get('score_fluxo')} | SOW Cartão: {r.get('score_sow')}\", axis=1)\n",
    "base_princ['bloco_tipo_pix'] = base_princ.apply(lambda r: f\"Tipo Pessoa: {r.get('tipo_pessoa')} | Chaves PIX fortes: {r.get('qtde_chaves_pix_fortes')}\", axis=1)\n",
    "base_princ['bloco_assoc'] = base_princ['class_assoc'].apply(lambda v: f\"Classificação Associação: {v}\")\n",
    "\n",
    "# Bloco ISA (recalcular se necessário)\n",
    "base_princ['bloco_isa'] = base_princ.apply(lambda r: (\n",
    "    f\"ISA atual: {r.get('isa_media')} | ISA média 3M: {r.get('isa_media_3m_calc'):.2f}\" if pd.notnull(r.get('isa_media_3m_calc')) else (\n",
    "        f\"ISA atual: {r.get('isa_media')}\" if pd.notnull(r.get('isa_media')) else ''\n",
    "    )\n",
    "), axis=1)\n",
    "\n",
    "base_princ['bloco_prod_basicos'] = base_princ['prod_cat1_faltantes'].apply(lambda lst: ('Produtos básicos a ofertar: ' + ', '.join(lst)) if lst else 'Produtos básicos completos')\n",
    "base_princ['bloco_fluxo'] = base_princ['prod_cat2_metricas'].apply(lambda m: 'Indicadores fluxo / SOW: ' + '; '.join(m))\n",
    "\n",
    "# Título e descrição (incluindo novos blocos)\n",
    "base_princ['titulo'] = base_princ.apply(lambda r: f\"Oferta Principalidade - {r.get('cpf_cnpj')}\", axis=1)\n",
    "base_princ['descricao'] = base_princ.apply(lambda r: ' | '.join([\n",
    "    f\"Segmento: {r.get('segmento')}\",\n",
    "    r.get('bloco_tipo_pix',''),\n",
    "    r.get('bloco_assoc',''),\n",
    "    r.get('bloco_isa',''),\n",
    "    r.get('bloco_principalidade',''),\n",
    "    r.get('bloco_familias',''),\n",
    "    f\"MC Total Atual: {r.get('cash_total')}\",\n",
    "    r.get('bloco_prod_basicos',''),\n",
    "    r.get('bloco_fluxo','')\n",
    "]), axis=1)\n",
    "\n",
    "# Categoria (1 se há produto faltante, senão 2)\n",
    "base_princ['categoriaChamadoId'] = base_princ['prod_cat1_faltantes'].apply(lambda l: 1 if len(l)>0 else 2)\n",
    "base_princ['nomeCategoria'] = base_princ['categoriaChamadoId'].map({1:'Acesso',2:'Fluxo de Caixa'})\n",
    "base_princ['descricaoCategoria'] = base_princ['nomeCategoria'].map({'Acesso':'Ofertar Produtos e Serviços Básicos','Fluxo de Caixa':'Ofertar Movimentações e Uso de Crédito'})\n",
    "\n",
    "# Distribuir datas\n",
    "base_princ = base_princ.sort_values('pontos_principalidade').reset_index(drop=True)\n",
    "qtd = len(base_princ)\n",
    "ini_mes = datetime.today().replace(day=1).date()\n",
    "semanas = [0,7,14,21]\n",
    "base_princ['dataNecessidade'] = base_princ.index.map(lambda i: (ini_mes + timedelta(days=semanas[(i * 4 // (qtd if qtd>0 else 1)) % 4])).isoformat()+'T08:00:00.000Z')\n",
    "\n",
    "base_princ['categoriaChamado'] = base_princ.apply(lambda r: {\n",
    "    'categoriaChamadoId': int(r['categoriaChamadoId']),\n",
    "    'nomeCategoria': r['nomeCategoria'],\n",
    "    'descricaoCategoria': r['descricaoCategoria'],\n",
    "    'dataCriacao': datetime.today().date().isoformat(),\n",
    "    'departamentoId': int(r.get('cod_agencia',0))\n",
    "}, axis=1)\n",
    "\n",
    "# Preparar base_final renombrando columnas esperadas\n",
    "base_final = base_princ.rename(columns={'cod_agencia': 'numeroAgencia', 'nome_agencia': 'nomeDepartamento'})\n",
    "\n",
    "# Responsable / solicitante\n",
    "usuario_default = globals().get('user', '') or globals().get('USER', '')\n",
    "base_final['usuarioResponsavel'] = base_final.get('gestor', usuario_default)\n",
    "base_final['usuarioSolicitante'] = base_final['usuarioResponsavel']\n",
    "base_final['statusChamado'] = 'RASCUNHO'\n",
    "base_final['departamentoId'] = base_final['numeroAgencia']\n",
    "\n",
    "# Selección columnas de salida\n",
    "cols_saida = ['titulo', 'usuarioResponsavel', 'usuarioSolicitante', 'numeroAgencia',\n",
    "             'descricao', 'statusChamado', 'departamentoId', 'nomeDepartamento',\n",
    "             'dataNecessidade', 'categoriaChamado']\n",
    "\n",
    "resultado_chamados = base_final.loc[:, [c for c in cols_saida if c in base_final.columns]].copy()\n",
    "\n",
    "# Salvar arquivos parquet\n",
    "saida_dir = RUTAS.get('saida_dir_giro') if isinstance(RUTAS, dict) else None\n",
    "if saida_dir:\n",
    "    parquet_completo = os.path.join(saida_dir, 'giro_completo_principalidade.parquet')\n",
    "    parquet_chamados = os.path.join(saida_dir, 'giro_chamados_principalidade.parquet')\n",
    "    try:\n",
    "        base_princ.to_parquet(parquet_completo, index=False)\n",
    "        resultado_chamados.to_parquet(parquet_chamados, index=False)\n",
    "        print({'completo': parquet_completo, 'chamados': parquet_chamados, 'linhas_chamados': len(resultado_chamados)})\n",
    "    except Exception as e:\n",
    "        print('Erro ao salvar parquet:', e)\n",
    "else:\n",
    "    print('RUTAS[\"saida_dir_giro\"] não encontrada; não foi possível salvar os arquivos.')\n",
    "\n",
    "resultado_chamados.to_parquet(rf\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar filtros e preparar base principal (usa base_princ já carregada do parquet associados_queda_principalidade)\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ não carregada. Execute a célula de leitura do parquet base antes desta.')\n",
    "\n",
    "if base_princ.empty:\n",
    "    raise ValueError('base_princ está vazia. Verifique o parquet base.')\n",
    "\n",
    "# Normalizar chave\n",
    "def normaliza_doc(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "    v = re.sub(r'\\D','', str(valor))\n",
    "    return v.lstrip('0')\n",
    "\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    possiveis_chaves = [c for c in base_princ.columns if 'cpf' in c.lower() or 'cnpj' in c.lower() or 'doc' in c.lower()]\n",
    "    if possiveis_chaves:\n",
    "        base_princ.rename(columns={possiveis_chaves[0]:'cpf_cnpj'}, inplace=True)\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    raise ValueError('Coluna cpf_cnpj ausente na base principalidade mês.')\n",
    "base_princ['cpf_cnpj'] = base_princ['cpf_cnpj'].map(normaliza_doc)\n",
    "\n",
    "# Inadimplentes\n",
    "path_inad = RUTAS['inadimplentes']\n",
    "if os.path.exists(path_inad):\n",
    "    inad = pd.read_parquet(path_inad)\n",
    "    for alt in ['cpf','cnpj','documento','doc','num_cpf_cnpj']:\n",
    "        if alt in inad.columns and 'cpf_cnpj' not in inad.columns:\n",
    "            inad.rename(columns={alt:'cpf_cnpj'}, inplace=True)\n",
    "    if 'cpf_cnpj' not in inad.columns:\n",
    "        inad['cpf_cnpj'] = inad.iloc[:,0]\n",
    "    inad['cpf_cnpj'] = inad['cpf_cnpj'].map(normaliza_doc)\n",
    "    inad = inad[['cpf_cnpj']].drop_duplicates()\n",
    "    inad['flag_inadimplente'] = 1\n",
    "    base_princ = base_princ.merge(inad, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['flag_inadimplente'] = np.nan\n",
    "\n",
    "# Risco BBM (normalizar caixa / acentuação mínima)\n",
    "if 'nivel_risco' in base_princ.columns:\n",
    "    base_princ['nivel_risco_norm'] = base_princ['nivel_risco'].astype(str).str.upper().str.strip()\n",
    "    riscos_ok = [r.upper() for r in PARAMS['filtros_risco_bbm']]\n",
    "    base_princ = base_princ[base_princ['nivel_risco_norm'].isin(riscos_ok)]\n",
    "else:\n",
    "    print('Aviso: coluna nivel_risco ausente.')\n",
    "\n",
    "# Sem inadimplência\n",
    "base_princ = base_princ[(base_princ['flag_inadimplente'].isna()) | (base_princ['flag_inadimplente']!=1)]\n",
    "\n",
    "# Merge dashboard (inclui ult_movimento)\n",
    "if 'assoc_dash' in globals():\n",
    "    assoc_dash['cpf_cnpj'] = assoc_dash['cpf_cnpj'].map(normaliza_doc)\n",
    "    base_princ = base_princ.merge(assoc_dash, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    print('Dashboard não carregado: execute a célula de carga do dashboard/ISA antes desta.')\n",
    "\n",
    "# Filtro movimento 20-45 dias usando ult_movimento\n",
    "if 'ult_movimento' in base_princ.columns:\n",
    "    hoje = pd.Timestamp.today().normalize()\n",
    "    base_princ['ult_movimento_dt'] = pd.to_datetime(base_princ['ult_movimento'], errors='coerce')\n",
    "    base_princ['dias_sem_mov'] = (hoje - base_princ['ult_movimento_dt']).dt.days\n",
    "    lim_inf, lim_sup = PARAMS['dias_sem_movimentacao']\n",
    "    base_princ = base_princ[(base_princ['dias_sem_mov']>=lim_inf) & (base_princ['dias_sem_mov']<=lim_sup)]\n",
    "else:\n",
    "    print('Coluna ult_movimento ausente no dashboard para filtro de movimento.')\n",
    "\n",
    "# Merge ISA se disponível\n",
    "if 'isa_sel' in globals() and isinstance(isa_sel, pd.DataFrame) and not isa_sel.empty and 'cpf_cnpj' in isa_sel.columns:\n",
    "    isa_sel['cpf_cnpj'] = isa_sel['cpf_cnpj'].map(normaliza_doc)\n",
    "    if 'gestor' in base_princ.columns and 'gestor' in isa_sel.columns:\n",
    "        base_princ = base_princ.merge(isa_sel, on=['cpf_cnpj','gestor'], how='left')\n",
    "    else:\n",
    "        base_princ = base_princ.merge(isa_sel, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['isa_media_3m_calc'] = np.nan\n",
    "\n",
    "# Corte ISA >= 2 (média atual ou média 3M). Remove quem está fora.\n",
    "if 'isa_media' in base_princ.columns:\n",
    "    base_princ = base_princ[(base_princ['isa_media'].fillna(0) >= 2) | (base_princ.get('isa_media_3m_calc', pd.Series(dtype=float)).fillna(0) >= 2)]\n",
    "\n",
    "print('Base após filtros (inclui corte ISA>=2):', base_princ.shape)\n",
    "base_princ.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar parquets: completo (dados filtrados) e card Kanban\n",
    "import json\n",
    "\n",
    "if 'base_princ' not in globals() or 'resultado_chamados' not in globals():\n",
    "    raise RuntimeError('Execute as células anteriores antes de gerar os parquets finais.')\n",
    "\n",
    "# 1. Parquet completo já salvo anteriormente, mas reforçamos aqui se desejar atualizar\n",
    "parquet_completo = os.path.join(RUTAS['saida_dir_giro'], 'giro_completo_principalidade.parquet')\n",
    "base_princ.to_parquet(parquet_completo, index=False)\n",
    "\n",
    "# 2. Parquet apenas com campos necessários para o card (Kanban)\n",
    "cols_card = [\n",
    "    'titulo',\n",
    "    'descricao',\n",
    "    'categoriaChamado',\n",
    "    'usuarioResponsavel',\n",
    "    'usuarioSolicitante',\n",
    "    'dataNecessidade',\n",
    "    'statusChamado',\n",
    "    'numeroAgencia',\n",
    "    'nomeDepartamento',\n",
    "    'departamentoId'\n",
    "]\n",
    "card_df = resultado_chamados[cols_card].copy()\n",
    "\n",
    "# Opcional: serializar categoriaChamado em JSON para consumo externo se necessário\n",
    "card_df['categoriaChamado_json'] = card_df['categoriaChamado'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "\n",
    "parquet_card = os.path.join(RUTAS['saida_dir_giro'], 'giro_cards_kanban.parquet')\n",
    "card_df.to_parquet(parquet_card, index=False)\n",
    "\n",
    "print({'parquet_completo': parquet_completo, 'parquet_card': parquet_card, 'linhas_card': len(card_df)})\n",
    "card_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE FINAL UNIFICADA\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ ausente. Execute leitura base.')\n",
    "\n",
    "# Garantir campos essenciais derivados\n",
    "if 'prod_cat1_faltantes' not in base_princ.columns:\n",
    "    raise RuntimeError('Execute a célula de derivação de produtos antes da pipeline final.')\n",
    "\n",
    "# Família pontuação por grupos (Acesso = produtos básicos, Fluxo = cash, SOW = sow_cartao)\n",
    "base_princ['score_acesso'] = base_princ.get('pontos_produtos_basicos', np.nan)\n",
    "base_princ['score_fluxo'] = base_princ.get('cash_total_fator', np.nan)\n",
    "base_princ['score_sow'] = base_princ.get('sow_cartao', np.nan)\n",
    "\n",
    "# Classificação novo associado (flg_novo_assoc ou meses_desde_associacao)\n",
    "base_princ['class_assoc'] = np.where(\n",
    "    (base_princ.get('flg_novo_assoc',0)==1) | (base_princ.get('meses_desde_associacao',999)<=6),\n",
    "    'Novo', 'Base Giro'\n",
    ")\n",
    "\n",
    "# Blocos texto adicionais\n",
    "base_princ['bloco_principalidade'] = base_princ.apply(lambda r: f\"Faixa: {r.get('faixa_categoria')} | Var pts: {r.get('var_pontos')} | Queda flag: {r.get('queda_flag')} | Soma quedas: {r.get('soma_quedas')}\", axis=1)\n",
    "base_princ['bloco_familias'] = base_princ.apply(lambda r: f\"Pontuações -> Acesso: {r.get('score_acesso')} | Fluxo: {r.get('score_fluxo')} | SOW Cartão: {r.get('score_sow')}\", axis=1)\n",
    "base_princ['bloco_tipo_pix'] = base_princ.apply(lambda r: f\"Tipo Pessoa: {r.get('tipo_pessoa')} | Chaves PIX fortes: {r.get('qtde_chaves_pix_fortes')}\", axis=1)\n",
    "base_princ['bloco_assoc'] = base_princ['class_assoc'].apply(lambda v: f\"Classificação Associação: {v}\")\n",
    "\n",
    "# Bloco ISA (recalcular se necessário)\n",
    "base_princ['bloco_isa'] = base_princ.apply(lambda r: (\n",
    "    f\"ISA atual: {r.get('isa_media')} | ISA média 3M: {r.get('isa_media_3m_calc'):.2f}\" if pd.notnull(r.get('isa_media_3m_calc')) else (\n",
    "        f\"ISA atual: {r.get('isa_media')}\" if pd.notnull(r.get('isa_media')) else ''\n",
    "    )\n",
    "), axis=1)\n",
    "\n",
    "base_princ['bloco_prod_basicos'] = base_princ['prod_cat1_faltantes'].apply(lambda lst: ('Produtos básicos a ofertar: ' + ', '.join(lst)) if lst else 'Produtos básicos completos')\n",
    "base_princ['bloco_fluxo'] = base_princ['prod_cat2_metricas'].apply(lambda m: 'Indicadores fluxo / SOW: ' + '; '.join(m))\n",
    "\n",
    "# Título e descrição (incluindo novos blocos)\n",
    "base_princ['titulo'] = base_princ.apply(lambda r: f\"Oferta Principalidade - {r.get('cpf_cnpj')}\", axis=1)\n",
    "base_princ['descricao'] = base_princ.apply(lambda r: ' | '.join([\n",
    "    f\"Segmento: {r.get('segmento')}\",\n",
    "    r.get('bloco_tipo_pix',''),\n",
    "    r.get('bloco_assoc',''),\n",
    "    r.get('bloco_isa',''),\n",
    "    r.get('bloco_principalidade',''),\n",
    "    r.get('bloco_familias',''),\n",
    "    f\"MC Total Atual: {r.get('cash_total')}\",\n",
    "    r.get('bloco_prod_basicos',''),\n",
    "    r.get('bloco_fluxo','')\n",
    "]), axis=1)\n",
    "\n",
    "# Categoria (1 se há produto faltante, senão 2)\n",
    "base_princ['categoriaChamadoId'] = base_princ['prod_cat1_faltantes'].apply(lambda l: 1 if len(l)>0 else 2)\n",
    "base_princ['nomeCategoria'] = base_princ['categoriaChamadoId'].map({1:'Acesso',2:'Fluxo de Caixa'})\n",
    "base_princ['descricaoCategoria'] = base_princ['nomeCategoria'].map({'Acesso':'Ofertar Produtos e Serviços Básicos','Fluxo de Caixa':'Ofertar Movimentações e Uso de Crédito'})\n",
    "\n",
    "# Distribuir datas\n",
    "base_princ = base_princ.sort_values('pontos_principalidade').reset_index(drop=True)\n",
    "qtd = len(base_princ)\n",
    "ini_mes = datetime.today().replace(day=1).date()\n",
    "semanas = [0,7,14,21]\n",
    "base_princ['dataNecessidade'] = base_princ.index.map(lambda i: (ini_mes + timedelta(days=semanas[(i * 4 // (qtd if qtd>0 else 1)) % 4])).isoformat()+'T08:00:00.000Z')\n",
    "\n",
    "base_princ['categoriaChamado'] = base_princ.apply(lambda r: {\n",
    "    'categoriaChamadoId': int(r['categoriaChamadoId']),\n",
    "    'nomeCategoria': r['nomeCategoria'],\n",
    "    'descricaoCategoria': r['descricaoCategoria'],\n",
    "    'dataCriacao': datetime.today().date().isoformat(),\n",
    "    'departamentoId': int(r.get('cod_agencia',0))\n",
    "}, axis=1)\n",
    "\n",
    "# Preparar base_final renombrando columnas esperadas\n",
    "base_final = base_princ.rename(columns={'cod_agencia': 'numeroAgencia', 'nome_agencia': 'nomeDepartamento'})\n",
    "\n",
    "# Responsable / solicitante\n",
    "usuario_default = globals().get('user', '') or globals().get('USER', '')\n",
    "base_final['usuarioResponsavel'] = base_final.get('gestor', usuario_default)\n",
    "base_final['usuarioSolicitante'] = base_final['usuarioResponsavel']\n",
    "base_final['statusChamado'] = 'RASCUNHO'\n",
    "base_final['departamentoId'] = base_final['numeroAgencia']\n",
    "\n",
    "# Selección columnas de salida\n",
    "cols_saida = ['titulo', 'usuarioResponsavel', 'usuarioSolicitante', 'numeroAgencia',\n",
    "             'descricao', 'statusChamado', 'departamentoId', 'nomeDepartamento',\n",
    "             'dataNecessidade', 'categoriaChamado']\n",
    "\n",
    "resultado_chamados = base_final.loc[:, [c for c in cols_saida if c in base_final.columns]].copy()\n",
    "\n",
    "# Salvar arquivos parquet\n",
    "saida_dir = RUTAS.get('saida_dir_giro') if isinstance(RUTAS, dict) else None\n",
    "if saida_dir:\n",
    "    parquet_completo = os.path.join(saida_dir, 'giro_completo_principalidade.parquet')\n",
    "    parquet_chamados = os.path.join(saida_dir, 'giro_chamados_principalidade.parquet')\n",
    "    try:\n",
    "        base_princ.to_parquet(parquet_completo, index=False)\n",
    "        resultado_chamados.to_parquet(parquet_chamados, index=False)\n",
    "        print({'completo': parquet_completo, 'chamados': parquet_chamados, 'linhas_chamados': len(resultado_chamados)})\n",
    "    except Exception as e:\n",
    "        print('Erro ao salvar parquet:', e)\n",
    "else:\n",
    "    print('RUTAS[\"saida_dir_giro\"] não encontrada; não foi possível salvar os arquivos.')\n",
    "\n",
    "resultado_chamados.to_parquet(rf\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar filtros e preparar base principal (usa base_princ já carregada do parquet associados_queda_principalidade)\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ não carregada. Execute a célula de leitura do parquet base antes desta.')\n",
    "\n",
    "if base_princ.empty:\n",
    "    raise ValueError('base_princ está vazia. Verifique o parquet base.')\n",
    "\n",
    "# Normalizar chave\n",
    "def normaliza_doc(valor):\n",
    "    if pd.isna(valor):\n",
    "        return valor\n",
    "    v = re.sub(r'\\D','', str(valor))\n",
    "    return v.lstrip('0')\n",
    "\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    possiveis_chaves = [c for c in base_princ.columns if 'cpf' in c.lower() or 'cnpj' in c.lower() or 'doc' in c.lower()]\n",
    "    if possiveis_chaves:\n",
    "        base_princ.rename(columns={possiveis_chaves[0]:'cpf_cnpj'}, inplace=True)\n",
    "if 'cpf_cnpj' not in base_princ.columns:\n",
    "    raise ValueError('Coluna cpf_cnpj ausente na base principalidade mês.')\n",
    "base_princ['cpf_cnpj'] = base_princ['cpf_cnpj'].map(normaliza_doc)\n",
    "\n",
    "# Inadimplentes\n",
    "path_inad = RUTAS['inadimplentes']\n",
    "if os.path.exists(path_inad):\n",
    "    inad = pd.read_parquet(path_inad)\n",
    "    for alt in ['cpf','cnpj','documento','doc','num_cpf_cnpj']:\n",
    "        if alt in inad.columns and 'cpf_cnpj' not in inad.columns:\n",
    "            inad.rename(columns={alt:'cpf_cnpj'}, inplace=True)\n",
    "    if 'cpf_cnpj' not in inad.columns:\n",
    "        inad['cpf_cnpj'] = inad.iloc[:,0]\n",
    "    inad['cpf_cnpj'] = inad['cpf_cnpj'].map(normaliza_doc)\n",
    "    inad = inad[['cpf_cnpj']].drop_duplicates()\n",
    "    inad['flag_inadimplente'] = 1\n",
    "    base_princ = base_princ.merge(inad, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['flag_inadimplente'] = np.nan\n",
    "\n",
    "# Risco BBM (normalizar caixa / acentuação mínima)\n",
    "if 'nivel_risco' in base_princ.columns:\n",
    "    base_princ['nivel_risco_norm'] = base_princ['nivel_risco'].astype(str).str.upper().str.strip()\n",
    "    riscos_ok = [r.upper() for r in PARAMS['filtros_risco_bbm']]\n",
    "    base_princ = base_princ[base_princ['nivel_risco_norm'].isin(riscos_ok)]\n",
    "else:\n",
    "    print('Aviso: coluna nivel_risco ausente.')\n",
    "\n",
    "# Sem inadimplência\n",
    "base_princ = base_princ[(base_princ['flag_inadimplente'].isna()) | (base_princ['flag_inadimplente']!=1)]\n",
    "\n",
    "# Merge dashboard (inclui ult_movimento)\n",
    "if 'assoc_dash' in globals():\n",
    "    assoc_dash['cpf_cnpj'] = assoc_dash['cpf_cnpj'].map(normaliza_doc)\n",
    "    base_princ = base_princ.merge(assoc_dash, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    print('Dashboard não carregado: execute a célula de carga do dashboard/ISA antes desta.')\n",
    "\n",
    "# Filtro movimento 20-45 dias usando ult_movimento\n",
    "if 'ult_movimento' in base_princ.columns:\n",
    "    hoje = pd.Timestamp.today().normalize()\n",
    "    base_princ['ult_movimento_dt'] = pd.to_datetime(base_princ['ult_movimento'], errors='coerce')\n",
    "    base_princ['dias_sem_mov'] = (hoje - base_princ['ult_movimento_dt']).dt.days\n",
    "    lim_inf, lim_sup = PARAMS['dias_sem_movimentacao']\n",
    "    base_princ = base_princ[(base_princ['dias_sem_mov']>=lim_inf) & (base_princ['dias_sem_mov']<=lim_sup)]\n",
    "else:\n",
    "    print('Coluna ult_movimento ausente no dashboard para filtro de movimento.')\n",
    "\n",
    "# Merge ISA se disponível\n",
    "if 'isa_sel' in globals() and isinstance(isa_sel, pd.DataFrame) and not isa_sel.empty and 'cpf_cnpj' in isa_sel.columns:\n",
    "    isa_sel['cpf_cnpj'] = isa_sel['cpf_cnpj'].map(normaliza_doc)\n",
    "    if 'gestor' in base_princ.columns and 'gestor' in isa_sel.columns:\n",
    "        base_princ = base_princ.merge(isa_sel, on=['cpf_cnpj','gestor'], how='left')\n",
    "    else:\n",
    "        base_princ = base_princ.merge(isa_sel, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    base_princ['isa_media_3m_calc'] = np.nan\n",
    "\n",
    "# Corte ISA >= 2 (média atual ou média 3M). Remove quem está fora.\n",
    "if 'isa_media' in base_princ.columns:\n",
    "    base_princ = base_princ[(base_princ['isa_media'].fillna(0) >= 2) | (base_princ.get('isa_media_3m_calc', pd.Series(dtype=float)).fillna(0) >= 2)]\n",
    "\n",
    "print('Base após filtros (inclui corte ISA>=2):', base_princ.shape)\n",
    "base_princ.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar parquets: completo (dados filtrados) e card Kanban\n",
    "import json\n",
    "\n",
    "if 'base_princ' not in globals() or 'resultado_chamados' not in globals():\n",
    "    raise RuntimeError('Execute as células anteriores antes de gerar os parquets finais.')\n",
    "\n",
    "# 1. Parquet completo já salvo anteriormente, mas reforçamos aqui se desejar atualizar\n",
    "parquet_completo = os.path.join(RUTAS['saida_dir_giro'], 'giro_completo_principalidade.parquet')\n",
    "base_princ.to_parquet(parquet_completo, index=False)\n",
    "\n",
    "# 2. Parquet apenas com campos necessários para o card (Kanban)\n",
    "cols_card = [\n",
    "    'titulo',\n",
    "    'descricao',\n",
    "    'categoriaChamado',\n",
    "    'usuarioResponsavel',\n",
    "    'usuarioSolicitante',\n",
    "    'dataNecessidade',\n",
    "    'statusChamado',\n",
    "    'numeroAgencia',\n",
    "    'nomeDepartamento',\n",
    "    'departamentoId'\n",
    "]\n",
    "card_df = resultado_chamados[cols_card].copy()\n",
    "\n",
    "# Opcional: serializar categoriaChamado em JSON para consumo externo se necessário\n",
    "card_df['categoriaChamado_json'] = card_df['categoriaChamado'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "\n",
    "parquet_card = os.path.join(RUTAS['saida_dir_giro'], 'giro_cards_kanban.parquet')\n",
    "card_df.to_parquet(parquet_card, index=False)\n",
    "\n",
    "print({'parquet_completo': parquet_completo, 'parquet_card': parquet_card, 'linhas_card': len(card_df)})\n",
    "card_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5016dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE FINAL UNIFICADA\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError('base_princ ausente. Execute leitura base.')\n",
    "\n",
    "# Garantir campos essenciais derivados\n",
    "if 'prod_cat1_faltantes' not in base_princ.columns:\n",
    "    raise RuntimeError('Execute a célula de derivação de produtos antes da pipeline final.')\n",
    "\n",
    "# Família pontuação por grupos (Acesso = produtos básicos, Fluxo = cash, SOW = sow_cartao)\n",
    "base_princ['score_acesso'] = base_princ.get('pontos_produtos_basicos', np.nan)\n",
    "base_princ['score_fluxo'] = base_princ.get('cash_total_fator', np.nan)\n",
    "base_princ['score_sow'] = base_princ.get('sow_cartao', np.nan)\n",
    "\n",
    "# Classificação novo associado (flg_novo_assoc ou meses_desde_associacao)\n",
    "base_princ['class_assoc'] = np.where(\n",
    "    (base_princ.get('flg_novo_assoc',0)==1) | (base_princ.get('meses_desde_associacao',999)<=6),\n",
    "    'Novo', 'Base Giro'\n",
    ")\n",
    "\n",
    "# Blocos texto adicionais\n",
    "base_princ['bloco_principalidade'] = base_princ.apply(lambda r: f\"Faixa: {r.get('faixa_categoria')} | Var pts: {r.get('var_pontos')} | Queda flag: {r.get('queda_flag')} | Soma quedas: {r.get('soma_quedas')}\", axis=1)\n",
    "base_princ['bloco_familias'] = base_princ.apply(lambda r: f\"Pontuações -> Acesso: {r.get('score_acesso')} | Fluxo: {r.get('score_fluxo')} | SOW Cartão: {r.get('score_sow')}\", axis=1)\n",
    "base_princ['bloco_tipo_pix'] = base_princ.apply(lambda r: f\"Tipo Pessoa: {r.get('tipo_pessoa')} | Chaves PIX fortes: {r.get('qtde_chaves_pix_fortes')}\", axis=1)\n",
    "base_princ['bloco_assoc'] = base_princ['class_assoc'].apply(lambda v: f\"Classificação Associação: {v}\")\n",
    "\n",
    "# Bloco ISA (recalcular se necessário)\n",
    "base_princ['bloco_isa'] = base_princ.apply(lambda r: (\n",
    "    f\"ISA atual: {r.get('isa_media')} | ISA média 3M: {r.get('isa_media_3m_calc'):.2f}\" if pd.notnull(r.get('isa_media_3m_calc')) else (\n",
    "        f\"ISA atual: {r.get('isa_media')}\" if pd.notnull(r.get('isa_media')) else ''\n",
    "    )\n",
    "), axis=1)\n",
    "\n",
    "base_princ['bloco_prod_basicos'] = base_princ['prod_cat1_faltantes'].apply(lambda lst: ('Produtos básicos a ofertar: ' + ', '.join(lst)) if lst else 'Produtos básicos completos')\n",
    "base_princ['bloco_fluxo'] = base_princ['prod_cat2_metricas'].apply(lambda m: 'Indicadores fluxo / SOW: ' + '; '.join(m))\n",
    "\n",
    "# Título e descrição (incluindo novos blocos)\n",
    "base_princ['titulo'] = base_princ.apply(lambda r: f\"Oferta Principalidade - {r.get('cpf_cnpj')}\", axis=1)\n",
    "base_princ['descricao'] = base_princ.apply(lambda r: ' | '.join([\n",
    "    f\"Segmento: {r.get('segmento')}\",\n",
    "    r.get('bloco_tipo_pix',''),\n",
    "    r.get('bloco_assoc',''),\n",
    "    r.get('bloco_isa',''),\n",
    "    r.get('bloco_principalidade',''),\n",
    "    r.get('bloco_familias',''),\n",
    "    f\"MC Total Atual: {r.get('cash_total')}\",\n",
    "    r.get('bloco_prod_basicos',''),\n",
    "    r.get('bloco_fluxo','')\n",
    "]), axis=1)\n",
    "\n",
    "# Categoria (1 se há produto faltante, senão 2)\n",
    "base_princ['categoriaChamadoId'] = base_princ['prod_cat1_faltantes'].apply(lambda l: 1 if len(l)>0 else 2)\n",
    "base_princ['nomeCategoria'] = base_princ['categoriaChamadoId'].map({1:'Acesso',2:'Fluxo de Caixa'})\n",
    "base_princ['descricaoCategoria'] = base_princ['nomeCategoria'].map({'Acesso':'Ofertar Produtos e Serviços Básicos','Fluxo de Caixa':'Ofertar Movimentações e Uso de Crédito'})\n",
    "\n",
    "# Distribuir datas\n",
    "base_princ = base_princ.sort_values('pontos_principalidade').reset_index(drop=True)\n",
    "qtd = len(base_princ)\n",
    "ini_mes = datetime.today().replace(day=1).date()\n",
    "semanas = [0,7,14,21]\n",
    "base_princ['dataNecessidade'] = base_princ.index.map(lambda i: (ini_mes + timedelta(days=semanas[(i * 4 // (qtd if qtd>0 else 1)) % 4])).isoformat()+'T08:00:00.000Z')\n",
    "\n",
    "base_princ['categoriaChamado'] = base_princ.apply(lambda r: {\n",
    "    'categoriaChamadoId': int(r['categoriaChamadoId']),\n",
    "    'nomeCategoria': r['nomeCategoria'],\n",
    "    'descricaoCategoria': r['descricaoCategoria'],\n",
    "    'dataCriacao': datetime.today().date().isoformat(),\n",
    "    'departamentoId': int(r.get('cod_agencia',0))\n",
    "}, axis=1)\n",
    "\n",
    "# Preparar base_final renombrando columnas esperadas\n",
    "base_final = base_princ.rename(columns={'cod_agencia': 'numeroAgencia', 'nome_agencia': 'nomeDepartamento'})\n",
    "\n",
    "# Responsable / solicitante\n",
    "usuario_default = globals().get('user', '') or globals().get('USER', '')\n",
    "base_final['usuarioResponsavel'] = base_final.get('gestor', usuario_default)\n",
    "base_final['usuarioSolicitante'] = base_final['usuarioResponsavel']\n",
    "base_final['statusChamado'] = 'RASCUNHO'\n",
    "base_final['departamentoId'] = base_final['numeroAgencia']\n",
    "\n",
    "# Selección columnas de salida\n",
    "cols_saida = ['titulo', 'usuarioResponsavel', 'usuarioSolicitante', 'numeroAgencia',\n",
    "             'descricao', 'statusChamado', 'departamentoId', 'nomeDepartamento',\n",
    "             'dataNecessidade', 'categoriaChamado']\n",
    "\n",
    "resultado_chamados = base_final.loc[:, [c for c in cols_saida if c in base_final.columns]].copy()\n",
    "\n",
    "# Salvar arquivos parquet\n",
    "saida_dir = RUTAS.get('saida_dir_giro') if isinstance(RUTAS, dict) else None\n",
    "if saida_dir:\n",
    "    parquet_completo = os.path.join(saida_dir, 'giro_completo_principalidade.parquet')\n",
    "    parquet_chamados = os.path.join(saida_dir, 'giro_chamados_principalidade.parquet')\n",
    "    try:\n",
    "        base_princ.to_parquet(parquet_completo, index=False)\n",
    "        resultado_chamados.to_parquet(parquet_chamados, index=False)\n",
    "        print({'completo': parquet_completo, 'chamados': parquet_chamados, 'linhas_chamados': len(resultado_chamados)})\n",
    "    except Exception as e:\n",
    "        print('Erro ao salvar parquet:', e)\n",
    "else:\n",
    "    print('RUTAS[\"saida_dir_giro\"] não encontrada; não foi possível salvar os arquivos.')\n",
    "\n",
    "resultado_chamados.to_parquet(rf\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a945f1",
   "metadata": {},
   "source": [
    "{  \n",
    "  \"titulo\": \"string\",    \n",
    "  \"usuarioResponsavel\": \"string\", ----> GESTOR\n",
    "  \"usuarioSolicitante\": \"string\", -----> \n",
    "  \"numeroAgencia\": 0, -----> cod_agencia\n",
    "  \"descricao\": \"string\",  --------> descricao texto com detalhes e indices do associado + o texto da oferta de produtos a entregar\n",
    "  \"statusChamado\": \"RASCUNHO\", \n",
    "  \"departamentoId\": 0,\n",
    "  \"nomeDepartamento\": \"string\", --------->  _nom_agencia\n",
    "  \"dataNecessidade\": \"2025-08-14T11:45:45.991Z\",\n",
    "  \"categoriaChamado\": {\n",
    "    \"categoriaChamadoId\": 0,  ---- > 1 ou 2\n",
    "    \"nomeCategoria\": \"string\", ------> \n",
    "    \"descricaoCategoria\": \"string\", \n",
    "    \"dataCriacao\": \"2025-08-14\",\n",
    "    \"departamentoId\": 0}\n",
    "}\n",
    "\n",
    "Titulo () + nome associado + cpf \n",
    "\n",
    "Descrição:\n",
    "PF OU PJ\n",
    "ISA atual \n",
    "ISA 3M\n",
    "MC TOTAL ATUAL\n",
    "MC 6M\n",
    "INDICES PRINCIPALIDADE\n",
    "\n",
    "OFERTA : TEXTO conformado por  categoria + descriçao categoria + produto  1 texto : \n",
    "\n",
    "  Oferta Acesso  ----->  Categoria 1 \n",
    "(descriçao categoria) Ofertar Produtos/Serviços Básicos : {} ou [] {add. Produto}-------> Titulo + nome associado + cpf  \n",
    ". Chave PIX ativa \n",
    "· Débito automático\n",
    "· Cartão de débito\n",
    "· Cartão de crédito\n",
    ". Canais digitais\n",
    ". Credenciamento ativo\n",
    "· Cobrança\n",
    ". Folha de Pagamento\n",
    ". Domicílio\n",
    ". Open Finance (receptor)\n",
    "\n",
    " Oferta Fluxo de Caixa   ----->  Categoria 2 \n",
    "if % produto in range \n",
    "(descriçao categoria) Ofertar Movimentações : [] ou {} {add. Crédito &| cartão}-------> Titulo + nome associado + cpf\n",
    ". Somatório do total de movimentações:\n",
    "- Receber (Cash-in)\n",
    "- Pagar (Cash-out)\n",
    "\n",
    "Crédito - cartão\n",
    ". SOW (Share of Wallet)\n",
    "referente à cartão\n",
    "\n",
    "Credito - outros\n",
    "SOW (Share of Wallet)\n",
    "referente à créditos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda final: merge inadimplencia + dashboard, salvar parquets (con telefone/gestor), teste CPF 06295052924 y analise resumen\n",
    "import os, re, json, pandas as pd, numpy as np\n",
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "# Normalizar documento\n",
    "def _norm_doc(v):\n",
    "    if pd.isna(v): return v\n",
    "    return re.sub(r'\\D','', str(v)).lstrip('0')\n",
    "\n",
    "# Cargar inadimplentes si existe\n",
    "inad_path = RUTAS.get('inadimplentes') if isinstance(RUTAS, dict) else None\n",
    "if inad_path and os.path.exists(inad_path):\n",
    "    inad = pd.read_parquet(inad_path)\n",
    "    if 'cpf_cnpj' not in inad.columns:\n",
    "        for alt in ['cpf','cnpj','documento','doc','num_cpf_cnpj']:\n",
    "            if alt in inad.columns:\n",
    "                inad = inad.rename(columns={alt:'cpf_cnpj'})\n",
    "                break\n",
    "    inad['cpf_cnpj'] = inad['cpf_cnpj'].map(_norm_doc)\n",
    "    inad = inad[['cpf_cnpj']].drop_duplicates()\n",
    "    inad['flag_inadimplente'] = 1\n",
    "else:\n",
    "    inad = pd.DataFrame(columns=['cpf_cnpj','flag_inadimplente'])\n",
    "\n",
    "# Asegurar assoc_dash (telefono y gestor)\n",
    "dash_path = rf\"C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\\associados_totais_tratados_dashboard.parquet\"\n",
    "if os.path.exists(dash_path):\n",
    "    try:\n",
    "        assoc_dash = pd.read_parquet(dash_path, columns=['cpf_cnpj','nome_agencia','telefone','gestor','gestor_ldap'])\n",
    "    except Exception:\n",
    "        assoc_dash = pd.read_parquet(dash_path, columns=['cpf_cnpj','nome_agencia','telefone','gestor'])\n",
    "else:\n",
    "    assoc_dash = pd.DataFrame(columns=['cpf_cnpj','nome_agencia','telefone','gestor','gestor_ldap'])\n",
    "\n",
    "assoc_dash['cpf_cnpj'] = assoc_dash['cpf_cnpj'].map(_norm_doc)\n",
    "\n",
    "# Preparar working copy de base_princ\n",
    "if 'base_princ' not in globals():\n",
    "    raise RuntimeError(\"base_princ no cargada.\")\n",
    "bp = base_princ.copy()\n",
    "if 'cpf_cnpj' in bp.columns:\n",
    "    bp['cpf_cnpj'] = bp['cpf_cnpj'].map(_norm_doc)\n",
    "\n",
    "# Merge inadimplencia y assoc_dash\n",
    "bp = bp.merge(inad, on='cpf_cnpj', how='left')\n",
    "if not assoc_dash.empty:\n",
    "    assoc_sel = assoc_dash[['cpf_cnpj','telefone'] + ([c for c in ['gestor_ldap','gestor','nome_agencia'] if c in assoc_dash.columns])]\n",
    "    bp = bp.merge(assoc_sel.drop_duplicates('cpf_cnpj'), on='cpf_cnpj', how='left')\n",
    "\n",
    "# asegurar gestor_ldap\n",
    "if 'gestor_ldap' not in bp.columns:\n",
    "    bp['gestor_ldap'] = bp.get('gestor', None)\n",
    "\n",
    "# Filtrar dias sem movimento (20-45) si ult_movimento existe\n",
    "lim_inf, lim_sup = PARAMS.get('dias_sem_movimentacao', (20,45))\n",
    "if 'ult_movimento' in bp.columns:\n",
    "    hoje = pd.Timestamp.today().normalize()\n",
    "    bp['ult_movimento_dt'] = pd.to_datetime(bp['ult_movimento'], errors='coerce')\n",
    "    bp['dias_sem_mov'] = (hoje - bp['ult_movimento_dt']).dt.days\n",
    "    bp = bp[(bp['dias_sem_mov']>=lim_inf) & (bp['dias_sem_mov']<=lim_sup)]\n",
    "else:\n",
    "    if 'dias_sem_mov' in bp.columns:\n",
    "        bp = bp[(bp['dias_sem_mov']>=lim_inf) & (bp['dias_sem_mov']<=lim_sup)]\n",
    "    else:\n",
    "        print(\"Aviso: no hay campo de último movimiento para filtrar.\")\n",
    "\n",
    "# Remover inadimplentes\n",
    "if 'flag_inadimplente' in bp.columns:\n",
    "    bp = bp[(bp['flag_inadimplente'].isna()) | (bp['flag_inadimplente']!=1)]\n",
    "\n",
    "# Guardar parquet completo atómico (incluir telefone y gestor_ldap)\n",
    "saida_dir = RUTAS.get('saida_dir_giro')\n",
    "if not saida_dir:\n",
    "    raise RuntimeError(\"RUTAS['saida_dir_giro'] no definido.\")\n",
    "full_path = os.path.join(saida_dir, 'giro_completo_principalidade.parquet')\n",
    "tmp_full = full_path + '.tmp'\n",
    "# columnas a conservar: intersección de interes\n",
    "core_cols = ['cpf_cnpj','ano_mes','segmento','cod_agencia','pontos_principalidade','var_pontos','queda_flag','soma_quedas','sow_cartao','cash_total','isa_media','isa_media_3m_calc']\n",
    "keep_final = [c for c in core_cols if c in bp.columns]\n",
    "for add in ['telefone','gestor_ldap','descricao_medidas','insight_medidas']:\n",
    "    if add in bp.columns and add not in keep_final:\n",
    "        keep_final.append(add)\n",
    "bp.loc[:, keep_final].to_parquet(tmp_full, index=False)\n",
    "os.replace(tmp_full, full_path)\n",
    "print(\"Guardado parquet completo:\", full_path, \"| filas:\", len(bp))\n",
    "\n",
    "# Construir resultado_chamados (kanban) con telefone/gestor y salvar\n",
    "if 'resultado_chamados' in globals() and isinstance(resultado_chamados, pd.DataFrame) and not resultado_chamados.empty:\n",
    "    kanban_df = resultado_chamados.copy()\n",
    "else:\n",
    "    kanban_df = pd.DataFrame({\n",
    "        'cpf_cnpj': bp['cpf_cnpj'],\n",
    "        'titulo': bp.get('titulo', bp['cpf_cnpj']),\n",
    "        'descricao': bp.get('descricao',''),\n",
    "        'usuarioResponsavel': bp.get('gestor_ldap', None),\n",
    "        'usuarioSolicitante': bp.get('gestor_ldap', None),\n",
    "        'numeroAgencia': bp.get('cod_agencia', None),\n",
    "        'nomeDepartamento': bp.get('nome_agencia', None),\n",
    "        'dataNecessidade': bp.get('dataNecessidade', None),\n",
    "        'statusChamado': bp.get('statusChamado', 'RASCUNHO'),\n",
    "        'departamentoId': bp.get('cod_agencia', None)\n",
    "    })\n",
    "\n",
    "if 'cpf_cnpj' in kanban_df.columns and 'cpf_cnpj' in bp.columns:\n",
    "    mapa = bp[['cpf_cnpj','telefone','gestor_ldap']].drop_duplicates('cpf_cnpj')\n",
    "    kanban_df = kanban_df.merge(mapa, on='cpf_cnpj', how='left')\n",
    "else:\n",
    "    kanban_df['telefone'] = None\n",
    "    kanban_df['gestor_ldap'] = None\n",
    "\n",
    "kanban_df['categoriaChamado_json'] = kanban_df.get('categoriaChamado').apply(lambda x: json.dumps(x, ensure_ascii=False) if pd.notnull(x) else None) if 'categoriaChamado' in kanban_df.columns else None\n",
    "\n",
    "kanban_path = os.path.join(saida_dir, 'giro_cards_kanban.parquet')\n",
    "tmp_kanban = kanban_path + '.tmp'\n",
    "kanban_df.to_parquet(tmp_kanban, index=False)\n",
    "os.replace(tmp_kanban, kanban_path)\n",
    "print(\"Guardado Kanban:\", kanban_path, \"| filas:\", len(kanban_df))\n",
    "\n",
    "# Test CPF: generar markdown para 06295052924\n",
    "cpf_test = _norm_doc('06295052924')\n",
    "sel = bp[bp['cpf_cnpj']==cpf_test]\n",
    "if sel.empty:\n",
    "    print(f\"CPF {cpf_test} no encontrado en la base filtrada. Mostrar primer registro como ejemplo.\")\n",
    "    sel = bp.head(1)\n",
    "r = sel.iloc[0]\n",
    "\n",
    "lines = []\n",
    "lines.append(f\"# Ejemplo CPF: {r.get('cpf_cnpj','')}\")\n",
    "lines.append(f\"- Nome: {r.get('nom_associado','')}\")\n",
    "lines.append(f\"- Gestor (ldap): {r.get('gestor_ldap','')}\")\n",
    "lines.append(f\"- Telefones: {r.get('telefone','')}\")\n",
    "lines.append(f\"- Segmento: {r.get('segmento','')}\")\n",
    "lines.append(f\"- ISA atual: {r.get('isa_media','')} | ISA 3M: {r.get('isa_media_3m_calc','')}\")\n",
    "lines.append(f\"- MC Total: {r.get('cash_total','')}\")\n",
    "lines.append(\"## Indices e variações\")\n",
    "for c in [col for col in bp.columns if col.startswith('delta_') or col.endswith('_flag') or col.startswith('soma_')]:\n",
    "    lines.append(f\"- {c}: {r.get(c)}\")\n",
    "lines.append(\"## Productos faltantes:\")\n",
    "lines.append(f\"- {r.get('prod_cat1_faltantes', [])}\")\n",
    "md_text = \"\\n\".join(lines)\n",
    "print(md_text)\n",
    "\n",
    "# Analisis final: conteos y balances\n",
    "summary = {}\n",
    "summary['total_filtrados'] = len(bp)\n",
    "summary['perdieron_puntos_total'] = int(((bp.get('var_pontos',0) < 0)).sum()) if 'var_pontos' in bp.columns else 0\n",
    "if 'dias_sem_mov' in bp.columns and 'var_pontos' in bp.columns:\n",
    "    summary['perdieron_puntos_20_45'] = int(((bp['var_pontos']<0) & (bp['dias_sem_mov']>=lim_inf) & (bp['dias_sem_mov']<=lim_sup)).sum())\n",
    "else:\n",
    "    summary['perdieron_puntos_20_45'] = None\n",
    "\n",
    "# ISA balance: columnas isa_YYYYMM o isa_media/isa_media_3m_calc\n",
    "isa_cols = [c for c in bp.columns if re.match(r'isa_\\d{6}$', c)]\n",
    "if isa_cols:\n",
    "    last3 = sorted(isa_cols)[-3:]\n",
    "    bp['isa_3m_mean_calc'] = bp[last3].mean(axis=1)\n",
    "    summary['isa_3m_media_promedio'] = float(bp['isa_3m_mean_calc'].mean())\n",
    "else:\n",
    "    summary['isa_3m_media_promedio'] = float(bp['isa_media_3m_calc'].dropna().mean()) if 'isa_media_3m_calc' in bp.columns else None\n",
    "\n",
    "# Variaciones de productos (deltas)\n",
    "delta_cols = [c for c in bp.columns if c.startswith('delta_')]\n",
    "if delta_cols:\n",
    "    summary['delta_neg_counts'] = (bp[delta_cols] < 0).sum().to_dict()\n",
    "else:\n",
    "    summary['delta_neg_counts'] = {}\n",
    "\n",
    "# soma_lost_produtos agregado\n",
    "summary['soma_lost_produtos_total'] = int(bp['soma_lost_produtos'].sum()) if 'soma_lost_produtos' in bp.columns else None\n",
    "\n",
    "# Guardar resumen\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_path = os.path.join(saida_dir, 'giro_analise_resumo.parquet')\n",
    "tmp_sum = summary_path + '.tmp'\n",
    "summary_df.to_parquet(tmp_sum, index=False)\n",
    "os.replace(tmp_sum, summary_path)\n",
    "print(\"Resumen guardado:\", summary_path)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
