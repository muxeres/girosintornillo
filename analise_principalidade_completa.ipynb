{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be16e0d2",
   "metadata": {},
   "source": [
    "# An√°lise Completa de Principalidade - Dashboard BI0730\n",
    "\n",
    "Este notebook cont√©m uma an√°lise completa dos dados de principalidade, incluindo:\n",
    "\n",
    "- An√°lise de quantidade de contas (PF/PJ) e ag√™ncias\n",
    "- An√°lise BBM vs n√£o-BBM\n",
    "- Evolu√ß√£o da principalidade nos √∫ltimos 3 meses\n",
    "- Uso de produtos e diferen√ßas de ISA\n",
    "- Chave PIX e chave forte\n",
    "- Diferen√ßa entre cash-in e cash-out\n",
    "- Produtos faltantes e an√°lise de categorias\n",
    "- SOW Sicredi\n",
    "- An√°lise temporal de movimenta√ß√£o\n",
    "- Pr√©-churn com Machine Learning\n",
    "- Gr√°ficos e visualiza√ß√µes para apresenta√ß√£o\n",
    "\n",
    "**Data de Execu√ß√£o**: Agosto 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√£o de bibliotecas necess√°rias\n",
    "import os\n",
    "import sys\n",
    "import duckdb\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, date, timedelta\n",
    "import json\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "import pyarrow.parquet as pq  # <- necess√°rio para read_schema\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "# Configura√ß√µes do sistema\n",
    "sys.path.append(rf\"C:\\Git\\BI0730\")\n",
    "from libs.geral.utils import * \n",
    "\n",
    "USER = os.getlogin()\n",
    "ANO = 2025\n",
    "\n",
    "# Fun√ß√£o utilit√°ria simples de merge + coalesce (SEM tratamentos extras)\n",
    "def merge_and_coalesce(df_left, df_right, on: str, cols_to_merge, how='left'):\n",
    "    \"\"\"Merge b√°sico e para cada coluna em cols_to_merge (se existir no right) preenche nulos do left.\n",
    "    N√£o cria colunas novas al√©m do necess√°rio e n√£o aplica nenhuma infer√™ncia.\n",
    "    \"\"\"\n",
    "    if df_right is None or isinstance(df_right, pd.DataFrame) and df_right.empty:\n",
    "        print('merge_and_coalesce: df_right vazio -> retorno df_left')\n",
    "        return df_left\n",
    "    if not isinstance(cols_to_merge, (list, tuple)):\n",
    "        cols_to_merge = []\n",
    "    disponiveis = [c for c in cols_to_merge if c in df_right.columns and c != on]\n",
    "    if not disponiveis:\n",
    "        print('merge_and_coalesce: nenhuma coluna dispon√≠vel para mesclar')\n",
    "        return df_left\n",
    "    tmp = df_right[[on] + disponiveis].copy()\n",
    "    out = df_left.merge(tmp, on=on, how=how, suffixes=('', '_y'))\n",
    "    for col in disponiveis:\n",
    "        cy = f'{col}_y'\n",
    "        if cy in out.columns:\n",
    "            if col in out.columns:\n",
    "                out[col] = out[col].where(out[col].notna(), out[cy])\n",
    "            else:\n",
    "                out[col] = out[cy]\n",
    "            out.drop(columns=[cy], inplace=True, errors='ignore')\n",
    "    return out\n",
    "\n",
    "# Rutas e configura√ß√µes baseadas no notebook original\n",
    "PATH_BASES_PARQUET = rf\"C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\"\n",
    "\n",
    "RUTAS = {\n",
    "    'inadimplentes': os.path.join(PATH_BASES_PARQUET, 'base_inadimplentes.parquet'),\n",
    "    'associados_dir': os.path.join(PATH_BASES_PARQUET, 'associados_total_diario'),\n",
    "    'dashboard': rf\"C:\\Users\\carola_luco\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\\associados_totais_tratados_dashboard.parquet\",\n",
    "    'saida_dir': rf'C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira',\n",
    "    'saida_dir_giro': rf'C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira',\n",
    "    'principalidade': rf\"C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\\cia_pcp_indicador_principalidade_historico\\2025*.parquet\"\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'ano': ANO,\n",
    "    'pasta_isa_historico': rf\"C:\\Users\\carola_luco\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\isa_historicos_extracao\",\n",
    "    'filtros_risco_bbm': [\"BAIX√çSSIMO\", \"BAIXO 1\", \"BAIXO 2\", \"M√âDIO 1\", \"M√âDIO 2\"],\n",
    "    'dias_sem_movimentacao': (20, 45)\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    'filtros_risco_bbm': [\"BAIX√çSSIMO\", \"BAIXO 1\", \"BAIXO 2\", \"M√âDIO 1\", \"M√âDIO 2\"],\n",
    "    'dias_sem_movimentacao': (20, 45),\n",
    "    'principalidade': ['sow']\n",
    "}\n",
    "\n",
    "print(\"‚úì Configura√ß√£o inicial conclu√≠da\")\n",
    "print(f\"Usu√°rio: {USER}\")\n",
    "print(f\"Ano de an√°lise: {ANO}\")\n",
    "print(f\"Data de execu√ß√£o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7a35f2",
   "metadata": {},
   "source": [
    "## 1. Carga de Dados Base e Merges\n",
    "\n",
    "Carregamento dos parquets principais: principalidade hist√≥rico, dashboard e ISA hist√≥rico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar conex√£o DuckDB e carregar dados de principalidade (SEM QUALQUER NORMALIZA√á√ÉO / PADRONIZA√á√ÉO)\n",
    "con = duckdb.connect()\n",
    "\n",
    "# ATEN√á√ÉO: Conforme solicita√ß√£o do usu√°rio TODA normaliza√ß√£o autom√°tica (zeros √† esquerda, renomeios, infer√™ncias)\n",
    "# FOI REMOVIDA. A base √© carregada exatamente como armazenada no parquet. Qualquer tratamento de cpf/cnpj\n",
    "# dever√° ser feito manualmente fora deste notebook ou em c√©lulas espec√≠ficas que o usu√°rio controlar.\n",
    "\n",
    "path_principalidade = RUTAS['principalidade']\n",
    "print(f\"Carregando dados de principalidade (bruto): {path_principalidade}\")\n",
    "\n",
    "query_principalidade = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{path_principalidade}')\n",
    "\"\"\"\n",
    "\n",
    "df_principalidade = con.sql(query_principalidade).fetchdf()\n",
    "\n",
    "print(f\"‚úì Dados de principalidade carregados (sem transforma√ß√£o): {len(df_principalidade):,} registros\")\n",
    "print(f\"  Colunas dispon√≠veis: {len(df_principalidade.columns)}\")\n",
    "if 'ano_mes' in df_principalidade.columns:\n",
    "    print(f\"  Per√≠odo: {df_principalidade['ano_mes'].min()} at√© {df_principalidade['ano_mes'].max()}\")\n",
    "\n",
    "print(\"\\nEstrutura (dtypes amostra):\")\n",
    "print(df_principalidade.dtypes.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados do dashboard (SEM normaliza√ß√£o / sem zfill)\n",
    "path_dashboard = RUTAS.get('dashboard')\n",
    "\n",
    "print(f\"Carregando dashboard bruto: {path_dashboard}\")\n",
    "\n",
    "# Ler esquema para validar colunas dispon√≠veis\n",
    "schema_cols = list(pq.read_schema(path_dashboard).names)\n",
    "\n",
    "# Colunas explicitamente permitidas (adicionados: pix_trans_30d, ult_movimento)\n",
    "base_cols = [\n",
    "    'cpf_cnpj','nome_agencia','mc_total_1','gestor','cod_agencia','num_conta',\n",
    "    'cod_carteira','tipo_pessoa','segmento','segmento_cliente',\n",
    "    'pix_trans_30d','ult_movimento'  # <- NOVO\n",
    "]\n",
    "cols_to_read = [c for c in base_cols if c in schema_cols]\n",
    "\n",
    "assoc_dash = pd.read_parquet(path_dashboard, columns=cols_to_read)\n",
    "\n",
    "# √öNICO rename permitido: segmento_cliente -> segmento (se segmento n√£o existir)\n",
    "if 'segmento_cliente' in assoc_dash.columns and 'segmento' not in assoc_dash.columns:\n",
    "    assoc_dash = assoc_dash.rename(columns={'segmento_cliente':'segmento'})\n",
    "\n",
    "print(f\"‚úì Dashboard carregado: {len(assoc_dash):,} registros\")\n",
    "print(\"Colunas:\", list(assoc_dash.columns))\n",
    "# Verifica√ß√£o r√°pida dos novos campos\n",
    "for novo_col in ['pix_trans_30d','ult_movimento']:\n",
    "    if novo_col not in assoc_dash.columns:\n",
    "        print(f\"‚ö†Ô∏è  Campo {novo_col} n√£o dispon√≠vel no dashboard.\")\n",
    "assoc_dash.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar ISA hist√≥rico (SEM normaliza√ß√£o autom√°tica)\n",
    "isa_dir = config['pasta_isa_historico']\n",
    "isa_files = sorted(glob.glob(os.path.join(isa_dir, 'isa_historico_analise_*.parquet')))\n",
    "\n",
    "if isa_files:\n",
    "    ultimo_isa = isa_files[-1]\n",
    "    print(f\"Carregando ISA hist√≥rico (bruto): {ultimo_isa}\")\n",
    "    isa_historico = pd.read_parquet(ultimo_isa)\n",
    "\n",
    "    col_isa_mes = [c for c in isa_historico.columns if re.match(r'isa_\\d{6}$', c)]\n",
    "    col_isa_mes_sorted = sorted(col_isa_mes)\n",
    "    ultimas4 = col_isa_mes_sorted[-4:]\n",
    "\n",
    "    print(f\"‚úì ISA hist√≥rico carregado: {len(isa_historico):,} registros\")\n",
    "    print(f\"Colunas: {list(isa_historico.columns)[:20]} ... total={len(isa_historico.columns)}\")\n",
    "    print(f\"  √öltimas colunas ISA detectadas: {ultimas4}\")\n",
    "\n",
    "    if len(ultimas4) >= 4:\n",
    "        isa_atual_col = ultimas4[-1]\n",
    "        tres_anteriores = ultimas4[-4:-1]\n",
    "    else:\n",
    "        isa_atual_col = ultimas4[-1] if ultimas4 else None\n",
    "        tres_anteriores = ultimas4[:-1] if len(ultimas4) > 1 else []\n",
    "\n",
    "    if tres_anteriores:\n",
    "        cols_exist = [c for c in tres_anteriores if c in isa_historico.columns]\n",
    "        isa_historico['isa_media_3m_calc'] = isa_historico[cols_exist].mean(axis=1)\n",
    "    else:\n",
    "        isa_historico['isa_media_3m_calc'] = np.nan\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  ISA hist√≥rico n√£o encontrado\")\n",
    "    isa_historico = pd.DataFrame()\n",
    "\n",
    "print(\"\\n‚úì Carga bruta conclu√≠da (nenhuma padroniza√ß√£o aplicada).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento m√≠nimo solicitado (APENAS rename num_cpf_cnpj -> cpf_cnpj nas bases carregadas)\n",
    "# N√£o aplica zfill, strip, nem qualquer outra normaliza√ß√£o.\n",
    "\n",
    "def _rename_num_cpf(df, nome):\n",
    "    if isinstance(df, pd.DataFrame) and not df.empty and 'num_cpf_cnpj' in df.columns and 'cpf_cnpj' not in df.columns:\n",
    "        df.rename(columns={'num_cpf_cnpj':'cpf_cnpj'}, inplace=True)\n",
    "        print(f\"‚úì {nome}: renomeado 'num_cpf_cnpj' -> 'cpf_cnpj'\")\n",
    "    else:\n",
    "        print(f\"{nome}: nenhum rename necess√°rio\")\n",
    "    return df\n",
    "\n",
    "bases_tratamento = [\n",
    "    ('df_principalidade', 'df_principalidade'),\n",
    "    ('assoc_dash', 'assoc_dash'),\n",
    "    ('isa_historico', 'isa_historico')\n",
    "]\n",
    "\n",
    "for var_name, label in bases_tratamento:\n",
    "    if var_name in globals():\n",
    "        globals()[var_name] = _rename_num_cpf(globals()[var_name], label)\n",
    "    else:\n",
    "        print(f\"{label}: vari√°vel inexistente\")\n",
    "\n",
    "print(\"Tratamento m√≠nimo conclu√≠do. Prosseguir com verifica√ß√£o e merges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica√ß√£o de 'tipo_pessoa' nas bases antes do merge (sem qualquer manipula√ß√£o)\n",
    "print(\"--- Verifica√ß√£o de 'tipo_pessoa' (valores brutos) ---\")\n",
    "\n",
    "bases_info = [\n",
    "    ('df_principalidade', df_principalidade),\n",
    "    ('assoc_dash', assoc_dash),\n",
    "    ('isa_historico', isa_historico if 'isa_historico' in globals() else pd.DataFrame())\n",
    "]\n",
    "\n",
    "snapshot_tipo = {}\n",
    "for nome, df in bases_info:\n",
    "    if df is not None and not df.empty and 'tipo_pessoa' in df.columns:\n",
    "        vals = df['tipo_pessoa'].dropna().astype(str).unique()[:10]\n",
    "        print(f\"\\n{nome}: valores √∫nicos de tipo_pessoa (primeiros 10):\")\n",
    "        print(vals)\n",
    "        print(f\"Nulos: {df['tipo_pessoa'].isna().sum():,}\")\n",
    "        snapshot_tipo[nome] = df[['cpf_cnpj','tipo_pessoa']].copy() if 'cpf_cnpj' in df.columns else df[['tipo_pessoa']].copy()\n",
    "    else:\n",
    "        print(f\"\\n{nome}: coluna 'tipo_pessoa' ausente ou dataframe vazio (ok se origem n√£o fornece).\")\n",
    "        snapshot_tipo[nome] = pd.DataFrame()\n",
    "\n",
    "snapshot_assoc_dash = snapshot_tipo.get('assoc_dash')\n",
    "snapshot_princ = snapshot_tipo.get('df_principalidade')\n",
    "snapshot_isa = snapshot_tipo.get('isa_historico')\n",
    "print(\"\\nSnapshot armazenado: snapshot_assoc_dash, snapshot_princ, snapshot_isa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padroniza_identificadores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza e padroniza as colunas de um DataFrame, com foco em chaves de jun√ß√£o.\n",
    "    Regras (EXATAMENTE como solicitado):\n",
    "      - Renomeia 'num_cpf_cnpj' para 'cpf_cnpj' se existir.\n",
    "      - Padroniza 'cpf_cnpj' com zeros √† esquerda: se len > 11 trata como CNPJ (14), caso contr√°rio CPF (11).\n",
    "      - Padroniza 'cod_agencia', 'num_conta', 'cod_carteira' apenas com zfill (sem heur√≠stica extra).\n",
    "    Nenhum merge, nenhuma infer√™ncia adicional. Apenas retorna o DF tratado.\n",
    "    \"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Renomear coluna principal\n",
    "    if 'num_cpf_cnpj' in df.columns and 'cpf_cnpj' not in df.columns:\n",
    "        df = df.rename(columns={'num_cpf_cnpj': 'cpf_cnpj'})\n",
    "\n",
    "    if 'cpf_cnpj' not in df.columns:\n",
    "        print(\"‚ö†Ô∏è  Aviso: Coluna 'cpf_cnpj' n√£o encontrada para padroniza√ß√£o.\")\n",
    "        return df\n",
    "\n",
    "    # Padroniza√ß√£o cpf_cnpj (sem remover caracteres, somente strip e zfill conforme regra original)\n",
    "    df['cpf_cnpj'] = (df['cpf_cnpj'].astype(str).str.strip()\n",
    "                      .apply(lambda x: x.zfill(14) if len(x) > 11 else x.zfill(11)))\n",
    "\n",
    "    # Demais colunas (apenas se existirem)\n",
    "    if 'cod_agencia' in df.columns:\n",
    "        df['cod_agencia'] = df['cod_agencia'].astype(str).str.zfill(2)\n",
    "    if 'num_conta' in df.columns:\n",
    "        df['num_conta'] = df['num_conta'].astype(str).str.zfill(6)\n",
    "    if 'cod_carteira' in df.columns:\n",
    "        df['cod_carteira'] = np.where(\n",
    "            df['cod_carteira'].astype(str).str.len() > 3,\n",
    "            df['cod_carteira'].astype(str).str.zfill(6),\n",
    "            df['cod_carteira'].astype(str).str.zfill(3)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "print('\\n=== Padronizando identificadores das tr√™s bases (sem merges) ===')\n",
    "if 'df_principalidade' in globals():\n",
    "    df_principalidade = padroniza_identificadores(df_principalidade)\n",
    "if 'assoc_dash' in globals():\n",
    "    assoc_dash = padroniza_identificadores(assoc_dash)\n",
    "if 'isa_historico' in globals():\n",
    "    isa_historico = padroniza_identificadores(isa_historico)\n",
    "print('Conclu√≠do. Continue com os merges nas c√©lulas seguintes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_dash = padroniza_identificadores(assoc_dash)\n",
    "df_principalidade = padroniza_identificadores(df_principalidade)\n",
    "isa_historico = padroniza_identificadores(isa_historico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principalidade['porte_padrao'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOVO MERGE CONTROLADO (usa apenas dados brutos carregados acima)\n",
    "print('--- Iniciando novo processo de merge controlado ---')\n",
    "\n",
    "df_base = df_principalidade.copy()\n",
    "\n",
    "# Merge dashboard\n",
    "if 'assoc_dash' in globals() and isinstance(assoc_dash, pd.DataFrame) and not assoc_dash.empty:\n",
    "    if 'cpf_cnpj' not in assoc_dash.columns:\n",
    "        raise KeyError(\"Dashboard sem coluna 'cpf_cnpj'.\")\n",
    "    cols_dash_to_merge = [c for c in assoc_dash.columns if c != 'cpf_cnpj']\n",
    "    df_base = df_base.merge(assoc_dash[['cpf_cnpj'] + cols_dash_to_merge], on='cpf_cnpj', how='left', suffixes=('', '_dash'))\n",
    "    # Coalesce sem sobrescrever valores j√° existentes em df_base (df_base √© c√≥pia bruta, ent√£o s√≥ pega do dash se nulo)\n",
    "    for col in cols_dash_to_merge:\n",
    "        col_y = f'{col}_dash'\n",
    "        if col_y in df_base.columns:\n",
    "            if col not in df_base.columns:\n",
    "                df_base[col] = df_base[col_y]\n",
    "            else:\n",
    "                df_base[col] = df_base[col].where(df_base[col].notna(), df_base[col_y])\n",
    "            df_base.drop(columns=[col_y], inplace=True)\n",
    "    print(f\"‚úì Merge dashboard conclu√≠do. Shape: {df_base.shape}\")\n",
    "else:\n",
    "    print('‚ö†Ô∏è Dashboard n√£o carregado ou vazio')\n",
    "\n",
    "# Merge ISA (n√£o deve alterar tipo_pessoa)\n",
    "if 'isa_historico' in globals() and isinstance(isa_historico, pd.DataFrame) and not isa_historico.empty:\n",
    "    if 'cpf_cnpj' in isa_historico.columns:\n",
    "        cols_isa_to_merge = [c for c in isa_historico.columns if c != 'cpf_cnpj' and c != 'tipo_pessoa']\n",
    "        df_base = df_base.merge(isa_historico[['cpf_cnpj'] + cols_isa_to_merge], on='cpf_cnpj', how='left', suffixes=('', '_isa'))\n",
    "        for col in cols_isa_to_merge:\n",
    "            col_y = f'{col}_isa'\n",
    "            if col_y in df_base.columns:\n",
    "                if col not in df_base.columns:\n",
    "                    df_base[col] = df_base[col_y]\n",
    "                else:\n",
    "                    df_base[col] = df_base[col].where(df_base[col].notna(), df_base[col_y])\n",
    "                df_base.drop(columns=[col_y], inplace=True)\n",
    "        print(f\"‚úì Merge ISA conclu√≠do. Shape: {df_base.shape}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è ISA hist√≥rico sem coluna 'cpf_cnpj' - merge ignorado.\")\n",
    "else:\n",
    "    print('‚ö†Ô∏è ISA n√£o carregado ou vazio')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Distribui√ß√£o final tipo_pessoa em df_base:')\n",
    "if 'tipo_pessoa' in df_base.columns:\n",
    "    print(df_base['tipo_pessoa'].value_counts(dropna=False))\n",
    "else:\n",
    "    print('Coluna tipo_pessoa ausente.')\n",
    "\n",
    "print('Colunas finais ap√≥s merges (primeiras 40):')\n",
    "print(list(df_base.columns)[:40])\n",
    "print('Total colunas:', len(df_base.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.query(\"tipo_pessoa.isna()\", engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagn√≥stico: onde 'PJ' se perdeu?\n",
    "# Motivo prov√°vel: merge LEFT partindo de df_principalidade elimina cpfs que s√≥ existem no dashboard (muitos PJ).\n",
    "# Esta c√©lula identifica:\n",
    "# 1. Quantos PJ existem no dashboard bruto.\n",
    "# 2. Quantos desses PJ aparecem em df_base ap√≥s o merge.\n",
    "# 3. Quantos PJ foram perdidos por aus√™ncia na principalidade.\n",
    "# 4. Amostras de cpfs perdidos.\n",
    "# 5. (Opcional) Reconstru√ß√£o de df_base usando uni√£o de todas as chaves para preservar PJ.\n",
    "\n",
    "\n",
    "\n",
    "if 'assoc_dash' in globals() and 'tipo_pessoa' in assoc_dash.columns:\n",
    "    pj_dash_total = (assoc_dash['tipo_pessoa'] == 'PJ').sum()\n",
    "    pf_dash_total = (assoc_dash['tipo_pessoa'] == 'PF').sum()\n",
    "    print(f\"Dashboard bruto -> PJ: {pj_dash_total:,} | PF: {pf_dash_total:,}\")\n",
    "else:\n",
    "    print('Dashboard n√£o dispon√≠vel para diagn√≥stico.')\n",
    "\n",
    "if 'snapshot_assoc_dash' in globals() and snapshot_assoc_dash is not None and not snapshot_assoc_dash.empty:\n",
    "    pj_snapshot = (snapshot_assoc_dash['tipo_pessoa'] == 'PJ').sum()\n",
    "    print(f\"Snapshot assoc_dash -> PJ: {pj_snapshot:,}\")\n",
    "\n",
    "if 'df_base' in globals() and 'tipo_pessoa' in df_base.columns:\n",
    "    pj_df_base = (df_base['tipo_pessoa'] == 'PJ').sum()\n",
    "    pf_df_base = (df_base['tipo_pessoa'] == 'PF').sum()\n",
    "    na_df_base = df_base['tipo_pessoa'].isna().sum()\n",
    "    print(f\"Ap√≥s merges -> PJ: {pj_df_base:,} | PF: {pf_df_base:,} | NA: {na_df_base:,}\")\n",
    "\n",
    "    # CPFs PJ no dashboard\n",
    "    pj_cpfs_dash = set(assoc_dash.loc[assoc_dash['tipo_pessoa']=='PJ','cpf_cnpj']) if 'assoc_dash' in globals() else set()\n",
    "    # CPFs presentes em df_base\n",
    "    base_cpfs = set(df_base['cpf_cnpj'])\n",
    "    # PJ preservados\n",
    "    pj_preservados = pj_cpfs_dash & base_cpfs\n",
    "    # PJ perdidos\n",
    "    pj_perdidos = pj_cpfs_dash - base_cpfs\n",
    "    print(f\"PJ preservados (existem em principalidade): {len(pj_preservados):,}\")\n",
    "    print(f\"PJ perdidos (n√£o existem em principalidade, por isso n√£o aparecem no LEFT merge): {len(pj_perdidos):,}\")\n",
    "    if pj_perdidos:\n",
    "        exemplo_perdidos = list(pj_perdidos)[:10]\n",
    "        print('Exemplos de PJ perdidos:', exemplo_perdidos)\n",
    "else:\n",
    "    print('df_base ou tipo_pessoa indispon√≠vel para diagn√≥stico.')\n",
    "\n",
    "# Opcional: reconstruir df_base com uni√£o de chaves para preservar PJ\n",
    "RECONSTRUIR_UNIAO = False  # coloque True se quiser gerar uma vers√£o alternativa preservando todos os CPFs\n",
    "if RECONSTRUIR_UNIAO:\n",
    "    print('\\nReconstruindo df_base_all_keys com uni√£o de chaves...')\n",
    "    chaves = set()\n",
    "    if 'df_principalidade' in globals():\n",
    "        chaves |= set(df_principalidade.get('cpf_cnpj', []))\n",
    "    if 'assoc_dash' in globals():\n",
    "        chaves |= set(assoc_dash.get('cpf_cnpj', []))\n",
    "    if 'isa_historico' in globals():\n",
    "        chaves |= set(isa_historico.get('cpf_cnpj', []))\n",
    "    df_base_all_keys = pd.DataFrame({'cpf_cnpj': list(chaves)})\n",
    "    # merge incremental\n",
    "    if 'df_principalidade' in globals():\n",
    "        df_base_all_keys = df_base_all_keys.merge(df_principalidade, on='cpf_cnpj', how='left')\n",
    "    if 'assoc_dash' in globals():\n",
    "        cols_dash_tmp = [c for c in assoc_dash.columns if c != 'cpf_cnpj']\n",
    "        df_base_all_keys = df_base_all_keys.merge(assoc_dash[['cpf_cnpj'] + cols_dash_tmp], on='cpf_cnpj', how='left', suffixes=('', '_dash2'))\n",
    "        for c in cols_dash_tmp:\n",
    "            cy = f'{c}_dash2'\n",
    "            if cy in df_base_all_keys.columns:\n",
    "                df_base_all_keys[c] = df_base_all_keys[c].where(df_base_all_keys[c].notna(), df_base_all_keys[cy])\n",
    "                df_base_all_keys.drop(columns=[cy], inplace=True)\n",
    "    if 'isa_historico' in globals():\n",
    "        cols_isa_tmp = [c for c in isa_historico.columns if c != 'cpf_cnpj' and c != 'tipo_pessoa']\n",
    "        df_base_all_keys = df_base_all_keys.merge(isa_historico[['cpf_cnpj'] + cols_isa_tmp], on='cpf_cnpj', how='left', suffixes=('', '_isa2'))\n",
    "        for c in cols_isa_tmp:\n",
    "            cy = f'{c}_isa2'\n",
    "            if cy in df_base_all_keys.columns:\n",
    "                df_base_all_keys[c] = df_base_all_keys[c].where(df_base_all_keys[c].notna(), df_base_all_keys[cy])\n",
    "                df_base_all_keys.drop(columns=[cy], inplace=True)\n",
    "    # Refor√ßar tipo_pessoa com dashboard novamente\n",
    "    if 'assoc_dash' in globals() and 'tipo_pessoa' in assoc_dash.columns:\n",
    "        tipos_prioritarios2 = (assoc_dash[['cpf_cnpj','tipo_pessoa']]\n",
    "                               .dropna()\n",
    "                               .groupby('cpf_cnpj', as_index=False)\n",
    "                               .agg(tipo_pessoa=lambda s: 'PJ' if 'PJ' in set(s) else ('PF' if 'PF' in set(s) else s.iloc[0])))\n",
    "        df_base_all_keys.drop(columns=[c for c in ['tipo_pessoa'] if c in df_base_all_keys.columns], inplace=True)\n",
    "        df_base_all_keys = df_base_all_keys.merge(tipos_prioritarios2, on='cpf_cnpj', how='left')\n",
    "    print('Distribui√ß√£o tipo_pessoa (df_base_all_keys):')\n",
    "    print(df_base_all_keys['tipo_pessoa'].value_counts(dropna=False))\n",
    "    print('Amostra df_base_all_keys:')\n",
    "    display(df_base_all_keys.head())\n",
    "    print('Use df_base_all_keys se quiser incluir PJ que n√£o aparecem na principalidade.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['tipo_pessoa'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e030082",
   "metadata": {},
   "source": [
    "## 2. An√°lise de Quantidade de Contas (PF/PJ) e Ag√™ncias\n",
    "\n",
    "An√°lise da distribui√ß√£o de contas por tipo de pessoa e ag√™ncias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a varia√ß√£o de pontos e identifica quedas\n",
    "df_base = df_base.sort_values(by=['cpf_cnpj', 'ano_mes'])\n",
    "df_base['var_pontos'] = df_base.groupby('cpf_cnpj')['pontos_principalidade'].diff().fillna(0)\n",
    "df_base['queda_flag'] = (df_base['var_pontos'] < 0).astype(int)\n",
    "df_base['soma_quedas'] = df_base.groupby('cpf_cnpj')['var_pontos'].transform(lambda x: x[x < 0].sum())\n",
    "\n",
    "df_base[['cpf_cnpj', 'ano_mes', 'pontos_principalidade', 'var_pontos', 'queda_flag', 'soma_quedas']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0162e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de quantidade de contas por tipo de pessoa e ag√™ncia\n",
    "print(\"üìä AN√ÅLISE DE QUANTIDADE DE CONTAS POR TIPO DE PESSOA E AG√äNCIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'df_base' not in globals():\n",
    "    raise RuntimeError('df_base inexistente: execute as c√©lulas de carga e merge primeiro.')\n",
    "\n",
    "# Dados mais recentes (requer ano_mes)\n",
    "if 'ano_mes' not in df_base.columns:\n",
    "    raise ValueError(\"Coluna 'ano_mes' n√£o encontrada em df_base. Verifique a base de principalidade.\")\n",
    "\n",
    "df_atual = df_base[df_base['ano_mes'] == df_base['ano_mes'].max()].copy()\n",
    "\n",
    "required_cols = ['tipo_pessoa', 'nome_agencia', 'cod_agencia', 'cpf_cnpj']\n",
    "faltantes = [c for c in required_cols if c not in df_atual.columns]\n",
    "if faltantes:\n",
    "    raise ValueError(f\"Colunas necess√°rias ausentes em df_atual: {faltantes}\")\n",
    "\n",
    "# Agrupar por tipo de pessoa e ag√™ncia\n",
    "analise_agencia = (df_atual\n",
    "                   .groupby(['tipo_pessoa', 'nome_agencia', 'cod_agencia'])\n",
    "                   .agg(\n",
    "                       associados_unicos=('cpf_cnpj', 'nunique'),\n",
    "                       registros_totais=('cpf_cnpj', 'count')\n",
    "                   )\n",
    "                   .sort_values(by=['tipo_pessoa', 'associados_unicos'], ascending=[True, False])\n",
    "                   .reset_index())\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"An√°lise de Associados e Registros por Tipo de Pessoa e Ag√™ncia:\")\n",
    "from IPython.display import display\n",
    "display(analise_agencia)\n",
    "\n",
    "# Resumo geral por tipo de pessoa\n",
    "resumo_tipo_pessoa = analise_agencia.groupby('tipo_pessoa').agg(\n",
    "    total_associados=('associados_unicos', 'sum'),\n",
    "    total_registros=('registros_totais', 'sum'),\n",
    "    numero_de_agencias=('nome_agencia', 'count')\n",
    ").sort_values(by='total_associados', ascending=False)\n",
    "\n",
    "print(\"\\nResumo Consolidado por Tipo de Pessoa:\")\n",
    "display(resumo_tipo_pessoa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de quantidade de contas por tipo de pessoa\n",
    "print(\"üìä AN√ÅLISE DE QUANTIDADE DE CONTAS E AG√äNCIAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dados mais recentes\n",
    "df_atual = df_base[df_base['ano_mes'] == df_base['ano_mes'].max()].copy()\n",
    "\n",
    "# Total de contas √∫nicas\n",
    "total_contas = df_atual['cpf_cnpj'].nunique()\n",
    "print(f\"Total de Contas √önicas: {total_contas:,}\")\n",
    "\n",
    "# Distribui√ß√£o por tipo de pessoa\n",
    "dist_pessoa = df_atual['tipo_pessoa'].value_counts()\n",
    "print(f\"\\nDistribui√ß√£o por Tipo de Pessoa:\")\n",
    "for tipo, qtd in dist_pessoa.items():\n",
    "    perc = (qtd / total_contas) * 100\n",
    "    print(f\"  {tipo}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Total de ag√™ncias\n",
    "total_agencias = df_atual['cod_agencia'].nunique()\n",
    "print(f\"\\nTotal de Ag√™ncias: {total_agencias:,}\")\n",
    "\n",
    "# Top 10 ag√™ncias com mais contas\n",
    "top_agencias = df_atual['cod_agencia'].value_counts().head(10)\n",
    "print(f\"\\nTop 10 Ag√™ncias (por n√∫mero de contas):\")\n",
    "for i, (agencia, qtd) in enumerate(top_agencias.items(), 1):\n",
    "    perc = (qtd / total_contas) * 100\n",
    "    print(f\"  {i:2d}. Ag√™ncia {agencia}: {qtd:,} contas ({perc:.1f}%)\")\n",
    "\n",
    "# Distribui√ß√£o PF/PJ por ag√™ncia (top 5)\n",
    "print(f\"\\nDistribui√ß√£o PF/PJ nas Top 5 Ag√™ncias:\")\n",
    "top5_agencias = top_agencias.head(5).index\n",
    "\n",
    "for agencia in top5_agencias:\n",
    "    df_agencia = df_atual[df_atual['cod_agencia'] == agencia]\n",
    "    dist_agencia = df_agencia['tipo_pessoa'].value_counts()\n",
    "    total_agencia = len(df_agencia)\n",
    "    \n",
    "    pf_count = dist_agencia.get('PF', 0)\n",
    "    pj_count = dist_agencia.get('PJ', 0)\n",
    "    \n",
    "    print(f\"  Ag√™ncia {agencia}: PF={pf_count} ({pf_count/total_agencia*100:.1f}%) | PJ={pj_count} ({pj_count/total_agencia*100:.1f}%)\")\n",
    "\n",
    "# Resumo consolidado\n",
    "resumo_contas = {\n",
    "    'total_contas': total_contas,\n",
    "    'total_pf': dist_pessoa.get('PF', 0),\n",
    "    'total_pj': dist_pessoa.get('PJ', 0),\n",
    "    'total_agencias': total_agencias,\n",
    "    'perc_pf': (dist_pessoa.get('PF', 0) / total_contas) * 100,\n",
    "    'perc_pj': (dist_pessoa.get('PJ', 0) / total_contas) * 100\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã RESUMO EXECUTIVO - CONTAS E AG√äNCIAS\")\n",
    "print(f\"Total: {resumo_contas['total_contas']:,} contas em {resumo_contas['total_agencias']:,} ag√™ncias\")\n",
    "print(f\"PF: {resumo_contas['total_pf']:,} ({resumo_contas['perc_pf']:.1f}%)\")\n",
    "print(f\"PJ: {resumo_contas['total_pj']:,} ({resumo_contas['perc_pj']:.1f}%)\")\n",
    "\n",
    "# Salvar resumo para uso posterior\n",
    "globals()['resumo_contas'] = resumo_contas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33863f04",
   "metadata": {},
   "source": [
    "## 3. An√°lise BBM e Fora de Filtro\n",
    "\n",
    "An√°lise da distribui√ß√£o dos associados dentro e fora dos filtros de risco BBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise BBM vs Fora de Filtro\n",
    "print(\"üìä AN√ÅLISE BBM E FORA DE FILTRO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classificar por BBM\n",
    "riscos_bbm = PARAMS['filtros_risco_bbm']\n",
    "print(f\"Filtros BBM considerados: {riscos_bbm}\")\n",
    "\n",
    "# Normalizar n√≠vel de risco\n",
    "df_atual['nivel_risco_norm'] = df_atual['nivel_risco'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Classificar BBM vs N√£o-BBM\n",
    "df_atual['classificacao_bbm'] = np.where(\n",
    "    df_atual['nivel_risco_norm'].isin([r.upper() for r in riscos_bbm]),\n",
    "    'BBM',\n",
    "    'Fora_BBM'\n",
    ")\n",
    "\n",
    "# Contagem por classifica√ß√£o BBM\n",
    "dist_bbm = df_atual['classificacao_bbm'].value_counts()\n",
    "total_analisados = len(df_atual)\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o por Classifica√ß√£o BBM:\")\n",
    "for classif, qtd in dist_bbm.items():\n",
    "    perc = (qtd / total_analisados) * 100\n",
    "    print(f\"  {classif}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# An√°lise detalhada por n√≠vel de risco\n",
    "print(f\"\\nDetalhamento por N√≠vel de Risco:\")\n",
    "dist_risco = df_atual['nivel_risco_norm'].value_counts()\n",
    "for risco, qtd in dist_risco.head(10).items():\n",
    "    perc = (qtd / total_analisados) * 100\n",
    "    status = \"‚úì BBM\" if risco in [r.upper() for r in riscos_bbm] else \"‚úó Fora BBM\"\n",
    "    print(f\"  {risco}: {qtd:,} ({perc:.1f}%) - {status}\")\n",
    "\n",
    "# Cruzamento BBM x Tipo de Pessoa\n",
    "print(f\"\\nCruzamento BBM x Tipo de Pessoa:\")\n",
    "crosstab_bbm_pessoa = pd.crosstab(df_atual['classificacao_bbm'], df_atual['tipo_pessoa'], margins=True)\n",
    "print(crosstab_bbm_pessoa)\n",
    "\n",
    "# Percentuais por tipo de pessoa\n",
    "print(f\"\\nPercentuais BBM por Tipo de Pessoa:\")\n",
    "for pessoa in ['PF', 'PJ']:\n",
    "    df_pessoa = df_atual[df_atual['tipo_pessoa'] == pessoa]\n",
    "    if not df_pessoa.empty:\n",
    "        bbm_count = (df_pessoa['classificacao_bbm'] == 'BBM').sum()\n",
    "        total_pessoa = len(df_pessoa)\n",
    "        perc_bbm = (bbm_count / total_pessoa) * 100\n",
    "        print(f\"  {pessoa}: {bbm_count:,}/{total_pessoa:,} s√£o BBM ({perc_bbm:.1f}%)\")\n",
    "\n",
    "# An√°lise por faixa de principalidade\n",
    "if 'faixa_categoria' in df_atual.columns:\n",
    "    print(f\"\\nBBM por Faixa de Principalidade:\")\n",
    "    crosstab_bbm_faixa = pd.crosstab(df_atual['faixa_categoria'], df_atual['classificacao_bbm'], margins=True)\n",
    "    print(crosstab_bbm_faixa)\n",
    "\n",
    "# Resumo executivo BBM\n",
    "bbm_count = dist_bbm.get('BBM', 0)\n",
    "fora_bbm_count = dist_bbm.get('Fora_BBM', 0)\n",
    "\n",
    "resumo_bbm = {\n",
    "    'total_analisados': total_analisados,\n",
    "    'bbm_count': bbm_count,\n",
    "    'fora_bbm_count': fora_bbm_count,\n",
    "    'perc_bbm': (bbm_count / total_analisados) * 100,\n",
    "    'perc_fora_bbm': (fora_bbm_count / total_analisados) * 100\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã RESUMO EXECUTIVO - BBM\")\n",
    "print(f\"Dentro do filtro BBM: {resumo_bbm['bbm_count']:,} ({resumo_bbm['perc_bbm']:.1f}%)\")\n",
    "print(f\"Fora do filtro BBM: {resumo_bbm['fora_bbm_count']:,} ({resumo_bbm['perc_fora_bbm']:.1f}%)\")\n",
    "\n",
    "globals()['resumo_bbm'] = resumo_bbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23724f53",
   "metadata": {},
   "source": [
    "## 4. An√°lise de Principalidade: Subida e Bajada √öltimos 3 Meses\n",
    "\n",
    "An√°lise da evolu√ß√£o da principalidade: contas que subiram e desceram nos √∫ltimos 3 meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Evolu√ß√£o da Principalidade\n",
    "print(\"üìä AN√ÅLISE DE EVOLU√á√ÉO DA PRINCIPALIDADE - √öLTIMOS 3 MESES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pegar √∫ltimos 3 meses dispon√≠veis\n",
    "meses_disponiveis = sorted(df_base['ano_mes'].unique())\n",
    "ultimos_3_meses = meses_disponiveis[-3:] if len(meses_disponiveis) >= 3 else meses_disponiveis\n",
    "\n",
    "print(f\"Per√≠odos analisados: {ultimos_3_meses}\")\n",
    "\n",
    "# Filtrar dados dos √∫ltimos 3 meses\n",
    "df_3meses = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].copy()\n",
    "\n",
    "# An√°lise por associado nos √∫ltimos 3 meses\n",
    "evolucao_princ = df_3meses.groupby('cpf_cnpj').agg({\n",
    "    'pontos_principalidade': ['first', 'last', 'min', 'max', 'std'],\n",
    "    'var_pontos': 'sum',\n",
    "    'queda_flag': 'sum',\n",
    "    'soma_quedas': 'max',\n",
    "    'tipo_pessoa': 'first',\n",
    "    'faixa_categoria': 'last'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "evolucao_princ.columns = ['_'.join(col).strip() for col in evolucao_princ.columns]\n",
    "evolucao_princ = evolucao_princ.reset_index()\n",
    "\n",
    "# Calcular varia√ß√£o total\n",
    "evolucao_princ['var_total'] = evolucao_princ['pontos_principalidade_last'] - evolucao_princ['pontos_principalidade_first']\n",
    "\n",
    "# Classificar evolu√ß√£o\n",
    "def classificar_evolucao(row):\n",
    "    if row['var_total'] > 0:\n",
    "        return 'SUBIU'\n",
    "    elif row['var_total'] < 0:\n",
    "        return 'DESCEU'\n",
    "    else:\n",
    "        return 'EST√ÅVEL'\n",
    "\n",
    "evolucao_princ['classificacao_evolucao'] = evolucao_princ.apply(classificar_evolucao, axis=1)\n",
    "\n",
    "# An√°lise geral de evolu√ß√£o\n",
    "print(f\"\\nRESULTADOS GERAIS:\")\n",
    "dist_evolucao = evolucao_princ['classificacao_evolucao'].value_counts()\n",
    "total_associados = len(evolucao_princ)\n",
    "\n",
    "for classif, qtd in dist_evolucao.items():\n",
    "    perc = (qtd / total_associados) * 100\n",
    "    print(f\"  {classif}: {qtd:,} associados ({perc:.1f}%)\")\n",
    "\n",
    "# Estat√≠sticas de varia√ß√£o\n",
    "print(f\"\\nESTAT√çSTICAS DE VARIA√á√ÉO DOS PONTOS:\")\n",
    "print(f\"  Varia√ß√£o m√©dia: {evolucao_princ['var_total'].mean():.2f} pontos\")\n",
    "print(f\"  Varia√ß√£o mediana: {evolucao_princ['var_total'].median():.2f} pontos\")\n",
    "print(f\"  Maior subida: {evolucao_princ['var_total'].max():.2f} pontos\")\n",
    "print(f\"  Maior queda: {evolucao_princ['var_total'].min():.2f} pontos\")\n",
    "print(f\"  Desvio padr√£o: {evolucao_princ['var_total'].std():.2f} pontos\")\n",
    "\n",
    "# Evolu√ß√£o por tipo de pessoa\n",
    "print(f\"\\nEVOLU√á√ÉO POR TIPO DE PESSOA:\")\n",
    "for pessoa in ['PF', 'PJ']:\n",
    "    df_pessoa = evolucao_princ[evolucao_princ['tipo_pessoa_first'] == pessoa]\n",
    "    if not df_pessoa.empty:\n",
    "        dist_pessoa = df_pessoa['classificacao_evolucao'].value_counts()\n",
    "        total_pessoa = len(df_pessoa)\n",
    "        \n",
    "        print(f\"\\n  {pessoa} (Total: {total_pessoa:,}):\")\n",
    "        for classif, qtd in dist_pessoa.items():\n",
    "            perc = (qtd / total_pessoa) * 100\n",
    "            print(f\"    {classif}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Top 10 maiores subidas\n",
    "print(f\"\\nTOP 10 MAIORES SUBIDAS:\")\n",
    "top_subidas = evolucao_princ.nlargest(10, 'var_total')\n",
    "for i, row in enumerate(top_subidas.itertuples(), 1):\n",
    "    print(f\"  {i:2d}. CPF: {row.cpf_cnpj[-4:]}... | Subida: +{row.var_total:.1f} pontos | {row.tipo_pessoa_first}\")\n",
    "\n",
    "# Top 10 maiores quedas  \n",
    "print(f\"\\nTOP 10 MAIORES QUEDAS:\")\n",
    "top_quedas = evolucao_princ.nsmallest(10, 'var_total')\n",
    "for i, row in enumerate(top_quedas.itertuples(), 1):\n",
    "    print(f\"  {i:2d}. CPF: {row.cpf_cnpj[-4:]}... | Queda: {row.var_total:.1f} pontos | {row.tipo_pessoa_first}\")\n",
    "\n",
    "# Associados com 3 quedas consecutivas (conforme query original)\n",
    "associados_3_quedas = evolucao_princ[evolucao_princ['soma_quedas_max'] >= 3]\n",
    "print(f\"\\nASSOCIADOS COM 3 QUEDAS CONSECUTIVAS: {len(associados_3_quedas):,}\")\n",
    "\n",
    "if len(associados_3_quedas) > 0:\n",
    "    print(f\"  PF: {(associados_3_quedas['tipo_pessoa_first'] == 'PF').sum():,}\")\n",
    "    print(f\"  PJ: {(associados_3_quedas['tipo_pessoa_first'] == 'PJ').sum():,}\")\n",
    "    \n",
    "    # Exemplos de associados com 3 quedas\n",
    "    print(f\"\\n  Exemplos (5 primeiros):\")\n",
    "    for i, row in enumerate(associados_3_quedas.head().itertuples(), 1):\n",
    "        print(f\"    {i}. CPF: {row.cpf_cnpj[-4:]}... | Var Total: {row.var_total:.1f} | Quedas: {row.soma_quedas_max}\")\n",
    "\n",
    "# Resumo executivo\n",
    "resumo_evolucao = {\n",
    "    'total_analisados': total_associados,\n",
    "    'subiu': dist_evolucao.get('SUBIU', 0),\n",
    "    'desceu': dist_evolucao.get('DESCEU', 0), \n",
    "    'estavel': dist_evolucao.get('EST√ÅVEL', 0),\n",
    "    'tres_quedas': len(associados_3_quedas),\n",
    "    'var_media': evolucao_princ['var_total'].mean()\n",
    "}\n",
    "\n",
    "print(f\"\\nüìã RESUMO EXECUTIVO - EVOLU√á√ÉO PRINCIPALIDADE\")\n",
    "print(f\"Analisados: {resumo_evolucao['total_analisados']:,} associados\")\n",
    "print(f\"Subiram: {resumo_evolucao['subiu']:,} ({resumo_evolucao['subiu']/total_associados*100:.1f}%)\")\n",
    "print(f\"Desceram: {resumo_evolucao['desceu']:,} ({resumo_evolucao['desceu']/total_associados*100:.1f}%)\")\n",
    "print(f\"3 quedas consecutivas: {resumo_evolucao['tres_quedas']:,}\")\n",
    "\n",
    "globals()['resumo_evolucao'] = resumo_evolucao\n",
    "globals()['evolucao_princ'] = evolucao_princ\n",
    "\n",
    "# Define as colunas espec√≠ficas do 'assoc_dash' que queremos mesclar.\n",
    "# Adicionados: 'pix_trans_30d', 'ult_movimento'\n",
    "cols_dash_to_merge = [\n",
    "    'tipo_pessoa', 'segmento_cliente', 'perfil_investidor',\n",
    "    'data_abertura', 'data_encerramento', 'status_conta', 'nome_agencia',\n",
    "    'pix_trans_30d', 'ult_movimento'  # <- NOVOS CAMPOS\n",
    "]\n",
    "\n",
    "# Realiza o merge usando a fun√ß√£o atualizada\n",
    "df_base = merge_and_coalesce(\n",
    "    df_left=df_principalidade,\n",
    "    df_right=assoc_dash,\n",
    "    on='cpf_cnpj',\n",
    "    cols_to_merge=cols_dash_to_merge,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verifica√ß√£o de valores nulos em 'tipo_pessoa' ap√≥s o merge\n",
    "if 'tipo_pessoa' in df_base.columns and df_base['tipo_pessoa'].isnull().any():\n",
    "    print(\"\\nAten√ß√£o: Existem registros com 'tipo_pessoa' nulo ap√≥s o merge com assoc_dash.\")\n",
    "    print(\"Isso indica que alguns CPFs/CNPJs de 'df_principalidade' n√£o foram encontrados em 'assoc_dash'.\")\n",
    "\n",
    "# Garantir convers√£o de ult_movimento para data (para uso posterior)\n",
    "if 'ult_movimento' in df_base.columns:\n",
    "    df_base['ult_movimento'] = pd.to_datetime(df_base['ult_movimento'], errors='coerce')\n",
    "\n",
    "df_base.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ddf09",
   "metadata": {},
   "source": [
    "## 6. Diferen√ßa de ISA √öltimos 3 Meses\n",
    "\n",
    "An√°lise das varia√ß√µes do ISA nos √∫ltimos 3 meses e insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Diferen√ßa de ISA √öltimos 3 Meses\n",
    "print(\"üìä AN√ÅLISE DE DIFEREN√áA DE ISA - √öLTIMOS 3 MESES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar disponibilidade de dados ISA\n",
    "if 'isa_media' in df_base.columns or 'isa_media_3m_calc' in df_base.columns:\n",
    "    \n",
    "    # Usar dados ISA dispon√≠veis\n",
    "    df_isa = df_base.copy()\n",
    "    \n",
    "    # Preparar an√°lise ISA\n",
    "    if 'isa_media_3m_calc' in df_isa.columns:\n",
    "        print(\"Usando ISA m√©dia 3M calculada\")\n",
    "        df_isa['isa_analise'] = df_isa['isa_media_3m_calc']\n",
    "    elif 'isa_media' in df_isa.columns:\n",
    "        print(\"Usando ISA m√©dia dispon√≠vel\")\n",
    "        df_isa['isa_analise'] = df_isa['isa_media']\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Dados de ISA n√£o dispon√≠veis para an√°lise completa\")\n",
    "        df_isa['isa_analise'] = np.nan\n",
    "    \n",
    "    # Filtrar dados com ISA v√°lido\n",
    "    df_isa_valido = df_isa[df_isa['isa_analise'].notna()].copy()\n",
    "    \n",
    "    if len(df_isa_valido) > 0:\n",
    "        print(f\"Registros com ISA v√°lido: {len(df_isa_valido):,}\")\n",
    "        \n",
    "        # Estat√≠sticas gerais de ISA\n",
    "        print(f\"\\nESTAT√çSTICAS GERAIS DE ISA:\")\n",
    "        isa_stats = df_isa_valido['isa_analise'].describe()\n",
    "        print(f\"  M√©dia: {isa_stats['mean']:.2f}\")\n",
    "        print(f\"  Mediana: {isa_stats['50%']:.2f}\")\n",
    "        print(f\"  M√≠nimo: {isa_stats['min']:.2f}\")\n",
    "        print(f\"  M√°ximo: {isa_stats['max']:.2f}\")\n",
    "        print(f\"  Desvio Padr√£o: {isa_stats['std']:.2f}\")\n",
    "        \n",
    "        # Distribui√ß√£o por faixas de ISA\n",
    "        df_isa_valido['faixa_isa'] = pd.cut(\n",
    "            df_isa_valido['isa_analise'], \n",
    "            bins=[0, 2, 4, 6, 8, float('inf')], \n",
    "            labels=['0-2', '2-4', '4-6', '6-8', '8+']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDISTRIBUI√á√ÉO POR FAIXAS DE ISA:\")\n",
    "        dist_isa = df_isa_valido['faixa_isa'].value_counts().sort_index()\n",
    "        total_isa = len(df_isa_valido)\n",
    "        \n",
    "        for faixa, qtd in dist_isa.items():\n",
    "            perc = (qtd / total_isa) * 100\n",
    "            print(f\"  {faixa}: {qtd:,} ({perc:.1f}%)\")\n",
    "        \n",
    "        # ISA por tipo de pessoa\n",
    "        print(f\"\\nISA M√âDIO POR TIPO DE PESSOA:\")\n",
    "        isa_por_pessoa = df_isa_valido.groupby('tipo_pessoa')['isa_analise'].agg(['mean', 'count', 'std']).round(2)\n",
    "        \n",
    "        for pessoa in isa_por_pessoa.index:\n",
    "            media = isa_por_pessoa.loc[pessoa, 'mean']\n",
    "            count = isa_por_pessoa.loc[pessoa, 'count']\n",
    "            std = isa_por_pessoa.loc[pessoa, 'std']\n",
    "            print(f\"  {pessoa}: {media:.2f} (n={count:,}, std={std:.2f})\")\n",
    "        \n",
    "        # ISA vs Principalidade\n",
    "        if 'pontos_principalidade' in df_isa_valido.columns:\n",
    "            print(f\"\\nCORRELA√á√ÉO ISA vs PRINCIPALIDADE:\")\n",
    "            correlacao = df_isa_valido['isa_analise'].corr(df_isa_valido['pontos_principalidade'])\n",
    "            print(f\"  Correla√ß√£o: {correlacao:.3f}\")\n",
    "            \n",
    "            # An√°lise por quartis de ISA\n",
    "            df_isa_valido['quartil_isa'] = pd.qcut(\n",
    "                df_isa_valido['isa_analise'], \n",
    "                q=4, \n",
    "                labels=['Q1_Baixo', 'Q2_MedioBaixo', 'Q3_MedioAlto', 'Q4_Alto']\n",
    "            )\n",
    "            \n",
    "            princ_por_quartil = df_isa_valido.groupby('quartil_isa')['pontos_principalidade'].agg(['mean', 'count']).round(1)\n",
    "            \n",
    "            print(f\"\\n  PRINCIPALIDADE M√âDIA POR QUARTIL DE ISA:\")\n",
    "            for quartil in princ_por_quartil.index:\n",
    "                media_princ = princ_por_quartil.loc[quartil, 'mean']\n",
    "                count = princ_por_quartil.loc[quartil, 'count']\n",
    "                print(f\"    {quartil}: {media_princ:.1f} pontos (n={count:,})\")\n",
    "        \n",
    "        # Top/Bottom ISA\n",
    "        print(f\"\\nTOP 10 MAIORES ISA:\")\n",
    "        top_isa = df_isa_valido.nlargest(10, 'isa_analise')[['cpf_cnpj', 'isa_analise', 'tipo_pessoa', 'pontos_principalidade']]\n",
    "        for i, row in enumerate(top_isa.itertuples(), 1):\n",
    "            cpf_mask = row.cpf_cnpj[-4:] if len(str(row.cpf_cnpj)) >= 4 else str(row.cpf_cnpj)\n",
    "            print(f\"  {i:2d}. CPF: ...{cpf_mask} | ISA: {row.isa_analise:.2f} | {row.tipo_pessoa} | Princ: {row.pontos_principalidade:.1f}\")\n",
    "        \n",
    "        print(f\"\\nTOP 10 MENORES ISA (acima de 0):\")\n",
    "        bottom_isa = df_isa_valido[df_isa_valido['isa_analise'] > 0].nsmallest(10, 'isa_analise')[['cpf_cnpj', 'isa_analise', 'tipo_pessoa', 'pontos_principalidade']]\n",
    "        for i, row in enumerate(bottom_isa.itertuples(), 1):\n",
    "            cpf_mask = row.cpf_cnpj[-4:] if len(str(row.cpf_cnpj)) >= 4 else str(row.cpf_cnpj)\n",
    "            print(f\"  {i:2d}. CPF: ...{cpf_mask} | ISA: {row.isa_analise:.2f} | {row.tipo_pessoa} | Princ: {row.pontos_principalidade:.1f}\")\n",
    "        \n",
    "        # Insights ISA\n",
    "        print(f\"\\nüí° INSIGHTS ISA:\")\n",
    "        \n",
    "        # ISA alto com baixa principalidade (oportunidades)\n",
    "        isa_alto_princ_baixo = df_isa_valido[\n",
    "            (df_isa_valido['isa_analise'] >= 6) & \n",
    "            (df_isa_valido['pontos_principalidade'] < df_isa_valido['pontos_principalidade'].median())\n",
    "        ]\n",
    "        \n",
    "        if len(isa_alto_princ_baixo) > 0:\n",
    "            print(f\"  ‚Ä¢ {len(isa_alto_princ_alto):,} associados com ISA alto (‚â•6) mas principalidade baixa\")\n",
    "            print(f\"    Oportunidade de cross-selling!\")\n",
    "        \n",
    "        # ISA baixo com alta principalidade (risco)\n",
    "        isa_baixo_princ_alto = df_isa_valido[\n",
    "            (df_isa_valido['isa_analise'] <= 2) & \n",
    "            (df_isa_valido['pontos_principalidade'] > df_isa_valido['pontos_principalidade'].quantile(0.75))\n",
    "        ]\n",
    "        \n",
    "        if len(isa_baixo_princ_alto) > 0:\n",
    "            print(f\"  ‚Ä¢ {len(isa_baixo_princ_alto):,} associados com ISA baixo (‚â§2) mas principalidade alta\")\n",
    "            print(f\"    Poss√≠vel risco de churn!\")\n",
    "        \n",
    "        # Concentra√ß√£o ISA por faixa\n",
    "        faixa_2_4 = (dist_isa.get('2-4', 0) / total_isa) * 100\n",
    "        if faixa_2_4 > 30:\n",
    "            print(f\"  ‚Ä¢ {faixa_2_4:.1f}% dos associados na faixa ISA 2-4 (faixa cr√≠tica)\")\n",
    "        \n",
    "        # Resumo ISA\n",
    "        resumo_isa = {\n",
    "            'total_com_isa': len(df_isa_valido),\n",
    "            'isa_medio': df_isa_valido['isa_analise'].mean(),\n",
    "            'isa_mediano': df_isa_valido['isa_analise'].median(),\n",
    "            'acima_6': (df_isa_valido['isa_analise'] >= 6).sum(),\n",
    "            'abaixo_2': (df_isa_valido['isa_analise'] <= 2).sum(),\n",
    "            'correlacao_princ': correlacao if 'correlacao' in locals() else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìã RESUMO EXECUTIVO - ISA\")\n",
    "        print(f\"ISA m√©dio: {resumo_isa['isa_medio']:.2f}\")\n",
    "        print(f\"Acima de 6: {resumo_isa['acima_6']:,} ({resumo_isa['acima_6']/total_isa*100:.1f}%)\")\n",
    "        print(f\"Abaixo de 2: {resumo_isa['abaixo_2']:,} ({resumo_isa['abaixo_2']/total_isa*100:.1f}%)\")\n",
    "        \n",
    "        globals()['resumo_isa'] = resumo_isa\n",
    "        globals()['df_isa_analise'] = df_isa_valido\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Nenhum registro com ISA v√°lido encontrado\")\n",
    "        resumo_isa = {'total_com_isa': 0, 'isa_medio': 0}\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Colunas de ISA n√£o encontradas nos dados\")\n",
    "    resumo_isa = {'total_com_isa': 0, 'isa_medio': 0}\n",
    "\n",
    "# Colunas do 'isa_historico' para fazer o merge\n",
    "cols_isa_to_merge = ['tipo_pessoa', 'segmento']\n",
    "\n",
    "# Realiza o merge usando a fun√ß√£o atualizada\n",
    "df_base = merge_and_coalesce(\n",
    "    df_left=df_base,\n",
    "    df_right=isa_historico,\n",
    "    on='cpf_cnpj',\n",
    "    cols_to_merge=cols_isa_to_merge,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verifica√ß√£o de valores nulos em 'tipo_pessoa' ap√≥s o merge final\n",
    "if df_base['tipo_pessoa'].isnull().any():\n",
    "    print(\"\\nAten√ß√£o: Existem registros com 'tipo_pessoa' nulo ap√≥s o merge final.\")\n",
    "    print(\"Esses registros n√£o foram encontrados nem em 'assoc_dash' nem em 'isa_historico'.\")\n",
    "\n",
    "df_base.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025af51",
   "metadata": {},
   "source": [
    "## 7. Chave PIX e Chave Forte √öltimos 3 Per√≠odos\n",
    "\n",
    "An√°lise do uso de chave PIX e chave forte nos √∫ltimos 3 per√≠odos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Chave PIX e Chave Forte √öltimos 3 Per√≠odos\n",
    "print(\"üìä AN√ÅLISE CHAVE PIX E CHAVE FORTE - √öLTIMOS 3 PER√çODOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Campos PIX dispon√≠veis\n",
    "campos_pix = {\n",
    "    'PIX Cadastrado': 'cad_pix',\n",
    "    'PIX Ativo': 'cad_pix_ativo',\n",
    "    'Transa√ß√µes PIX 30d': 'pix_trans_30d'\n",
    "}\n",
    "\n",
    "# Dados dos √∫ltimos 3 per√≠odos\n",
    "df_pix_3m = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].copy()\n",
    "\n",
    "print(f\"Per√≠odos analisados: {ultimos_3_meses}\")\n",
    "print(f\"Total de registros: {len(df_pix_3m):,}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Normaliza√ß√£o de campos PIX para evitar erro:\n",
    "# Algumas bases usam 'S'/'N', 'SIM'/'NAO' ou strings num√©ricas.\n",
    "# Convertemos para num√©rico (0/1) ou 0 quando n√£o reconhecido.\n",
    "# ------------------------------------------------------------------\n",
    "mapeamentos_binarios = {\n",
    "    'S': 1, 'N': 0,\n",
    "    'SIM': 1, 'NAO': 0, 'N√ÉO': 0,\n",
    "    'Y': 1, 'NOK': 0, 'OK': 1,\n",
    "    True: 1, False: 0\n",
    "}\n",
    "\n",
    "for col in ['cad_pix_ativo', 'pix_trans_30d']:\n",
    "    if col in df_pix_3m.columns:\n",
    "        # Substitui valores mape√°veis\n",
    "        df_pix_3m[col] = df_pix_3m[col].replace(mapeamentos_binarios)\n",
    "        # Converte strings num√©ricas; valores n√£o convert√≠veis viram NaN e depois 0\n",
    "        df_pix_3m[col] = pd.to_numeric(df_pix_3m[col], errors='coerce').fillna(0)\n",
    "        # Para coluna bin√°ria, for√ßa inteiro 0/1\n",
    "        if col == 'cad_pix_ativo':\n",
    "            df_pix_3m[col] = (df_pix_3m[col] > 0).astype(int)\n",
    "\n",
    "# Garantir mesma limpeza no √∫ltimo per√≠odo (usado mais abaixo)\n",
    "df_ultimo_periodo = df_pix_3m[df_pix_3m['ano_mes'] == ultimos_3_meses[-1]].copy()\n",
    "if 'cad_pix_ativo' in df_ultimo_periodo.columns:\n",
    "    df_ultimo_periodo['cad_pix_ativo'] = df_ultimo_periodo['cad_pix_ativo'].replace(mapeamentos_binarios)\n",
    "    df_ultimo_periodo['cad_pix_ativo'] = pd.to_numeric(df_ultimo_periodo['cad_pix_ativo'], errors='coerce').fillna(0)\n",
    "    df_ultimo_periodo['cad_pix_ativo'] = (df_ultimo_periodo['cad_pix_ativo'] > 0).astype(int)\n",
    "if 'pix_trans_30d' in df_ultimo_periodo.columns:\n",
    "    df_ultimo_periodo['pix_trans_30d'] = df_ultimo_periodo['pix_trans_30d'].replace(mapeamentos_binarios)\n",
    "    df_ultimo_periodo['pix_trans_30d'] = pd.to_numeric(df_ultimo_periodo['pix_trans_30d'], errors='coerce').fillna(0)\n",
    "\n",
    "# An√°lise evolutiva PIX por per√≠odo\n",
    "print(f\"\\nEVOLU√á√ÉO PIX POR PER√çODO:\")\n",
    "print(f\"{'Per√≠odo':<10} {'PIX Ativo':<12} {'% Total':<10} {'Transa√ß√µes':<12} {'Varia√ß√£o':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "evolucao_pix = []\n",
    "for i, periodo in enumerate(ultimos_3_meses):\n",
    "    df_periodo = df_pix_3m[df_pix_3m['ano_mes'] == periodo]\n",
    "    total_periodo = len(df_periodo)\n",
    "    \n",
    "    # PIX Ativo\n",
    "    if 'cad_pix_ativo' in df_periodo.columns:\n",
    "        pix_ativo = df_periodo['cad_pix_ativo'].sum()\n",
    "        perc_pix_ativo = (pix_ativo / total_periodo) * 100 if total_periodo > 0 else 0\n",
    "    else:\n",
    "        pix_ativo = 0\n",
    "        perc_pix_ativo = 0\n",
    "    \n",
    "    # Transa√ß√µes PIX\n",
    "    if 'pix_trans_30d' in df_periodo.columns:\n",
    "        pix_transacoes = df_periodo['pix_trans_30d'].sum()\n",
    "    else:\n",
    "        pix_transacoes = 0\n",
    "    \n",
    "    # Varia√ß√£o em rela√ß√£o ao per√≠odo anterior\n",
    "    if i > 0:\n",
    "        var_pix = pix_ativo - evolucao_pix[-1]['pix_ativo']\n",
    "        var_perc = ((pix_ativo / evolucao_pix[-1]['pix_ativo']) - 1) * 100 if evolucao_pix[-1]['pix_ativo'] > 0 else 0\n",
    "        variacao_str = f\"{var_pix:+,} ({var_perc:+.1f}%)\"\n",
    "    else:\n",
    "        variacao_str = \"Base\"\n",
    "    \n",
    "    \n",
    "    evolucao_pix.append({\n",
    "        'periodo': periodo,\n",
    "        'pix_ativo': pix_ativo,\n",
    "        'perc_pix_ativo': perc_pix_ativo,\n",
    "        'pix_transacoes': pix_transacoes,\n",
    "        'total_periodo': total_periodo\n",
    "    })\n",
    "\n",
    "# An√°lise PIX por tipo de pessoa\n",
    "print(f\"\\nPIX POR TIPO DE PESSOA (√∫ltimo per√≠odo):\")\n",
    "df_ultimo_periodo = df_pix_3m[df_pix_3m['ano_mes'] == ultimos_3_meses[-1]]\n",
    "\n",
    "for pessoa in ['PF', 'PJ']:\n",
    "    df_pessoa = df_ultimo_periodo[df_ultimo_periodo['tipo_pessoa'] == pessoa]\n",
    "    if not df_pessoa.empty:\n",
    "        total_pessoa = len(df_pessoa)\n",
    "        \n",
    "        # PIX Ativo\n",
    "        if 'cad_pix_ativo' in df_pessoa.columns:\n",
    "            pix_ativo_pessoa = df_pessoa['cad_pix_ativo'].sum()\n",
    "            perc_pix_pessoa = (pix_ativo_pessoa / total_pessoa) * 100\n",
    "        else:\n",
    "            pix_ativo_pessoa = 0\n",
    "            perc_pix_pessoa = 0\n",
    "        \n",
    "        # Transa√ß√µes m√©dias\n",
    "        if 'pix_trans_30d' in df_pessoa.columns:\n",
    "            # Se existir coluna de PIX ativo, calcula m√©dia apenas para ativos\n",
    "            if 'cad_pix_ativo' in df_pessoa.columns:\n",
    "                ativos_mask = df_pessoa['cad_pix_ativo'] == 1\n",
    "                if ativos_mask.any():\n",
    "                    trans_media = df_pessoa.loc[ativos_mask, 'pix_trans_30d'].mean()\n",
    "                else:\n",
    "                    trans_media = 0\n",
    "            else:\n",
    "                trans_media = df_pessoa['pix_trans_30d'].mean()\n",
    "            if pd.isna(trans_media):\n",
    "                trans_media = 0\n",
    "        else:\n",
    "            trans_media = 0\n",
    "        \n",
    "        print(f\"  {pessoa}: {pix_ativo_pessoa:,}/{total_pessoa:,} ({perc_pix_pessoa:.1f}%) | Trans/m√™s: {trans_media:.1f}\")\n",
    "\n",
    "# Chaves fortes (an√°lise baseada em campos dispon√≠veis)\n",
    "print(f\"\\nAN√ÅLISE DE CHAVES FORTES:\")\n",
    "\n",
    "# Assumindo que chaves fortes s√£o: CPF, CNPJ, telefone, email\n",
    "# Como n√£o temos detalhamento, usamos PIX ativo como proxy para chaves fortes\n",
    "if 'cad_pix_ativo' in df_ultimo_periodo.columns:\n",
    "    chaves_fortes = df_ultimo_periodo['cad_pix_ativo'].sum()\n",
    "    total_ultimo = len(df_ultimo_periodo)\n",
    "    perc_fortes = (chaves_fortes / total_ultimo) * 100\n",
    "    \n",
    "    print(f\"  Associados com PIX ativo (proxy chaves fortes): {chaves_fortes:,} ({perc_fortes:.1f}%)\")\n",
    "    \n",
    "    # Por segmento\n",
    "    for pessoa in ['PF', 'PJ']:\n",
    "        df_pessoa = df_ultimo_periodo[df_ultimo_periodo['tipo_pessoa'] == pessoa]\n",
    "        if not df_pessoa.empty:\n",
    "            fortes_pessoa = df_pessoa['cad_pix_ativo'].sum()\n",
    "            total_pessoa = len(df_pessoa)\n",
    "            perc_pessoa = (fortes_pessoa / total_pessoa) * 100\n",
    "            print(f\"    {pessoa}: {fortes_pessoa:,}/{total_pessoa:,} ({perc_pessoa:.1f}%)\")\n",
    "\n",
    "# An√°lise de intensidade de uso PIX\n",
    "if 'pix_trans_30d' in df_ultimo_periodo.columns:\n",
    "    df_pix_users = df_ultimo_periodo[df_ultimo_periodo['cad_pix_ativo'] == 1].copy()\n",
    "    \n",
    "    if not df_pix_users.empty:\n",
    "        print(f\"\\nINTENSIDADE DE USO PIX (usu√°rios ativos):\")\n",
    "        \n",
    "        # Classificar por intensidade\n",
    "        df_pix_users['intensidade_pix'] = pd.cut(\n",
    "            df_pix_users['pix_trans_30d'],\n",
    "            bins=[0, 1, 5, 15, float('inf')],\n",
    "            labels=['Muito_Baixo', 'Baixo', 'M√©dio', 'Alto']\n",
    "        )\n",
    "        \n",
    "        dist_intensidade = df_pix_users['intensidade_pix'].value_counts()\n",
    "        total_pix_users = len(df_pix_users)\n",
    "        \n",
    "        print(f\"  Total usu√°rios PIX: {total_pix_users:,}\")\n",
    "        for intensidade, qtd in dist_intensidade.items():\n",
    "            perc = (qtd / total_pix_users) * 100\n",
    "            print(f\"    {intensidade}: {qtd:,} ({perc:.1f}%)\")\n",
    "        \n",
    "        # M√©dia de transa√ß√µes\n",
    "        media_trans = df_pix_users['pix_trans_30d'].mean()\n",
    "        mediana_trans = df_pix_users['pix_trans_30d'].median()\n",
    "        print(f\"  M√©dia transa√ß√µes/m√™s: {media_trans:.1f}\")\n",
    "        print(f\"  Mediana transa√ß√µes/m√™s: {mediana_trans:.1f}\")\n",
    "\n",
    "# PIX vs Principalidade\n",
    "if 'pontos_principalidade' in df_ultimo_periodo.columns and 'cad_pix_ativo' in df_ultimo_periodo.columns:\n",
    "    print(f\"\\nPIX vs PRINCIPALIDADE:\")\n",
    "    \n",
    "    # M√©dia de principalidade com/sem PIX\n",
    "    princ_com_pix = df_ultimo_periodo[df_ultimo_periodo['cad_pix_ativo'] == 1]['pontos_principalidade'].mean()\n",
    "    princ_sem_pix = df_ultimo_periodo[df_ultimo_periodo['cad_pix_ativo'] == 0]['pontos_principalidade'].mean()\n",
    "    \n",
    "    print(f\"  Com PIX: {princ_com_pix:.1f} pontos de principalidade\")\n",
    "    print(f\"  Sem PIX: {princ_sem_pix:.1f} pontos de principalidade\")\n",
    "    print(f\"  Diferen√ßa: {princ_com_pix - princ_sem_pix:+.1f} pontos\")\n",
    "\n",
    "# Resumo PIX\n",
    "if evolucao_pix:\n",
    "    primeiro_periodo = evolucao_pix[0]\n",
    "    ultimo_periodo = evolucao_pix[-1]\n",
    "    \n",
    "    crescimento_pix = ultimo_periodo['pix_ativo'] - primeiro_periodo['pix_ativo']\n",
    "    crescimento_perc = ((ultimo_periodo['pix_ativo'] / primeiro_periodo['pix_ativo']) - 1) * 100 if primeiro_periodo['pix_ativo'] > 0 else 0\n",
    "    \n",
    "    resumo_pix = {\n",
    "        'pix_ativo_atual': ultimo_periodo['pix_ativo'],\n",
    "        'perc_penetracao': ultimo_periodo['perc_pix_ativo'],\n",
    "        'crescimento_absoluto': crescimento_pix,\n",
    "        'crescimento_percentual': crescimento_perc,\n",
    "        'transacoes_totais': ultimo_periodo['pix_transacoes']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìã RESUMO EXECUTIVO - PIX\")\n",
    "    print(f\"PIX ativo atual: {resumo_pix['pix_ativo_atual']:,} ({resumo_pix['perc_penetracao']:.1f}%)\")\n",
    "    print(f\"Crescimento 3M: {resumo_pix['crescimento_absoluto']:+,} ({resumo_pix['crescimento_percentual']:+.1f}%)\")\n",
    "    print(f\"Transa√ß√µes totais: {resumo_pix['transacoes_totais']:,}\")\n",
    "    \n",
    "    globals()['resumo_pix'] = resumo_pix\n",
    "    globals()['evolucao_pix'] = evolucao_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise Cash-In vs Cash-Out √öltimos 3 Meses\n",
    "print(\"üìä AN√ÅLISE CASH-IN vs CASH-OUT - √öLTIMOS 3 MESES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar disponibilidade dos campos\n",
    "campos_cash = ['cash_in', 'cash_out', 'cash_total']\n",
    "campos_disponiveis = [c for c in campos_cash if c in df_base.columns]\n",
    "\n",
    "print(f\"Campos de cash dispon√≠veis: {campos_disponiveis}\")\n",
    "\n",
    "if 'cash_in' in df_base.columns and 'cash_out' in df_base.columns:\n",
    "    \n",
    "    # Dados dos √∫ltimos 3 meses\n",
    "    df_cash_3m = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].copy()\n",
    "    \n",
    "    # Calcular diferen√ßa cash-in vs cash-out\n",
    "    df_cash_3m['saldo_cash'] = df_cash_3m['cash_in'] - df_cash_3m['cash_out']\n",
    "    df_cash_3m['cash_total_calc'] = df_cash_3m['cash_in'] + df_cash_3m['cash_out']\n",
    "    \n",
    "    print(f\"\\nAN√ÅLISE POR PER√çODO:\")\n",
    "    print(f\"{'Per√≠odo':<10} {'Cash-In':<15} {'Cash-Out':<15} {'Saldo':<15} {'Total Mov':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    analise_cash_periodo = []\n",
    "    \n",
    "    for periodo in ultimos_3_meses:\n",
    "        df_periodo = df_cash_3m[df_cash_3m['ano_mes'] == periodo]\n",
    "        \n",
    "        cash_in_total = df_periodo['cash_in'].sum()\n",
    "        cash_out_total = df_periodo['cash_out'].sum()\n",
    "        saldo_periodo = cash_in_total - cash_out_total\n",
    "        movimentacao_total = cash_in_total + cash_out_total\n",
    "        \n",
    "        print(f\"{periodo:<10} {cash_in_total:<15,.0f} {cash_out_total:<15,.0f} \"\n",
    "              f\"{saldo_periodo:<15,.0f} {movimentacao_total:<15,.0f}\")\n",
    "        \n",
    "        analise_cash_periodo.append({\n",
    "            'periodo': periodo,\n",
    "            'cash_in': cash_in_total,\n",
    "            'cash_out': cash_out_total,\n",
    "            'saldo': saldo_periodo,\n",
    "            'movimentacao_total': movimentacao_total,\n",
    "            'num_associados': len(df_periodo)\n",
    "        })\n",
    "    \n",
    "    # Estat√≠sticas gerais (√∫ltimo per√≠odo)\n",
    "    df_ultimo = df_cash_3m[df_cash_3m['ano_mes'] == ultimos_3_meses[-1]]\n",
    "    \n",
    "    print(f\"\\nESTAT√çSTICAS √öLTIMO PER√çODO ({ultimos_3_meses[-1]}):\")\n",
    "    print(f\"  Total associados: {len(df_ultimo):,}\")\n",
    "    \n",
    "    # Estat√≠sticas cash-in\n",
    "    cash_in_stats = df_ultimo['cash_in'].describe()\n",
    "    print(f\"\\n  Cash-In:\")\n",
    "    print(f\"    M√©dia por associado: R$ {cash_in_stats['mean']:,.2f}\")\n",
    "    print(f\"    Mediana: R$ {cash_in_stats['50%']:,.2f}\")\n",
    "    print(f\"    Total: R$ {cash_in_stats['count'] * cash_in_stats['mean']:,.0f}\")\n",
    "    \n",
    "    # Estat√≠sticas cash-out\n",
    "    cash_out_stats = df_ultimo['cash_out'].describe()\n",
    "    print(f\"\\n  Cash-Out:\")\n",
    "    print(f\"    M√©dia por associado: R$ {cash_out_stats['mean']:,.2f}\")\n",
    "    print(f\"    Mediana: R$ {cash_out_stats['50%']:,.2f}\")\n",
    "    print(f\"    Total: R$ {cash_out_stats['count'] * cash_out_stats['mean']:,.0f}\")\n",
    "    \n",
    "    # Saldo m√©dio\n",
    "    saldo_medio = df_ultimo['saldo_cash'].mean()\n",
    "    saldo_mediano = df_ultimo['saldo_cash'].median()\n",
    "    print(f\"\\n  Saldo (Cash-In - Cash-Out):\")\n",
    "    print(f\"    Saldo m√©dio: R$ {saldo_medio:,.2f}\")\n",
    "    print(f\"    Saldo mediano: R$ {saldo_mediano:,.2f}\")\n",
    "    \n",
    "    \n",
    "    # Classifica√ß√£o por tipo de fluxo\n",
    "    df_ultimo['tipo_fluxo'] = np.select([\n",
    "        df_ultimo['saldo_cash'] > 100,\n",
    "        df_ultimo['saldo_cash'] < -100,\n",
    "        True\n",
    "    ], [\n",
    "        'Entrada_Liquida',\n",
    "        'Saida_Liquida', \n",
    "        'Equilibrio'\n",
    "    ])\n",
    "    \n",
    "    dist_fluxo = df_ultimo['tipo_fluxo'].value_counts()\n",
    "    total_ultimo = len(df_ultimo)\n",
    "    \n",
    "    print(f\"\\nDISTRIBUI√á√ÉO POR TIPO DE FLUXO:\")\n",
    "    for tipo, qtd in dist_fluxo.items():\n",
    "        perc = (qtd / total_ultimo) * 100\n",
    "        print(f\"  {tipo.replace('_', ' ')}: {qtd:,} ({perc:.1f}%)\")\n",
    "    \n",
    "    # An√°lise por tipo de pessoa\n",
    "    print(f\"\\nCASH FLOW POR TIPO DE PESSOA:\")\n",
    "    \n",
    "    for pessoa in ['PF', 'PJ']:\n",
    "        df_pessoa = df_ultimo[df_ultimo['tipo_pessoa'] == pessoa]\n",
    "        if not df_pessoa.empty:\n",
    "            cash_in_pessoa = df_pessoa['cash_in'].mean()\n",
    "            cash_out_pessoa = df_pessoa['cash_out'].mean()\n",
    "            saldo_pessoa = df_pessoa['saldo_cash'].mean()\n",
    "            \n",
    "            print(f\"\\n  {pessoa} (n={len(df_pessoa):,}):\")\n",
    "            print(f\"    Cash-In m√©dio: R$ {cash_in_pessoa:,.2f}\")\n",
    "            print(f\"    Cash-Out m√©dio: R$ {cash_out_pessoa:,.2f}\")\n",
    "            print(f\"    Saldo m√©dio: R$ {saldo_pessoa:,.2f}\")\n",
    "    \n",
    "    # Top movimentadores\n",
    "    print(f\"\\nTOP 10 MAIORES MOVIMENTA√á√ïES TOTAIS:\")\n",
    "    top_movimentacao = df_ultimo.nlargest(10, 'cash_total_calc')\n",
    "    \n",
    "    for i, row in enumerate(top_movimentacao.itertuples(), 1):\n",
    "        cpf_mask = str(row.cpf_cnpj)[-4:] if len(str(row.cpf_cnpj)) >= 4 else str(row.cpf_cnpj)\n",
    "        print(f\"  {i:2d}. CPF: ...{cpf_mask} | {row.tipo_pessoa} | \"\n",
    "              f\"In: R${row.cash_in:,.0f} | Out: R${row.cash_out:,.0f} | \"\n",
    "              f\"Saldo: R${row.saldo_cash:+,.0f}\")\n",
    "    \n",
    "    # Correla√ß√£o cash vs principalidade\n",
    "    if 'pontos_principalidade' in df_ultimo.columns:\n",
    "        print(f\"\\nCORRELA√á√ÉO CASH vs PRINCIPALIDADE:\")\n",
    "        \n",
    "        corr_cash_in = df_ultimo['cash_in'].corr(df_ultimo['pontos_principalidade'])\n",
    "        corr_cash_out = df_ultimo['cash_out'].corr(df_ultimo['pontos_principalidade'])\n",
    "        corr_saldo = df_ultimo['saldo_cash'].corr(df_ultimo['pontos_principalidade'])\n",
    "        \n",
    "        print(f\"  Cash-In vs Principalidade: {corr_cash_in:.3f}\")\n",
    "        print(f\"  Cash-Out vs Principalidade: {corr_cash_out:.3f}\")\n",
    "        print(f\"  Saldo vs Principalidade: {corr_saldo:.3f}\")\n",
    "    \n",
    "    # Tend√™ncia de evolu√ß√£o\n",
    "    if len(analise_cash_periodo) >= 2:\n",
    "        print(f\"\\nTEND√äNCIA DE EVOLU√á√ÉO:\")\n",
    "        \n",
    "        primeiro = analise_cash_periodo[0]\n",
    "        ultimo = analise_cash_periodo[-1]\n",
    "        \n",
    "        var_cash_in = ((ultimo['cash_in'] / primeiro['cash_in']) - 1) * 100 if primeiro['cash_in'] > 0 else 0\n",
    "        var_cash_out = ((ultimo['cash_out'] / primeiro['cash_out']) - 1) * 100 if primeiro['cash_out'] > 0 else 0\n",
    "        var_movimentacao = ((ultimo['movimentacao_total'] / primeiro['movimentacao_total']) - 1) * 100 if primeiro['movimentacao_total'] > 0 else 0\n",
    "        \n",
    "        print(f\"  Cash-In: {var_cash_in:+.1f}%\")\n",
    "        print(f\"  Cash-Out: {var_cash_out:+.1f}%\")\n",
    "        print(f\"  Movimenta√ß√£o Total: {var_movimentacao:+.1f}%\")\n",
    "    \n",
    "    # Resumo cash\n",
    "    resumo_cash = {\n",
    "        'cash_in_total': analise_cash_periodo[-1]['cash_in'],\n",
    "        'cash_out_total': analise_cash_periodo[-1]['cash_out'],\n",
    "        'saldo_total': analise_cash_periodo[-1]['saldo'],\n",
    "        'movimentacao_total': analise_cash_periodo[-1]['movimentacao_total'],\n",
    "        'saldo_medio_associado': saldo_medio,\n",
    "        'entrada_liquida': dist_fluxo.get('Entrada_Liquida', 0),\n",
    "        'saida_liquida': dist_fluxo.get('Saida_Liquida', 0)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìã RESUMO EXECUTIVO - CASH FLOW\")\n",
    "    print(f\"Movimenta√ß√£o total: R$ {resumo_cash['movimentacao_total']:,.0f}\")\n",
    "    print(f\"Saldo geral: R$ {resumo_cash['saldo_total']:+,.0f}\")\n",
    "    print(f\"Entrada l√≠quida: {resumo_cash['entrada_liquida']:,} associados\")\n",
    "    print(f\"Sa√≠da l√≠quida: {resumo_cash['saida_liquida']:,} associados\")\n",
    "    \n",
    "    globals()['resumo_cash'] = resumo_cash\n",
    "    globals()['analise_cash_periodo'] = analise_cash_periodo\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Campos de cash-in e cash-out n√£o dispon√≠veis para an√°lise completa\")\n",
    "    resumo_cash = {'cash_in_total': 0, 'cash_out_total': 0, 'saldo_total': 0}\n",
    "\n",
    "# Normalizando os campos booleanos para True/False\n",
    "campos_bool = [\n",
    "    'tem_conta_corrente', 'tem_poupanca', 'tem_investimento', 'tem_previdencia', \n",
    "    'tem_seguro', 'tem_consorcio', 'tem_cartao_credito', 'tem_credito', \n",
    "    'tem_cambio', 'tem_cobranca'\n",
    "]\n",
    "\n",
    "for col in campos_bool:\n",
    "    if col in df_base.columns:\n",
    "        df_base[col] = df_base[col].fillna(False).astype(bool)\n",
    "\n",
    "# Convertendo colunas de data para datetime\n",
    "for col in ['data_abertura', 'data_encerramento']:\n",
    "    if col in df_base.columns:\n",
    "        df_base[col] = pd.to_datetime(df_base[col], errors='coerce')\n",
    "\n",
    "df_base.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56d03d",
   "metadata": {},
   "source": [
    "## 9. Productos Faltantes: Categor√≠a 1 y Categor√≠a 2\n",
    "\n",
    "Identificaci√≥n de brechas de productos en dos categor√≠as estrat√©gicas y c√°lculo de cobertura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de Produtos Faltantes (Categoria 1 e 2)\n",
    "print(\"üìä AN√ÅLISE DE PRODUTOS FALTANTES - CATEGORIAS 1 E 2\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Defini√ß√£o das categorias (ajustar conforme necessidade)\n",
    "CATEGORIA_1 = {\n",
    "    'PIX Ativo': 'cad_pix_ativo',\n",
    "    'Cart√£o D√©bito': 'possui_cartao_debito',\n",
    "    'App/Canais Digitais': 'transacao_app',\n",
    "    'D√©bito Autom√°tico': 'debito_conta_ativo'\n",
    "}\n",
    "\n",
    "CATEGORIA_2 = {\n",
    "    'Cart√£o Cr√©dito': 'possui_cartao_credito',\n",
    "    'Credenciamento': 'possui_adquirencia',\n",
    "    'Cobran√ßa': 'possui_cobranca',\n",
    "    'Folha Pagamento': 'possui_folha_pagamento',\n",
    "    'Domic√≠lio': 'possui_domicilio',\n",
    "    'Open Finance': 'ativou_open_finance'\n",
    "}\n",
    "\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_prod_atual = df_base[df_base['ano_mes'] == ultimo_mes].copy()\n",
    "\n",
    "# Garantir exist√™ncia das colunas (criar se ausentes)\n",
    "for col in list(CATEGORIA_1.values()) + list(CATEGORIA_2.values()):\n",
    "    if col not in df_prod_atual.columns:\n",
    "        df_prod_atual[col] = 0\n",
    "\n",
    "# Calcular quantidade de produtos usados por categoria\n",
    "cat1_cols = list(CATEGORIA_1.values())\n",
    "cat2_cols = list(CATEGORIA_2.values())\n",
    "\n",
    "df_prod_atual['cat1_usados'] = df_prod_atual[cat1_cols].sum(axis=1)\n",
    "df_prod_atual['cat2_usados'] = df_prod_atual[cat2_cols].sum(axis=1)\n",
    "\n",
    "df_prod_atual['cat1_faltantes'] = len(cat1_cols) - df_prod_atual['cat1_usados']\n",
    "df_prod_atual['cat2_faltantes'] = len(cat2_cols) - df_prod_atual['cat2_usados']\n",
    "\n",
    "# Cobertura (percentual de produtos utilizados em cada categoria)\n",
    "df_prod_atual['cobertura_cat1'] = df_prod_atual['cat1_usados'] / len(cat1_cols)\n",
    "df_prod_atual['cobertura_cat2'] = df_prod_atual['cat2_usados'] / len(cat2_cols)\n",
    "\n",
    "# Distribui√ß√£o de faltantes\n",
    "print(\"\\nDISTRIBUI√á√ÉO - CATEGORIA 1 (Produtos Transacionais B√°sicos):\")\n",
    "for falt in range(len(cat1_cols) + 1):\n",
    "    qtd = (df_prod_atual['cat1_faltantes'] == falt).sum()\n",
    "    perc = qtd / len(df_prod_atual) * 100\n",
    "    print(f\"  Faltam {falt} prod.: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "print(\"\\nDISTRIBUI√á√ÉO - CATEGORIA 2 (Cross-Sell / Relacionamento):\")\n",
    "for falt in range(len(cat2_cols) + 1):\n",
    "    qtd = (df_prod_atual['cat2_faltantes'] == falt).sum()\n",
    "    perc = qtd / len(df_prod_atual) * 100\n",
    "    print(f\"  Faltam {falt} prod.: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Penetra√ß√£o por produto (ordenar)\n",
    "print(\"\\nPENETRA√á√ÉO POR PRODUTO (√öLTIMO PER√çODO):\")\n",
    "penetracao = []\n",
    "for nome, col in {**CATEGORIA_1, **CATEGORIA_2}.items():\n",
    "    if col in df_prod_atual.columns:\n",
    "        usuarios = df_prod_atual[col].sum()\n",
    "        perc = usuarios / len(df_prod_atual) * 100 if len(df_prod_atual) > 0 else 0\n",
    "        penetracao.append({'produto': nome, 'usuarios': int(usuarios), 'perc': perc})\n",
    "\n",
    "df_penetracao = pd.DataFrame(penetracao).sort_values('perc', ascending=False)\n",
    "for row in df_penetracao.itertuples():\n",
    "    print(f\"  {row.produto:<18} {row.usuarios:>8,} ({row.perc:5.1f}%)\")\n",
    "\n",
    "# Identificar oportunidades (associados com baixa cobertura transacional e alta principalidade)\n",
    "if 'pontos_principalidade' in df_prod_atual.columns:\n",
    "    mediana_princ = df_prod_atual['pontos_principalidade'].median()\n",
    "    oportunidades_cat1 = df_prod_atual[(df_prod_atual['cobertura_cat1'] < 0.5) & (df_prod_atual['pontos_principalidade'] > mediana_princ)]\n",
    "    oportunidades_cat2 = df_prod_atual[(df_prod_atual['cobertura_cat2'] < 0.5) & (df_prod_atual['pontos_principalidade'] > mediana_princ)]\n",
    "    print(f\"\\nOPORTUNIDADES - Alta principalidade mas baixa cobertura transacional (<50%): {len(oportunidades_cat1):,}\")\n",
    "    print(f\"OPORTUNIDADES - Alta principalidade mas baixa cobertura cross-sell (<50%): {len(oportunidades_cat2):,}\")\n",
    "\n",
    "# Resumo executivo\n",
    "resumo_prod_faltantes = {\n",
    "    'total': len(df_prod_atual),\n",
    "    'cobertura_cat1_media': df_prod_atual['cobertura_cat1'].mean(),\n",
    "    'cobertura_cat2_media': df_prod_atual['cobertura_cat2'].mean(),\n",
    "    'sem_prod_cat1': (df_prod_atual['cat1_usados'] == 0).sum(),\n",
    "    'sem_prod_cat2': (df_prod_atual['cat2_usados'] == 0).sum()\n",
    "}\n",
    "\n",
    "print(\"\\nüìã RESUMO EXECUTIVO - PRODUTOS FALTANTES\")\n",
    "print(f\"Cobertura m√©dia Cat 1: {resumo_prod_faltantes['cobertura_cat1_media']*100:,.1f}%\")\n",
    "print(f\"Cobertura m√©dia Cat 2: {resumo_prod_faltantes['cobertura_cat2_media']*100:,.1f}%\")\n",
    "print(f\"Sem nenhum produto Cat1: {resumo_prod_faltantes['sem_prod_cat1']:,} ({resumo_prod_faltantes['sem_prod_cat1']/len(df_prod_atual)*100:.1f}%)\")\n",
    "print(f\"Sem nenhum produto Cat2: {resumo_prod_faltantes['sem_prod_cat2']:,} ({resumo_prod_faltantes['sem_prod_cat2']/len(df_prod_atual)*100:.1f}%)\")\n",
    "\n",
    "globals()['df_produtos_atual'] = df_prod_atual\n",
    "globals()['df_penetracao_produtos'] = df_penetracao\n",
    "globals()['resumo_prod_faltantes'] = resumo_prod_faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c8b99",
   "metadata": {},
   "source": [
    "## 10. SOW Sicredi vs Fora\n",
    "\n",
    "An√°lise do indicador de Share of Wallet (SOW) para identificar potencial de captura adicional de relacionamento entre associados dentro e fora da cooperativa. Caso a m√©trica `sow` n√£o esteja dispon√≠vel na base, criamos um proxy a partir de pontos de principalidade e uso de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise SOW Sicredi vs Fora\n",
    "print(\"üìä AN√ÅLISE SOW (SHARE OF WALLET) - PROXY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar exist√™ncia ou criar proxy\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_sow = df_base[df_base['ano_mes'] == ultimo_mes].copy()\n",
    "\n",
    "if 'sow' not in df_sow.columns:\n",
    "    # Proxy simples: normalizar pontos de principalidade 0-1 e ponderar por cobertura de produtos transacionais\n",
    "    produtos_trans = [c for c in ['cad_pix_ativo','transacao_app','possui_cartao_debito','debito_conta_ativo'] if c in df_sow.columns]\n",
    "    if produtos_trans:\n",
    "        df_sow['cobertura_trans'] = df_sow[produtos_trans].mean(axis=1)\n",
    "    else:\n",
    "        df_sow['cobertura_trans'] = 0\n",
    "    princ_min = df_sow['pontos_principalidade'].min()\n",
    "    princ_max = df_sow['pontos_principalidade'].max()\n",
    "    escala = (df_sow['pontos_principalidade'] - princ_min) / (princ_max - princ_min) if princ_max > princ_min else 0\n",
    "    df_sow['sow_proxy'] = (0.6 * escala + 0.4 * df_sow['cobertura_trans']).clip(0,1)\n",
    "    sow_col = 'sow_proxy'\n",
    "else:\n",
    "    sow_col = 'sow'\n",
    "\n",
    "# Faixas de SOW\n",
    "bins = [0,0.25,0.5,0.75,1.01]\n",
    "labels = ['Baixo','M√©dio-Baixo','M√©dio-Alto','Alto']\n",
    "df_sow['faixa_sow'] = pd.cut(df_sow[sow_col], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Distribui√ß√£o geral\n",
    "dist_sow = df_sow['faixa_sow'].value_counts().reindex(labels)\n",
    "print(\"\\nDISTRIBUI√á√ÉO GERAL SOW:\")\n",
    "for faixa, qtd in dist_sow.items():\n",
    "    perc = qtd / len(df_sow) * 100 if len(df_sow) else 0\n",
    "    print(f\"  {faixa:<11}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# SOW por tipo de pessoa\n",
    "print(\"\\nSOW M√âDIO POR TIPO DE PESSOA:\")\n",
    "for tp in ['PF','PJ']:\n",
    "    subset = df_sow[df_sow['tipo_pessoa']==tp]\n",
    "    if not subset.empty:\n",
    "        print(f\"  {tp}: {subset[sow_col].mean():.3f} (n={len(subset):,})\")\n",
    "\n",
    "# Rela√ß√£o SOW x Principalidade\n",
    "corr_sow_princ = df_sow[sow_col].corr(df_sow['pontos_principalidade']) if 'pontos_principalidade' in df_sow.columns else np.nan\n",
    "print(f\"\\nCorrela√ß√£o SOW vs Principalidade: {corr_sow_princ:.3f}\")\n",
    "\n",
    "# Oportunidades: alta principalidade (>=P75) e SOW baixo/medio-baixo\n",
    "p75_princ = df_sow['pontos_principalidade'].quantile(0.75)\n",
    "oportunidades_sow = df_sow[(df_sow['pontos_principalidade']>=p75_princ) & (df_sow['faixa_sow'].isin(['Baixo','M√©dio-Baixo']))]\n",
    "print(f\"\\nOportunidades de expans√£o (alta principalidade & SOW<=0.5): {len(oportunidades_sow):,}\")\n",
    "\n",
    "# Resumo executivo\n",
    "resumo_sow = {\n",
    "    'total': len(df_sow),\n",
    "    'sow_medio': df_sow[sow_col].mean(),\n",
    "    'alta_participacao': (df_sow['faixa_sow']=='Alto').sum(),\n",
    "    'baixo_share': (df_sow['faixa_sow']=='Baixo').sum(),\n",
    "    'corr_princ': corr_sow_princ,\n",
    "    'oportunidades': len(oportunidades_sow)\n",
    "}\n",
    "\n",
    "print(\"\\nüìã RESUMO EXECUTIVO - SOW\")\n",
    "print(f\"SOW m√©dio: {resumo_sow['sow_medio']:.3f}\")\n",
    "print(f\"Alto SOW: {resumo_sow['alta_participacao']:,} ({resumo_sow['alta_participacao']/len(df_sow)*100:.1f}%)\")\n",
    "print(f\"Baixo SOW: {resumo_sow['baixo_share']:,} ({resumo_sow['baixo_share']/len(df_sow)*100:.1f}%)\")\n",
    "print(f\"Correla√ß√£o com Principalidade: {resumo_sow['corr_princ']:.3f}\")\n",
    "print(f\"Oportunidades expans√£o: {resumo_sow['oportunidades']:,}\")\n",
    "\n",
    "globals()['resumo_sow'] = resumo_sow\n",
    "globals()['df_sow'] = df_sow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bebe8",
   "metadata": {},
   "source": [
    "## 11. An√°lise PJ: Domic√≠lio, Cobran√ßa, PIX e Uso Transacional\n",
    "\n",
    "Foco em produtos cr√≠ticos para relacionamento PJ: domic√≠lio, cobran√ßa, credenciamento, folha pagamento e ado√ß√£o de PIX. Avalia√ß√£o de penetra√ß√£o, gaps e rela√ß√£o com principalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise PJ detalhada\n",
    "print(\"üìä AN√ÅLISE PJ - PRODUTOS E USO TRANSACIONAL\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_pj = df_base[(df_base['ano_mes']==ultimo_mes) & (df_base['tipo_pessoa']=='PJ')].copy()\n",
    "print(f\"Total PJ analisados: {len(df_pj):,}\")\n",
    "\n",
    "produtos_pj = {\n",
    "    'Domic√≠lio': 'possui_domicilio',\n",
    "    'Cobran√ßa': 'possui_cobranca',\n",
    "    'Credenciamento': 'possui_adquirencia',\n",
    "    'Folha Pagamento': 'possui_folha_pagamento',\n",
    "    'PIX Ativo': 'cad_pix_ativo'\n",
    "}\n",
    "\n",
    "# Garantir colunas\n",
    "for c in produtos_pj.values():\n",
    "    if c not in df_pj.columns:\n",
    "        df_pj[c] = 0\n",
    "\n",
    "# Penetra√ß√£o\n",
    "penetracao_pj = []\n",
    "for nome, col in produtos_pj.items():\n",
    "    usuarios = df_pj[col].sum()\n",
    "    perc = usuarios/len(df_pj)*100 if len(df_pj)>0 else 0\n",
    "    penetracao_pj.append({'produto': nome, 'usuarios': int(usuarios), 'perc': perc})\n",
    "\n",
    "df_pen_pj = pd.DataFrame(penetracao_pj).sort_values('perc', ascending=False)\n",
    "print(\"\\nPENETRA√á√ÉO DE PRODUTOS PJ:\")\n",
    "for r in df_pen_pj.itertuples():\n",
    "    print(f\"  {r.produto:<16} {r.usuarios:>7,} ({r.perc:5.1f}%)\")\n",
    "\n",
    "# Cobertura composta (quantos desses chave usa)\n",
    "cols_chave = list(produtos_pj.values())\n",
    "df_pj['prod_chave_usados'] = df_pj[cols_chave].sum(axis=1)\n",
    "print(\"\\nDistribui√ß√£o n¬∫ de produtos chave usados:\")\n",
    "for k in range(len(cols_chave)+1):\n",
    "    qtd = (df_pj['prod_chave_usados']==k).sum()\n",
    "    print(f\"  Usa {k} prod.: {qtd:,} ({qtd/len(df_pj)*100:.1f}%)\")\n",
    "\n",
    "# PIX transa√ß√µes (intensidade) se existir\n",
    "if 'pix_trans_30d' in df_pj.columns and 'cad_pix_ativo' in df_pj.columns:\n",
    "    ativos_pix = df_pj[df_pj['cad_pix_ativo']==1]\n",
    "    if not ativos_pix.empty:\n",
    "        ativos_pix['intensidade_pix'] = pd.cut(\n",
    "            ativos_pix['pix_trans_30d'],\n",
    "            bins=[0,1,5,15,float('inf')],\n",
    "            labels=['Muito_Baixo','Baixo','M√©dio','Alto']\n",
    "        )\n",
    "        print(\"\\nINTENSIDADE PIX (PJ com PIX Ativo):\")\n",
    "        dist_int = ativos_pix['intensidade_pix'].value_counts()\n",
    "        for cat, qtd in dist_int.items():\n",
    "            print(f\"  {cat:<12}: {qtd:,} ({qtd/len(ativos_pix)*100:.1f}%)\")\n",
    "\n",
    "# Rela√ß√£o cada produto x principalidade m√©dia\n",
    "if 'pontos_principalidade' in df_pj.columns:\n",
    "    print(\"\\nPRINCIPALIDADE M√âDIA POR ADO√á√ÉO DE PRODUTO:\")\n",
    "    princ_med_base = df_pj['pontos_principalidade'].mean()\n",
    "    for nome, col in produtos_pj.items():\n",
    "        if col in df_pj.columns:\n",
    "            media_com = df_pj[df_pj[col]==1]['pontos_principalidade'].mean()\n",
    "            media_sem = df_pj[df_pj[col]==0]['pontos_principalidade'].mean()\n",
    "            uplift = media_com - media_sem\n",
    "            print(f\"  {nome:<16} Com: {media_com:7.2f} | Sem: {media_sem:7.2f} | Uplift: {uplift:+.2f}\")\n",
    "\n",
    "# Oportunidades: alta principalidade mas sem domic√≠lio ou cobran√ßa\n",
    "if 'pontos_principalidade' in df_pj.columns:\n",
    "    p75 = df_pj['pontos_principalidade'].quantile(0.75)\n",
    "    oportunidades_domicilio = df_pj[(df_pj['pontos_principalidade']>=p75) & (df_pj['possui_domicilio']==0)]\n",
    "    oportunidades_cobranca = df_pj[(df_pj['pontos_principalidade']>=p75) & (df_pj['possui_cobranca']==0)]\n",
    "    print(f\"\\nOportunidades Domic√≠lio (alto princ, sem domic√≠lio): {len(oportunidades_domicilio):,}\")\n",
    "    print(f\"Oportunidades Cobran√ßa (alto princ, sem cobran√ßa): {len(oportunidades_cobranca):,}\")\n",
    "\n",
    "resumo_pj = {\n",
    "    'total_pj': len(df_pj),\n",
    "    'pix_ativo': df_pj['cad_pix_ativo'].sum() if 'cad_pix_ativo' in df_pj.columns else 0,\n",
    "    'domicilio': df_pj['possui_domicilio'].sum() if 'possui_domicilio' in df_pj.columns else 0,\n",
    "    'cobranca': df_pj['possui_cobranca'].sum() if 'possui_cobranca' in df_pj.columns else 0\n",
    "}\n",
    "print(\"\\nüìã RESUMO EXECUTIVO - PJ\")\n",
    "print(f\"Total PJ: {resumo_pj['total_pj']:,}\")\n",
    "print(f\"Domic√≠lio: {resumo_pj['domicilio']:,} ({resumo_pj['domicilio']/len(df_pj)*100:.1f}%)\")\n",
    "print(f\"Cobran√ßa: {resumo_pj['cobranca']:,} ({resumo_pj['cobranca']/len(df_pj)*100:.1f}%)\")\n",
    "print(f\"PIX Ativo: {resumo_pj['pix_ativo']:,} ({resumo_pj['pix_ativo']/len(df_pj)*100:.1f}%)\")\n",
    "\n",
    "globals()['resumo_pj'] = resumo_pj\n",
    "globals()['df_pj_analise'] = df_pj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f9a09",
   "metadata": {},
   "source": [
    "## 12. Uso por Faixa de Dias sem Movimento\n",
    "\n",
    "Avalia ado√ß√£o de produtos, principalidade e SOW conforme faixas de inatividade (dias sem movimento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso por faixa de dias sem movimento\n",
    "print(\"üìä AN√ÅLISE POR FAIXA DE INATIVIDADE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_mov = df_base[df_base['ano_mes']==ultimo_mes].copy()\n",
    "\n",
    "# C√°lculo direto de dias_sem_mov usando 'ult_movimento' (priorit√°rio)\n",
    "if 'ult_movimento' in df_mov.columns:\n",
    "    df_mov['ult_movimento'] = pd.to_datetime(df_mov['ult_movimento'], errors='coerce')\n",
    "    hoje = pd.Timestamp.today().normalize()\n",
    "    df_mov['dias_sem_mov'] = (hoje - df_mov['ult_movimento']).dt.days\n",
    "else:\n",
    "    # Fallback legacy (mantido como conting√™ncia)\n",
    "    possiveis_col_datas = [\n",
    "        'ult_movimento_dt', 'dt_ult_movimento', 'data_ult_movimento',\n",
    "        'dt_ult_movto', 'dt_ult_mov', 'dat_ult_movimento'\n",
    "    ]\n",
    "    col_dt_encontrada = next((c for c in possiveis_col_datas if c in df_mov.columns), None)\n",
    "    if col_dt_encontrada:\n",
    "        df_mov[col_dt_encontrada] = pd.to_datetime(df_mov[col_dt_encontrada], errors='coerce')\n",
    "        hoje = pd.Timestamp.today().normalize()\n",
    "        df_mov['dias_sem_mov'] = (hoje - df_mov[col_dt_encontrada]).dt.days\n",
    "\n",
    "# Criar faixa_movimento (somente se dias_sem_mov dispon√≠vel)\n",
    "if 'dias_sem_mov' in df_mov.columns:\n",
    "    df_mov['faixa_movimento'] = pd.cut(\n",
    "        df_mov['dias_sem_mov'],\n",
    "        bins=[-1,20,45,float('inf')],\n",
    "        labels=['Menos_20_dias','20_45_dias','Mais_45_dias']\n",
    "    )\n",
    "\n",
    "faixas = ['Menos_20_dias','20_45_dias','Mais_45_dias']\n",
    "produtos_chave = ['cad_pix_ativo','transacao_app','possui_cartao_debito','possui_cartao_credito']\n",
    "\n",
    "print(\"\\nM√âTRICAS POR FAIXA:\")\n",
    "print(f\"{'Faixa':<15} {'Qtde':>8} {'Princ M√©d':>10} {'PIX%':>7} {'App%':>7} {'D√©b%':>7} {'Cr√©d%':>7}\")\n",
    "print('-'*70)\n",
    "resumo_faixas = []\n",
    "\n",
    "if 'faixa_movimento' in df_mov.columns:\n",
    "    for f in faixas:\n",
    "        subset = df_mov[df_mov['faixa_movimento'] == f]\n",
    "        if subset.empty: \n",
    "            continue\n",
    "        princ_med = subset['pontos_principalidade'].mean() if 'pontos_principalidade' in subset.columns else float('nan')\n",
    "        linha = {'faixa': f, 'qtde': len(subset), 'princ_media': princ_med}\n",
    "        valores_print = [f\"{f:<15} {len(subset):>8,} {princ_med:>10.1f}\"]\n",
    "        for prod in produtos_chave:\n",
    "            if prod in subset.columns:\n",
    "                perc = subset[prod].mean()*100\n",
    "                linha[prod+'_perc'] = perc\n",
    "                valores_print.append(f\"{perc:>6.1f}%\")\n",
    "            else:\n",
    "                valores_print.append(f\"{'-':>6}\")\n",
    "        print(' '.join(valores_print))\n",
    "        resumo_faixas.append(linha)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  N√£o foi poss√≠vel criar 'faixa_movimento' (campo 'ult_movimento' ausente ou inv√°lido).\")\n",
    "\n",
    "# Correla√ß√£o dias vs principalidade\n",
    "if {'dias_sem_mov','pontos_principalidade'} <= set(df_mov.columns):\n",
    "    corr_mov_princ = df_mov['dias_sem_mov'].corr(df_mov['pontos_principalidade'])\n",
    "    print(f\"\\nCorrela√ß√£o Dias Sem Mov. vs Principalidade: {corr_mov_princ:.3f}\")\n",
    "\n",
    "# Oportunidades: inativos + alta principalidade + sem PIX\n",
    "if {'dias_sem_mov','pontos_principalidade','cad_pix_ativo'} <= set(df_mov.columns):\n",
    "    p75 = df_mov['pontos_principalidade'].quantile(0.75)\n",
    "    oportunidades_inativos = df_mov[\n",
    "        (df_mov['dias_sem_mov']>45) &\n",
    "        (df_mov['pontos_principalidade']>=p75) &\n",
    "        (df_mov['cad_pix_ativo']==0)\n",
    "    ]\n",
    "    print(f\"Inativos com alta principalidade sem PIX: {len(oportunidades_inativos):,}\")\n",
    "\n",
    "# Resumo\n",
    "if 'faixa_movimento' in df_mov.columns:\n",
    "    resumo_movimento = {\n",
    "        'total': len(df_mov),\n",
    "        'menor_20': (df_mov['faixa_movimento']=='Menos_20_dias').sum(),\n",
    "        '20_45': (df_mov['faixa_movimento']=='20_45_dias').sum(),\n",
    "        'mais_45': (df_mov['faixa_movimento']=='Mais_45_dias').sum()\n",
    "    }\n",
    "else:\n",
    "    resumo_movimento = {'total': len(df_mov), 'menor_20': 0, '20_45': 0, 'mais_45': 0}\n",
    "\n",
    "print(\"\\nüìã RESUMO EXECUTIVO - FAIXAS INATIVIDADE\")\n",
    "print(f\"Menos 20 dias: {resumo_movimento['menor_20']:,}\")\n",
    "print(f\"20-45 dias: {resumo_movimento['20_45']:,}\")\n",
    "print(f\"Mais 45 dias: {resumo_movimento['mais_45']:,}\")\n",
    "\n",
    "globals()['resumo_movimento'] = resumo_movimento\n",
    "globals()['resumo_faixas_movimento'] = resumo_faixas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa4a03",
   "metadata": {},
   "source": [
    "## 13. Visualiza√ß√µes e Gr√°ficos Comparativos\n",
    "\n",
    "Gr√°ficos para apresenta√ß√£o: distribui√ß√µes, evolu√ß√µes, heatmaps de correla√ß√£o e penetra√ß√£o de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√µes chave para apresenta√ß√£o\n",
    "print(\"üé® GERA√á√ÉO DE GR√ÅFICOS\")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "figs_apresentacao = {}\n",
    "\n",
    "# 1. Distribui√ß√£o Principalidade (√∫ltimo m√™s)\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_ult = df_base[df_base['ano_mes']==ultimo_mes]\n",
    "if 'pontos_principalidade' in df_ult.columns:\n",
    "    fig = px.histogram(df_ult, x='pontos_principalidade', nbins=40, title='Distribui√ß√£o de Principalidade')\n",
    "    figs_apresentacao['dist_principalidade'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 2. Evolu√ß√£o m√©dia principalidade 3 √∫ltimos meses\n",
    "if 'pontos_principalidade' in df_base.columns:\n",
    "    evol_princ = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].groupby('ano_mes')['pontos_principalidade'].mean().reset_index()\n",
    "    fig = px.line(evol_princ, x='ano_mes', y='pontos_principalidade', markers=True, title='Evolu√ß√£o M√©dia Principalidade (3M)')\n",
    "    figs_apresentacao['evol_principalidade'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 3. Penetra√ß√£o produtos (bar)\n",
    "if 'df_penetracao_produtos' in globals():\n",
    "    fig = px.bar(df_penetracao_produtos.sort_values('perc'), x='perc', y='produto', orientation='h', title='Penetra√ß√£o de Produtos (%)')\n",
    "    figs_apresentacao['penet_produtos'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 4. Heatmap correla√ß√£o (subset num√©rico)\n",
    "num_cols = []\n",
    "for c in ['pontos_principalidade','isa_analise','cad_pix_ativo','possui_cartao_credito','cash_in','cash_out']:\n",
    "    if c in df_ult.columns:\n",
    "        num_cols.append(c)\n",
    "if len(num_cols) >= 3:\n",
    "    corr = df_ult[num_cols].corr()\n",
    "    fig = px.imshow(corr, text_auto='.2f', title='Heatmap Correla√ß√£o Indicadores')\n",
    "    figs_apresentacao['heatmap_corr'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 5. SOW Faixa x Principalidade m√©dia\n",
    "if 'df_sow' in globals() and 'faixa_sow' in df_sow.columns:\n",
    "    base_sow = df_sow.groupby('faixa_sow')['pontos_principalidade'].mean().reset_index()\n",
    "    fig = px.bar(base_sow, x='faixa_sow', y='pontos_principalidade', title='Principalidade M√©dia por Faixa SOW')\n",
    "    figs_apresentacao['princ_por_sow'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 6. Inatividade x Ado√ß√£o PIX\n",
    "if 'faixa_movimento' in df_ult.columns and 'cad_pix_ativo' in df_ult.columns:\n",
    "    inat_pix = df_ult.groupby('faixa_movimento')['cad_pix_ativo'].mean().reset_index()\n",
    "    fig = px.bar(inat_pix, x='faixa_movimento', y='cad_pix_ativo', title='Penetra√ß√£o PIX por Faixa de Inatividade', labels={'cad_pix_ativo':'PIX %'})\n",
    "    figs_apresentacao['pix_inatividade'] = fig\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\nTotal de gr√°ficos gerados:\", len(figs_apresentacao))\n",
    "globals()['figs_apresentacao'] = figs_apresentacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Modelo de Pr√©-Churn (Machine Learning)\n",
    "\n",
    "Cria√ß√£o de features, defini√ß√£o de vari√°vel alvo (proxy churn = queda principalidade + inatividade + aus√™ncia de PIX), treino de modelo e identifica√ß√£o de associados com maior propens√£o a churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de Pr√©-Churn\n",
    "print(\"ü§ñ MODELO PR√â-CHURN - EXPERIMENTO INICIAL\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Preparar base hist√≥rica m√≠nima (√∫ltimos 4 registros por associado j√° carregados na query inicial)\n",
    "base_ml = df_base.copy()\n",
    "\n",
    "# Criar alvo churn_proxy: queda recente + inatividade + sem PIX\n",
    "# Condi√ß√µes: (queda_flag==1 no √∫ltimo m√™s) OU (soma_quedas>=2) E (dias_sem_mov>45) E (cad_pix_ativo==0)\n",
    "ultimo_mes = base_ml['ano_mes'].max()\n",
    "ult = base_ml[base_ml['ano_mes']==ultimo_mes].copy()\n",
    "\n",
    "# Garantir colunas\n",
    "for c in ['queda_flag','soma_quedas','dias_sem_mov','cad_pix_ativo']:\n",
    "    if c not in ult.columns:\n",
    "        ult[c] = 0\n",
    "\n",
    "ult['churn_proxy'] = np.where(\n",
    "    ( (ult['queda_flag']==1) | (ult['soma_quedas']>=2) ) &\n",
    "    (ult['dias_sem_mov']>45) &\n",
    "    (ult['cad_pix_ativo']==0),\n",
    "    1,0\n",
    ")\n",
    "\n",
    "print(f\"Taxa alvo churn proxy: {ult['churn_proxy'].mean()*100:.2f}%\")\n",
    "\n",
    "# Features selecionadas\n",
    "features = [c for c in [\n",
    "    'pontos_principalidade','cad_pix_ativo','transacao_app','possui_cartao_debito',\n",
    "    'possui_cartao_credito','possui_adquirencia','possui_cobranca','possui_domicilio',\n",
    "    'possui_folha_pagamento','ativou_open_finance','dias_sem_mov','cash_in','cash_out'\n",
    "] if c in ult.columns]\n",
    "\n",
    "df_model = ult[['cpf_cnpj','churn_proxy'] + features].dropna().copy()\n",
    "\n",
    "if df_model['churn_proxy'].nunique() >= 2 and len(df_model) > 100:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score, classification_report\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    X = df_model[features]\n",
    "    y = df_model['churn_proxy']\n",
    "\n",
    "    # Balance simples se a taxa for muito baixa\n",
    "    if y.mean() < 0.05:\n",
    "        # Oversampling simples (duplicar positivos)\n",
    "        pos = df_model[df_model['churn_proxy']==1]\n",
    "        mult = int((len(df_model)-len(pos))/len(pos)*0.5)+1 if len(pos)>0 else 1\n",
    "        df_model_bal = pd.concat([df_model, pd.concat([pos]*mult)])\n",
    "        X = df_model_bal[features]\n",
    "        y = df_model_bal['churn_proxy']\n",
    "        print(f\"Aplicado oversampling simples. Nova taxa: {y.mean()*100:.2f}%\")\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced_subsample')\n",
    "    rf.fit(X_train,y_train)\n",
    "    probas = rf.predict_proba(X_test)[:,1]\n",
    "    preds = (probas>=0.5).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(y_test, probas)\n",
    "    print(f\"ROC AUC: {auc:.3f}\")\n",
    "    print(\"\\nRelat√≥rio classifica√ß√£o:\")\n",
    "    print(classification_report(y_test,preds, digits=3))\n",
    "\n",
    "    # Import√¢ncia features\n",
    "    imp = pd.DataFrame({'feature':features,'importance':rf.feature_importances_}).sort_values('importance', ascending=False)\n",
    "    print(\"\\nIMPORT√ÇNCIA DAS FEATURES:\")\n",
    "    for r in imp.itertuples():\n",
    "        print(f\"  {r.feature:<25} {r.importance:6.3f}\")\n",
    "\n",
    "    # Top risco (aplicar em todo dataset atual)\n",
    "    full_probas = rf.predict_proba(ult[features])[:,1]\n",
    "    ult['prob_churn'] = full_probas\n",
    "    top_risco = ult.nlargest(20,'prob_churn')[['cpf_cnpj','prob_churn','pontos_principalidade','dias_sem_mov']]\n",
    "    print(\"\\nTOP 20 ASSOCIADOS MAIOR RISCO (proxy):\")\n",
    "    for i,row in enumerate(top_risco.itertuples(),1):\n",
    "        print(f\"  {i:2d}. ...{str(row.cpf_cnpj)[-4:]} | Prob: {row.prob_churn:.2f} | Princ: {row.pontos_principalidade:.1f} | Dias SEM: {row.dias_sem_mov}\")\n",
    "\n",
    "    globals()['modelo_churn'] = rf\n",
    "    globals()['importancia_churn'] = imp\n",
    "    globals()['scoring_churn'] = ult[['cpf_cnpj','prob_churn','churn_proxy']]\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Base insuficiente ou alvo sem variabilidade para modelagem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42a254",
   "metadata": {},
   "source": [
    "## 15. Gera√ß√£o de Relat√≥rio Final\n",
    "\n",
    "Convers√£o dos principais resumos, tabelas e m√©tricas em um relat√≥rio execut√°vel (HTML/PDF) para apresenta√ß√£o. Inclui ap√™ndice t√©cnico e recomenda√ß√µes estrat√©gicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera√ß√£o de Relat√≥rio Final (HTML simplificado)\n",
    "print(\"üìù GERA√á√ÉO DE RELAT√ìRIO FINAL\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "resumos = {}\n",
    "for nome_var in [\n",
    "    'resumo_contas','resumo_bbm','resumo_evolucao','resumo_produtos','resumo_isa','resumo_pix',\n",
    "    'resumo_cash','resumo_prod_faltantes','resumo_sow','resumo_pj','resumo_movimento'\n",
    "]:\n",
    "    if nome_var in globals():\n",
    "        resumos[nome_var] = globals()[nome_var]\n",
    "\n",
    "html_parts = [\"<html><head><meta charset='utf-8'><title>Relat√≥rio Principalidade</title>\" \\\n",
    "              \"<style>body{font-family:Arial; margin:30px;} h2{border-bottom:1px solid #ccc;} table{border-collapse:collapse;} td,th{border:1px solid #ddd;padding:4px 8px;} .kpi{display:inline-block;margin:10px 20px;}</style></head><body>\"]\n",
    "html_parts.append(\"<h1>Relat√≥rio Executivo - Principalidade</h1>\")\n",
    "html_parts.append(f\"<p>Data gera√ß√£o: {datetime.now().strftime('%d/%m/%Y %H:%M')}</p>\")\n",
    "\n",
    "# KPIs principais (se existirem)\n",
    "if 'resumo_evolucao' in resumos and 'resumo_pix' in resumos:\n",
    "    kpis = [\n",
    "        (\"Associados Analisados\", resumos['resumo_evolucao'].get('total_analisados','-')),\n",
    "        (\"% Subiram\", f\"{resumos['resumo_evolucao'].get('subiu',0)/resumos['resumo_evolucao'].get('total_analisados',1)*100:.1f}%\"),\n",
    "        (\"PIX Penetra√ß√£o\", f\"{resumos['resumo_pix'].get('perc_penetracao',0):.1f}%\"),\n",
    "        (\"ISA M√©dio\", f\"{resumos.get('resumo_isa',{}).get('isa_medio',0):.2f}\"),\n",
    "        (\"SOW M√©dio\", f\"{resumos.get('resumo_sow',{}).get('sow_medio',0):.2f}\"),\n",
    "    ]\n",
    "    html_parts.append(\"<div>\")\n",
    "    for k,v in kpis:\n",
    "        html_parts.append(f\"<div class='kpi'><strong>{k}</strong><br>{v}</div>\")\n",
    "    html_parts.append(\"</div>\")\n",
    "\n",
    "# Tabelas de resumos\n",
    "for nome, dados in resumos.items():\n",
    "    html_parts.append(f\"<h2>{nome.replace('_',' ').title()}</h2>\")\n",
    "    html_parts.append(\"<table><tr><th>M√©trica</th><th>Valor</th></tr>\")\n",
    "    for k,v in dados.items():\n",
    "        html_parts.append(f\"<tr><td>{k}</td><td>{v}</td></tr>\")\n",
    "    html_parts.append(\"</table>\")\n",
    "\n",
    "html_parts.append(\"<h2>Recomenda√ß√µes Estrat√©gicas</h2>\")\n",
    "html_parts.append(\"<ul>\")\n",
    "html_parts.append(\"<li>Acelerar ativa√ß√£o PIX em faixas de alta principalidade e inatividade prolongada.</li>\")\n",
    "html_parts.append(\"<li>Priorizar cross-sell em contas com SOW baixo e principalidade elevada.</li>\")\n",
    "html_parts.append(\"<li>Atacar gaps de produtos transacionais b√°sicos para elevar reten√ß√£o.</li>\")\n",
    "html_parts.append(\"<li>Focar PJ sem domic√≠lio/cobran√ßa mas com relacionamento forte.</li>\")\n",
    "html_parts.append(\"<li>Acompanhar cohort de queda sucessiva (3 quedas) para preven√ß√£o churn.</li>\")\n",
    "html_parts.append(\"</ul>\")\n",
    "\n",
    "html_parts.append(\"<p><em>Relat√≥rio gerado automaticamente. Ajustes visuais podem ser aplicados em ferramenta de apresenta√ß√£o.</em></p>\")\n",
    "html_parts.append(\"</body></html>\")\n",
    "\n",
    "html_final = ''.join(html_parts)\n",
    "saida = Path('relatorio_principalidade.html')\n",
    "saida.write_text(html_final, encoding='utf-8')\n",
    "print(f\"Relat√≥rio salvo em: {saida.resolve()}\")\n",
    "\n",
    "globals()['relatorio_html'] = html_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida√ß√µes finais e export opcional\n",
    "from IPython.display import display\n",
    "\n",
    "print('=== Valida√ß√µes Finais ===')\n",
    "if 'df_base' not in globals():\n",
    "    raise RuntimeError('df_base inexistente.')\n",
    "\n",
    "print(f\"Registros df_base: {len(df_base):,}\")\n",
    "print(f\"Colunas ({len(df_base.columns)}): {list(df_base.columns)[:25]} ...\")\n",
    "\n",
    "# 1. Checar duplicidade da chave principal (cpf_cnpj + ano_mes se existir)\n",
    "if 'cpf_cnpj' in df_base.columns and 'ano_mes' in df_base.columns:\n",
    "    dup = df_base.duplicated(subset=['cpf_cnpj','ano_mes']).sum()\n",
    "    print(f\"Duplicados cpf_cnpj+ano_mes: {dup}\")\n",
    "else:\n",
    "    print('Aviso: N√£o foi poss√≠vel checar duplicidade (faltam colunas).')\n",
    "\n",
    "# 2. Percentual de tipo_pessoa preenchido\n",
    "if 'tipo_pessoa' in df_base.columns:\n",
    "    perc = df_base['tipo_pessoa'].notna().mean()*100\n",
    "    print(f\"tipo_pessoa preenchido: {perc:0.2f}%\")\n",
    "\n",
    "# 3. Amostra final\n",
    "print('\\nAmostra final:')\n",
    "cols_show = [c for c in ['cpf_cnpj','ano_mes','pontos_principalidade','tipo_pessoa','segmento'] if c in df_base.columns]\n",
    "print(df_base[cols_show].head())\n",
    "\n",
    "# 4. Export opcional\n",
    "EXPORTAR = False  # coloque True se quiser exportar\n",
    "CAMINHO_EXPORT = os.path.join(RUTAS['saida_dir'], 'principalidade_merge_bruto.parquet')\n",
    "if EXPORTAR:\n",
    "    df_base.to_parquet(CAMINHO_EXPORT, index=False)\n",
    "    print(f\"‚úì Exportado para {CAMINHO_EXPORT}\")\n",
    "else:\n",
    "    print('Exporta√ß√£o desativada (EXPORTAR=False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d0d95",
   "metadata": {},
   "source": [
    "# Conclusi√≥n y Validaciones Finales\n",
    "\n",
    "Se realizaron las cargas brutas, merges controlados y an√°lisis inicial sin ninguna normalizaci√≥n autom√°tica de identificadores ni inferencia de tipo de persona.\n",
    "\n",
    "Pr√≥ximos passos (opcionales):\n",
    "- Ajustar manualmente `cpf_cnpj` activando la celda opcional (solo si lo necesitas).\n",
    "- Agregar an√°lisis adicionales en nuevas celdas sin modificar `df_base` original.\n",
    "- Exportar subconjuntos espec√≠ficos si deseas trabajar en otro entorno.\n",
    "\n",
    "La siguiente celda ejecuta validaciones finales e opcionalmente guarda un parquet limpio. Ajusta rutas seg√∫n necesidad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
