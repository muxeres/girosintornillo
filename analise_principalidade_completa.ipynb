{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be16e0d2",
   "metadata": {},
   "source": [
    "# Análise Completa de Principalidade - Dashboard BI0730\n",
    "\n",
    "Este notebook contém uma análise completa dos dados de principalidade, incluindo:\n",
    "\n",
    "- Análise de quantidade de contas (PF/PJ) e agências\n",
    "- Análise BBM vs não-BBM\n",
    "- Evolução da principalidade nos últimos 3 meses\n",
    "- Uso de produtos e diferenças de ISA\n",
    "- Chave PIX e chave forte\n",
    "- Diferença entre cash-in e cash-out\n",
    "- Produtos faltantes e análise de categorias\n",
    "- SOW Sicredi\n",
    "- Análise temporal de movimentação\n",
    "- Pré-churn com Machine Learning\n",
    "- Gráficos e visualizações para apresentação\n",
    "\n",
    "**Data de Execução**: Agosto 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de bibliotecas necessárias\n",
    "import os\n",
    "import sys\n",
    "import duckdb\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from datetime import datetime, date, timedelta\n",
    "import json\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.stats as stats\n",
    "import pyarrow.parquet as pq  # <- necessário para read_schema\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "# Configurações do sistema\n",
    "sys.path.append(rf\"C:\\Git\\BI0730\")\n",
    "from libs.geral.utils import * \n",
    "\n",
    "USER = os.getlogin()\n",
    "ANO = 2025\n",
    "\n",
    "# Função utilitária simples de merge + coalesce (SEM tratamentos extras)\n",
    "def merge_and_coalesce(df_left, df_right, on: str, cols_to_merge, how='left'):\n",
    "    \"\"\"Merge básico e para cada coluna em cols_to_merge (se existir no right) preenche nulos do left.\n",
    "    Não cria colunas novas além do necessário e não aplica nenhuma inferência.\n",
    "    \"\"\"\n",
    "    if df_right is None or isinstance(df_right, pd.DataFrame) and df_right.empty:\n",
    "        print('merge_and_coalesce: df_right vazio -> retorno df_left')\n",
    "        return df_left\n",
    "    if not isinstance(cols_to_merge, (list, tuple)):\n",
    "        cols_to_merge = []\n",
    "    disponiveis = [c for c in cols_to_merge if c in df_right.columns and c != on]\n",
    "    if not disponiveis:\n",
    "        print('merge_and_coalesce: nenhuma coluna disponível para mesclar')\n",
    "        return df_left\n",
    "    tmp = df_right[[on] + disponiveis].copy()\n",
    "    out = df_left.merge(tmp, on=on, how=how, suffixes=('', '_y'))\n",
    "    for col in disponiveis:\n",
    "        cy = f'{col}_y'\n",
    "        if cy in out.columns:\n",
    "            if col in out.columns:\n",
    "                out[col] = out[col].where(out[col].notna(), out[cy])\n",
    "            else:\n",
    "                out[col] = out[cy]\n",
    "            out.drop(columns=[cy], inplace=True, errors='ignore')\n",
    "    return out\n",
    "\n",
    "# Rutas e configurações baseadas no notebook original\n",
    "PATH_BASES_PARQUET = rf\"C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\"\n",
    "\n",
    "RUTAS = {\n",
    "    'inadimplentes': os.path.join(PATH_BASES_PARQUET, 'base_inadimplentes.parquet'),\n",
    "    'associados_dir': os.path.join(PATH_BASES_PARQUET, 'associados_total_diario'),\n",
    "    'dashboard': rf\"C:\\Users\\carola_luco\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\\associados_totais_tratados_dashboard.parquet\",\n",
    "    'saida_dir': rf'C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira',\n",
    "    'saida_dir_giro': rf'C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\giro_de_carteira',\n",
    "    'principalidade': rf\"C:\\Users\\{USER}\\Sicredi\\TimeBI_0730 - Documentos\\_BASES\\arquivos_parquet\\cia_pcp_indicador_principalidade_historico\\2025*.parquet\"\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'ano': ANO,\n",
    "    'pasta_isa_historico': rf\"C:\\Users\\carola_luco\\Sicredi\\TimeBI_0730 - Documentos\\01_Rotineiros\\33_GiroCarteira\\isa_historicos_extracao\",\n",
    "    'filtros_risco_bbm': [\"BAIXÍSSIMO\", \"BAIXO 1\", \"BAIXO 2\", \"MÉDIO 1\", \"MÉDIO 2\"],\n",
    "    'dias_sem_movimentacao': (20, 45)\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    'filtros_risco_bbm': [\"BAIXÍSSIMO\", \"BAIXO 1\", \"BAIXO 2\", \"MÉDIO 1\", \"MÉDIO 2\"],\n",
    "    'dias_sem_movimentacao': (20, 45),\n",
    "    'principalidade': ['sow']\n",
    "}\n",
    "\n",
    "print(\"✓ Configuração inicial concluída\")\n",
    "print(f\"Usuário: {USER}\")\n",
    "print(f\"Ano de análise: {ANO}\")\n",
    "print(f\"Data de execução: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7a35f2",
   "metadata": {},
   "source": [
    "## 1. Carga de Dados Base e Merges\n",
    "\n",
    "Carregamento dos parquets principais: principalidade histórico, dashboard e ISA histórico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar conexão DuckDB e carregar dados de principalidade (SEM QUALQUER NORMALIZAÇÃO / PADRONIZAÇÃO)\n",
    "con = duckdb.connect()\n",
    "\n",
    "# ATENÇÃO: Conforme solicitação do usuário TODA normalização automática (zeros à esquerda, renomeios, inferências)\n",
    "# FOI REMOVIDA. A base é carregada exatamente como armazenada no parquet. Qualquer tratamento de cpf/cnpj\n",
    "# deverá ser feito manualmente fora deste notebook ou em células específicas que o usuário controlar.\n",
    "\n",
    "path_principalidade = RUTAS['principalidade']\n",
    "print(f\"Carregando dados de principalidade (bruto): {path_principalidade}\")\n",
    "\n",
    "query_principalidade = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM read_parquet('{path_principalidade}')\n",
    "\"\"\"\n",
    "\n",
    "df_principalidade = con.sql(query_principalidade).fetchdf()\n",
    "\n",
    "print(f\"✓ Dados de principalidade carregados (sem transformação): {len(df_principalidade):,} registros\")\n",
    "print(f\"  Colunas disponíveis: {len(df_principalidade.columns)}\")\n",
    "if 'ano_mes' in df_principalidade.columns:\n",
    "    print(f\"  Período: {df_principalidade['ano_mes'].min()} até {df_principalidade['ano_mes'].max()}\")\n",
    "\n",
    "print(\"\\nEstrutura (dtypes amostra):\")\n",
    "print(df_principalidade.dtypes.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados do dashboard (SEM normalização / sem zfill)\n",
    "path_dashboard = RUTAS.get('dashboard')\n",
    "\n",
    "print(f\"Carregando dashboard bruto: {path_dashboard}\")\n",
    "\n",
    "# Ler esquema para validar colunas disponíveis\n",
    "schema_cols = list(pq.read_schema(path_dashboard).names)\n",
    "\n",
    "# Colunas explicitamente permitidas (adicionados: pix_trans_30d, ult_movimento)\n",
    "base_cols = [\n",
    "    'cpf_cnpj','nome_agencia','mc_total_1','gestor','cod_agencia','num_conta',\n",
    "    'cod_carteira','tipo_pessoa','segmento','segmento_cliente',\n",
    "    'pix_trans_30d','ult_movimento'  # <- NOVO\n",
    "]\n",
    "cols_to_read = [c for c in base_cols if c in schema_cols]\n",
    "\n",
    "assoc_dash = pd.read_parquet(path_dashboard, columns=cols_to_read)\n",
    "\n",
    "# ÚNICO rename permitido: segmento_cliente -> segmento (se segmento não existir)\n",
    "if 'segmento_cliente' in assoc_dash.columns and 'segmento' not in assoc_dash.columns:\n",
    "    assoc_dash = assoc_dash.rename(columns={'segmento_cliente':'segmento'})\n",
    "\n",
    "print(f\"✓ Dashboard carregado: {len(assoc_dash):,} registros\")\n",
    "print(\"Colunas:\", list(assoc_dash.columns))\n",
    "# Verificação rápida dos novos campos\n",
    "for novo_col in ['pix_trans_30d','ult_movimento']:\n",
    "    if novo_col not in assoc_dash.columns:\n",
    "        print(f\"⚠️  Campo {novo_col} não disponível no dashboard.\")\n",
    "assoc_dash.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar ISA histórico (SEM normalização automática)\n",
    "isa_dir = config['pasta_isa_historico']\n",
    "isa_files = sorted(glob.glob(os.path.join(isa_dir, 'isa_historico_analise_*.parquet')))\n",
    "\n",
    "if isa_files:\n",
    "    ultimo_isa = isa_files[-1]\n",
    "    print(f\"Carregando ISA histórico (bruto): {ultimo_isa}\")\n",
    "    isa_historico = pd.read_parquet(ultimo_isa)\n",
    "\n",
    "    col_isa_mes = [c for c in isa_historico.columns if re.match(r'isa_\\d{6}$', c)]\n",
    "    col_isa_mes_sorted = sorted(col_isa_mes)\n",
    "    ultimas4 = col_isa_mes_sorted[-4:]\n",
    "\n",
    "    print(f\"✓ ISA histórico carregado: {len(isa_historico):,} registros\")\n",
    "    print(f\"Colunas: {list(isa_historico.columns)[:20]} ... total={len(isa_historico.columns)}\")\n",
    "    print(f\"  Últimas colunas ISA detectadas: {ultimas4}\")\n",
    "\n",
    "    if len(ultimas4) >= 4:\n",
    "        isa_atual_col = ultimas4[-1]\n",
    "        tres_anteriores = ultimas4[-4:-1]\n",
    "    else:\n",
    "        isa_atual_col = ultimas4[-1] if ultimas4 else None\n",
    "        tres_anteriores = ultimas4[:-1] if len(ultimas4) > 1 else []\n",
    "\n",
    "    if tres_anteriores:\n",
    "        cols_exist = [c for c in tres_anteriores if c in isa_historico.columns]\n",
    "        isa_historico['isa_media_3m_calc'] = isa_historico[cols_exist].mean(axis=1)\n",
    "    else:\n",
    "        isa_historico['isa_media_3m_calc'] = np.nan\n",
    "else:\n",
    "    print(\"⚠️  ISA histórico não encontrado\")\n",
    "    isa_historico = pd.DataFrame()\n",
    "\n",
    "print(\"\\n✓ Carga bruta concluída (nenhuma padronização aplicada).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento mínimo solicitado (APENAS rename num_cpf_cnpj -> cpf_cnpj nas bases carregadas)\n",
    "# Não aplica zfill, strip, nem qualquer outra normalização.\n",
    "\n",
    "def _rename_num_cpf(df, nome):\n",
    "    if isinstance(df, pd.DataFrame) and not df.empty and 'num_cpf_cnpj' in df.columns and 'cpf_cnpj' not in df.columns:\n",
    "        df.rename(columns={'num_cpf_cnpj':'cpf_cnpj'}, inplace=True)\n",
    "        print(f\"✓ {nome}: renomeado 'num_cpf_cnpj' -> 'cpf_cnpj'\")\n",
    "    else:\n",
    "        print(f\"{nome}: nenhum rename necessário\")\n",
    "    return df\n",
    "\n",
    "bases_tratamento = [\n",
    "    ('df_principalidade', 'df_principalidade'),\n",
    "    ('assoc_dash', 'assoc_dash'),\n",
    "    ('isa_historico', 'isa_historico')\n",
    "]\n",
    "\n",
    "for var_name, label in bases_tratamento:\n",
    "    if var_name in globals():\n",
    "        globals()[var_name] = _rename_num_cpf(globals()[var_name], label)\n",
    "    else:\n",
    "        print(f\"{label}: variável inexistente\")\n",
    "\n",
    "print(\"Tratamento mínimo concluído. Prosseguir com verificação e merges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificação de 'tipo_pessoa' nas bases antes do merge (sem qualquer manipulação)\n",
    "print(\"--- Verificação de 'tipo_pessoa' (valores brutos) ---\")\n",
    "\n",
    "bases_info = [\n",
    "    ('df_principalidade', df_principalidade),\n",
    "    ('assoc_dash', assoc_dash),\n",
    "    ('isa_historico', isa_historico if 'isa_historico' in globals() else pd.DataFrame())\n",
    "]\n",
    "\n",
    "snapshot_tipo = {}\n",
    "for nome, df in bases_info:\n",
    "    if df is not None and not df.empty and 'tipo_pessoa' in df.columns:\n",
    "        vals = df['tipo_pessoa'].dropna().astype(str).unique()[:10]\n",
    "        print(f\"\\n{nome}: valores únicos de tipo_pessoa (primeiros 10):\")\n",
    "        print(vals)\n",
    "        print(f\"Nulos: {df['tipo_pessoa'].isna().sum():,}\")\n",
    "        snapshot_tipo[nome] = df[['cpf_cnpj','tipo_pessoa']].copy() if 'cpf_cnpj' in df.columns else df[['tipo_pessoa']].copy()\n",
    "    else:\n",
    "        print(f\"\\n{nome}: coluna 'tipo_pessoa' ausente ou dataframe vazio (ok se origem não fornece).\")\n",
    "        snapshot_tipo[nome] = pd.DataFrame()\n",
    "\n",
    "snapshot_assoc_dash = snapshot_tipo.get('assoc_dash')\n",
    "snapshot_princ = snapshot_tipo.get('df_principalidade')\n",
    "snapshot_isa = snapshot_tipo.get('isa_historico')\n",
    "print(\"\\nSnapshot armazenado: snapshot_assoc_dash, snapshot_princ, snapshot_isa.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padroniza_identificadores(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normaliza e padroniza as colunas de um DataFrame, com foco em chaves de junção.\n",
    "    Regras (EXATAMENTE como solicitado):\n",
    "      - Renomeia 'num_cpf_cnpj' para 'cpf_cnpj' se existir.\n",
    "      - Padroniza 'cpf_cnpj' com zeros à esquerda: se len > 11 trata como CNPJ (14), caso contrário CPF (11).\n",
    "      - Padroniza 'cod_agencia', 'num_conta', 'cod_carteira' apenas com zfill (sem heurística extra).\n",
    "    Nenhum merge, nenhuma inferência adicional. Apenas retorna o DF tratado.\n",
    "    \"\"\"\n",
    "    if df is None or not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Renomear coluna principal\n",
    "    if 'num_cpf_cnpj' in df.columns and 'cpf_cnpj' not in df.columns:\n",
    "        df = df.rename(columns={'num_cpf_cnpj': 'cpf_cnpj'})\n",
    "\n",
    "    if 'cpf_cnpj' not in df.columns:\n",
    "        print(\"⚠️  Aviso: Coluna 'cpf_cnpj' não encontrada para padronização.\")\n",
    "        return df\n",
    "\n",
    "    # Padronização cpf_cnpj (sem remover caracteres, somente strip e zfill conforme regra original)\n",
    "    df['cpf_cnpj'] = (df['cpf_cnpj'].astype(str).str.strip()\n",
    "                      .apply(lambda x: x.zfill(14) if len(x) > 11 else x.zfill(11)))\n",
    "\n",
    "    # Demais colunas (apenas se existirem)\n",
    "    if 'cod_agencia' in df.columns:\n",
    "        df['cod_agencia'] = df['cod_agencia'].astype(str).str.zfill(2)\n",
    "    if 'num_conta' in df.columns:\n",
    "        df['num_conta'] = df['num_conta'].astype(str).str.zfill(6)\n",
    "    if 'cod_carteira' in df.columns:\n",
    "        df['cod_carteira'] = np.where(\n",
    "            df['cod_carteira'].astype(str).str.len() > 3,\n",
    "            df['cod_carteira'].astype(str).str.zfill(6),\n",
    "            df['cod_carteira'].astype(str).str.zfill(3)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "print('\\n=== Padronizando identificadores das três bases (sem merges) ===')\n",
    "if 'df_principalidade' in globals():\n",
    "    df_principalidade = padroniza_identificadores(df_principalidade)\n",
    "if 'assoc_dash' in globals():\n",
    "    assoc_dash = padroniza_identificadores(assoc_dash)\n",
    "if 'isa_historico' in globals():\n",
    "    isa_historico = padroniza_identificadores(isa_historico)\n",
    "print('Concluído. Continue com os merges nas células seguintes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_dash = padroniza_identificadores(assoc_dash)\n",
    "df_principalidade = padroniza_identificadores(df_principalidade)\n",
    "isa_historico = padroniza_identificadores(isa_historico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_principalidade['porte_padrao'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOVO MERGE CONTROLADO (usa apenas dados brutos carregados acima)\n",
    "print('--- Iniciando novo processo de merge controlado ---')\n",
    "\n",
    "df_base = df_principalidade.copy()\n",
    "\n",
    "# Merge dashboard\n",
    "if 'assoc_dash' in globals() and isinstance(assoc_dash, pd.DataFrame) and not assoc_dash.empty:\n",
    "    if 'cpf_cnpj' not in assoc_dash.columns:\n",
    "        raise KeyError(\"Dashboard sem coluna 'cpf_cnpj'.\")\n",
    "    cols_dash_to_merge = [c for c in assoc_dash.columns if c != 'cpf_cnpj']\n",
    "    df_base = df_base.merge(assoc_dash[['cpf_cnpj'] + cols_dash_to_merge], on='cpf_cnpj', how='left', suffixes=('', '_dash'))\n",
    "    # Coalesce sem sobrescrever valores já existentes em df_base (df_base é cópia bruta, então só pega do dash se nulo)\n",
    "    for col in cols_dash_to_merge:\n",
    "        col_y = f'{col}_dash'\n",
    "        if col_y in df_base.columns:\n",
    "            if col not in df_base.columns:\n",
    "                df_base[col] = df_base[col_y]\n",
    "            else:\n",
    "                df_base[col] = df_base[col].where(df_base[col].notna(), df_base[col_y])\n",
    "            df_base.drop(columns=[col_y], inplace=True)\n",
    "    print(f\"✓ Merge dashboard concluído. Shape: {df_base.shape}\")\n",
    "else:\n",
    "    print('⚠️ Dashboard não carregado ou vazio')\n",
    "\n",
    "# Merge ISA (não deve alterar tipo_pessoa)\n",
    "if 'isa_historico' in globals() and isinstance(isa_historico, pd.DataFrame) and not isa_historico.empty:\n",
    "    if 'cpf_cnpj' in isa_historico.columns:\n",
    "        cols_isa_to_merge = [c for c in isa_historico.columns if c != 'cpf_cnpj' and c != 'tipo_pessoa']\n",
    "        df_base = df_base.merge(isa_historico[['cpf_cnpj'] + cols_isa_to_merge], on='cpf_cnpj', how='left', suffixes=('', '_isa'))\n",
    "        for col in cols_isa_to_merge:\n",
    "            col_y = f'{col}_isa'\n",
    "            if col_y in df_base.columns:\n",
    "                if col not in df_base.columns:\n",
    "                    df_base[col] = df_base[col_y]\n",
    "                else:\n",
    "                    df_base[col] = df_base[col].where(df_base[col].notna(), df_base[col_y])\n",
    "                df_base.drop(columns=[col_y], inplace=True)\n",
    "        print(f\"✓ Merge ISA concluído. Shape: {df_base.shape}\")\n",
    "    else:\n",
    "        print(\"⚠️ ISA histórico sem coluna 'cpf_cnpj' - merge ignorado.\")\n",
    "else:\n",
    "    print('⚠️ ISA não carregado ou vazio')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Distribuição final tipo_pessoa em df_base:')\n",
    "if 'tipo_pessoa' in df_base.columns:\n",
    "    print(df_base['tipo_pessoa'].value_counts(dropna=False))\n",
    "else:\n",
    "    print('Coluna tipo_pessoa ausente.')\n",
    "\n",
    "print('Colunas finais após merges (primeiras 40):')\n",
    "print(list(df_base.columns)[:40])\n",
    "print('Total colunas:', len(df_base.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.query(\"tipo_pessoa.isna()\", engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnóstico: onde 'PJ' se perdeu?\n",
    "# Motivo provável: merge LEFT partindo de df_principalidade elimina cpfs que só existem no dashboard (muitos PJ).\n",
    "# Esta célula identifica:\n",
    "# 1. Quantos PJ existem no dashboard bruto.\n",
    "# 2. Quantos desses PJ aparecem em df_base após o merge.\n",
    "# 3. Quantos PJ foram perdidos por ausência na principalidade.\n",
    "# 4. Amostras de cpfs perdidos.\n",
    "# 5. (Opcional) Reconstrução de df_base usando união de todas as chaves para preservar PJ.\n",
    "\n",
    "\n",
    "\n",
    "if 'assoc_dash' in globals() and 'tipo_pessoa' in assoc_dash.columns:\n",
    "    pj_dash_total = (assoc_dash['tipo_pessoa'] == 'PJ').sum()\n",
    "    pf_dash_total = (assoc_dash['tipo_pessoa'] == 'PF').sum()\n",
    "    print(f\"Dashboard bruto -> PJ: {pj_dash_total:,} | PF: {pf_dash_total:,}\")\n",
    "else:\n",
    "    print('Dashboard não disponível para diagnóstico.')\n",
    "\n",
    "if 'snapshot_assoc_dash' in globals() and snapshot_assoc_dash is not None and not snapshot_assoc_dash.empty:\n",
    "    pj_snapshot = (snapshot_assoc_dash['tipo_pessoa'] == 'PJ').sum()\n",
    "    print(f\"Snapshot assoc_dash -> PJ: {pj_snapshot:,}\")\n",
    "\n",
    "if 'df_base' in globals() and 'tipo_pessoa' in df_base.columns:\n",
    "    pj_df_base = (df_base['tipo_pessoa'] == 'PJ').sum()\n",
    "    pf_df_base = (df_base['tipo_pessoa'] == 'PF').sum()\n",
    "    na_df_base = df_base['tipo_pessoa'].isna().sum()\n",
    "    print(f\"Após merges -> PJ: {pj_df_base:,} | PF: {pf_df_base:,} | NA: {na_df_base:,}\")\n",
    "\n",
    "    # CPFs PJ no dashboard\n",
    "    pj_cpfs_dash = set(assoc_dash.loc[assoc_dash['tipo_pessoa']=='PJ','cpf_cnpj']) if 'assoc_dash' in globals() else set()\n",
    "    # CPFs presentes em df_base\n",
    "    base_cpfs = set(df_base['cpf_cnpj'])\n",
    "    # PJ preservados\n",
    "    pj_preservados = pj_cpfs_dash & base_cpfs\n",
    "    # PJ perdidos\n",
    "    pj_perdidos = pj_cpfs_dash - base_cpfs\n",
    "    print(f\"PJ preservados (existem em principalidade): {len(pj_preservados):,}\")\n",
    "    print(f\"PJ perdidos (não existem em principalidade, por isso não aparecem no LEFT merge): {len(pj_perdidos):,}\")\n",
    "    if pj_perdidos:\n",
    "        exemplo_perdidos = list(pj_perdidos)[:10]\n",
    "        print('Exemplos de PJ perdidos:', exemplo_perdidos)\n",
    "else:\n",
    "    print('df_base ou tipo_pessoa indisponível para diagnóstico.')\n",
    "\n",
    "# Opcional: reconstruir df_base com união de chaves para preservar PJ\n",
    "RECONSTRUIR_UNIAO = False  # coloque True se quiser gerar uma versão alternativa preservando todos os CPFs\n",
    "if RECONSTRUIR_UNIAO:\n",
    "    print('\\nReconstruindo df_base_all_keys com união de chaves...')\n",
    "    chaves = set()\n",
    "    if 'df_principalidade' in globals():\n",
    "        chaves |= set(df_principalidade.get('cpf_cnpj', []))\n",
    "    if 'assoc_dash' in globals():\n",
    "        chaves |= set(assoc_dash.get('cpf_cnpj', []))\n",
    "    if 'isa_historico' in globals():\n",
    "        chaves |= set(isa_historico.get('cpf_cnpj', []))\n",
    "    df_base_all_keys = pd.DataFrame({'cpf_cnpj': list(chaves)})\n",
    "    # merge incremental\n",
    "    if 'df_principalidade' in globals():\n",
    "        df_base_all_keys = df_base_all_keys.merge(df_principalidade, on='cpf_cnpj', how='left')\n",
    "    if 'assoc_dash' in globals():\n",
    "        cols_dash_tmp = [c for c in assoc_dash.columns if c != 'cpf_cnpj']\n",
    "        df_base_all_keys = df_base_all_keys.merge(assoc_dash[['cpf_cnpj'] + cols_dash_tmp], on='cpf_cnpj', how='left', suffixes=('', '_dash2'))\n",
    "        for c in cols_dash_tmp:\n",
    "            cy = f'{c}_dash2'\n",
    "            if cy in df_base_all_keys.columns:\n",
    "                df_base_all_keys[c] = df_base_all_keys[c].where(df_base_all_keys[c].notna(), df_base_all_keys[cy])\n",
    "                df_base_all_keys.drop(columns=[cy], inplace=True)\n",
    "    if 'isa_historico' in globals():\n",
    "        cols_isa_tmp = [c for c in isa_historico.columns if c != 'cpf_cnpj' and c != 'tipo_pessoa']\n",
    "        df_base_all_keys = df_base_all_keys.merge(isa_historico[['cpf_cnpj'] + cols_isa_tmp], on='cpf_cnpj', how='left', suffixes=('', '_isa2'))\n",
    "        for c in cols_isa_tmp:\n",
    "            cy = f'{c}_isa2'\n",
    "            if cy in df_base_all_keys.columns:\n",
    "                df_base_all_keys[c] = df_base_all_keys[c].where(df_base_all_keys[c].notna(), df_base_all_keys[cy])\n",
    "                df_base_all_keys.drop(columns=[cy], inplace=True)\n",
    "    # Reforçar tipo_pessoa com dashboard novamente\n",
    "    if 'assoc_dash' in globals() and 'tipo_pessoa' in assoc_dash.columns:\n",
    "        tipos_prioritarios2 = (assoc_dash[['cpf_cnpj','tipo_pessoa']]\n",
    "                               .dropna()\n",
    "                               .groupby('cpf_cnpj', as_index=False)\n",
    "                               .agg(tipo_pessoa=lambda s: 'PJ' if 'PJ' in set(s) else ('PF' if 'PF' in set(s) else s.iloc[0])))\n",
    "        df_base_all_keys.drop(columns=[c for c in ['tipo_pessoa'] if c in df_base_all_keys.columns], inplace=True)\n",
    "        df_base_all_keys = df_base_all_keys.merge(tipos_prioritarios2, on='cpf_cnpj', how='left')\n",
    "    print('Distribuição tipo_pessoa (df_base_all_keys):')\n",
    "    print(df_base_all_keys['tipo_pessoa'].value_counts(dropna=False))\n",
    "    print('Amostra df_base_all_keys:')\n",
    "    display(df_base_all_keys.head())\n",
    "    print('Use df_base_all_keys se quiser incluir PJ que não aparecem na principalidade.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base['tipo_pessoa'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e030082",
   "metadata": {},
   "source": [
    "## 2. Análise de Quantidade de Contas (PF/PJ) e Agências\n",
    "\n",
    "Análise da distribuição de contas por tipo de pessoa e agências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a variação de pontos e identifica quedas\n",
    "df_base = df_base.sort_values(by=['cpf_cnpj', 'ano_mes'])\n",
    "df_base['var_pontos'] = df_base.groupby('cpf_cnpj')['pontos_principalidade'].diff().fillna(0)\n",
    "df_base['queda_flag'] = (df_base['var_pontos'] < 0).astype(int)\n",
    "df_base['soma_quedas'] = df_base.groupby('cpf_cnpj')['var_pontos'].transform(lambda x: x[x < 0].sum())\n",
    "\n",
    "df_base[['cpf_cnpj', 'ano_mes', 'pontos_principalidade', 'var_pontos', 'queda_flag', 'soma_quedas']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0162e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de quantidade de contas por tipo de pessoa e agência\n",
    "print(\"📊 ANÁLISE DE QUANTIDADE DE CONTAS POR TIPO DE PESSOA E AGÊNCIA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if 'df_base' not in globals():\n",
    "    raise RuntimeError('df_base inexistente: execute as células de carga e merge primeiro.')\n",
    "\n",
    "# Dados mais recentes (requer ano_mes)\n",
    "if 'ano_mes' not in df_base.columns:\n",
    "    raise ValueError(\"Coluna 'ano_mes' não encontrada em df_base. Verifique a base de principalidade.\")\n",
    "\n",
    "df_atual = df_base[df_base['ano_mes'] == df_base['ano_mes'].max()].copy()\n",
    "\n",
    "required_cols = ['tipo_pessoa', 'nome_agencia', 'cod_agencia', 'cpf_cnpj']\n",
    "faltantes = [c for c in required_cols if c not in df_atual.columns]\n",
    "if faltantes:\n",
    "    raise ValueError(f\"Colunas necessárias ausentes em df_atual: {faltantes}\")\n",
    "\n",
    "# Agrupar por tipo de pessoa e agência\n",
    "analise_agencia = (df_atual\n",
    "                   .groupby(['tipo_pessoa', 'nome_agencia', 'cod_agencia'])\n",
    "                   .agg(\n",
    "                       associados_unicos=('cpf_cnpj', 'nunique'),\n",
    "                       registros_totais=('cpf_cnpj', 'count')\n",
    "                   )\n",
    "                   .sort_values(by=['tipo_pessoa', 'associados_unicos'], ascending=[True, False])\n",
    "                   .reset_index())\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Análise de Associados e Registros por Tipo de Pessoa e Agência:\")\n",
    "from IPython.display import display\n",
    "display(analise_agencia)\n",
    "\n",
    "# Resumo geral por tipo de pessoa\n",
    "resumo_tipo_pessoa = analise_agencia.groupby('tipo_pessoa').agg(\n",
    "    total_associados=('associados_unicos', 'sum'),\n",
    "    total_registros=('registros_totais', 'sum'),\n",
    "    numero_de_agencias=('nome_agencia', 'count')\n",
    ").sort_values(by='total_associados', ascending=False)\n",
    "\n",
    "print(\"\\nResumo Consolidado por Tipo de Pessoa:\")\n",
    "display(resumo_tipo_pessoa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de quantidade de contas por tipo de pessoa\n",
    "print(\"📊 ANÁLISE DE QUANTIDADE DE CONTAS E AGÊNCIAS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dados mais recentes\n",
    "df_atual = df_base[df_base['ano_mes'] == df_base['ano_mes'].max()].copy()\n",
    "\n",
    "# Total de contas únicas\n",
    "total_contas = df_atual['cpf_cnpj'].nunique()\n",
    "print(f\"Total de Contas Únicas: {total_contas:,}\")\n",
    "\n",
    "# Distribuição por tipo de pessoa\n",
    "dist_pessoa = df_atual['tipo_pessoa'].value_counts()\n",
    "print(f\"\\nDistribuição por Tipo de Pessoa:\")\n",
    "for tipo, qtd in dist_pessoa.items():\n",
    "    perc = (qtd / total_contas) * 100\n",
    "    print(f\"  {tipo}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Total de agências\n",
    "total_agencias = df_atual['cod_agencia'].nunique()\n",
    "print(f\"\\nTotal de Agências: {total_agencias:,}\")\n",
    "\n",
    "# Top 10 agências com mais contas\n",
    "top_agencias = df_atual['cod_agencia'].value_counts().head(10)\n",
    "print(f\"\\nTop 10 Agências (por número de contas):\")\n",
    "for i, (agencia, qtd) in enumerate(top_agencias.items(), 1):\n",
    "    perc = (qtd / total_contas) * 100\n",
    "    print(f\"  {i:2d}. Agência {agencia}: {qtd:,} contas ({perc:.1f}%)\")\n",
    "\n",
    "# Distribuição PF/PJ por agência (top 5)\n",
    "print(f\"\\nDistribuição PF/PJ nas Top 5 Agências:\")\n",
    "top5_agencias = top_agencias.head(5).index\n",
    "\n",
    "for agencia in top5_agencias:\n",
    "    df_agencia = df_atual[df_atual['cod_agencia'] == agencia]\n",
    "    dist_agencia = df_agencia['tipo_pessoa'].value_counts()\n",
    "    total_agencia = len(df_agencia)\n",
    "    \n",
    "    pf_count = dist_agencia.get('PF', 0)\n",
    "    pj_count = dist_agencia.get('PJ', 0)\n",
    "    \n",
    "    print(f\"  Agência {agencia}: PF={pf_count} ({pf_count/total_agencia*100:.1f}%) | PJ={pj_count} ({pj_count/total_agencia*100:.1f}%)\")\n",
    "\n",
    "# Resumo consolidado\n",
    "resumo_contas = {\n",
    "    'total_contas': total_contas,\n",
    "    'total_pf': dist_pessoa.get('PF', 0),\n",
    "    'total_pj': dist_pessoa.get('PJ', 0),\n",
    "    'total_agencias': total_agencias,\n",
    "    'perc_pf': (dist_pessoa.get('PF', 0) / total_contas) * 100,\n",
    "    'perc_pj': (dist_pessoa.get('PJ', 0) / total_contas) * 100\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 RESUMO EXECUTIVO - CONTAS E AGÊNCIAS\")\n",
    "print(f\"Total: {resumo_contas['total_contas']:,} contas em {resumo_contas['total_agencias']:,} agências\")\n",
    "print(f\"PF: {resumo_contas['total_pf']:,} ({resumo_contas['perc_pf']:.1f}%)\")\n",
    "print(f\"PJ: {resumo_contas['total_pj']:,} ({resumo_contas['perc_pj']:.1f}%)\")\n",
    "\n",
    "# Salvar resumo para uso posterior\n",
    "globals()['resumo_contas'] = resumo_contas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33863f04",
   "metadata": {},
   "source": [
    "## 3. Análise BBM e Fora de Filtro\n",
    "\n",
    "Análise da distribuição dos associados dentro e fora dos filtros de risco BBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise BBM vs Fora de Filtro\n",
    "print(\"📊 ANÁLISE BBM E FORA DE FILTRO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classificar por BBM\n",
    "riscos_bbm = PARAMS['filtros_risco_bbm']\n",
    "print(f\"Filtros BBM considerados: {riscos_bbm}\")\n",
    "\n",
    "# Normalizar nível de risco\n",
    "df_atual['nivel_risco_norm'] = df_atual['nivel_risco'].astype(str).str.upper().str.strip()\n",
    "\n",
    "# Classificar BBM vs Não-BBM\n",
    "df_atual['classificacao_bbm'] = np.where(\n",
    "    df_atual['nivel_risco_norm'].isin([r.upper() for r in riscos_bbm]),\n",
    "    'BBM',\n",
    "    'Fora_BBM'\n",
    ")\n",
    "\n",
    "# Contagem por classificação BBM\n",
    "dist_bbm = df_atual['classificacao_bbm'].value_counts()\n",
    "total_analisados = len(df_atual)\n",
    "\n",
    "print(f\"\\nDistribuição por Classificação BBM:\")\n",
    "for classif, qtd in dist_bbm.items():\n",
    "    perc = (qtd / total_analisados) * 100\n",
    "    print(f\"  {classif}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Análise detalhada por nível de risco\n",
    "print(f\"\\nDetalhamento por Nível de Risco:\")\n",
    "dist_risco = df_atual['nivel_risco_norm'].value_counts()\n",
    "for risco, qtd in dist_risco.head(10).items():\n",
    "    perc = (qtd / total_analisados) * 100\n",
    "    status = \"✓ BBM\" if risco in [r.upper() for r in riscos_bbm] else \"✗ Fora BBM\"\n",
    "    print(f\"  {risco}: {qtd:,} ({perc:.1f}%) - {status}\")\n",
    "\n",
    "# Cruzamento BBM x Tipo de Pessoa\n",
    "print(f\"\\nCruzamento BBM x Tipo de Pessoa:\")\n",
    "crosstab_bbm_pessoa = pd.crosstab(df_atual['classificacao_bbm'], df_atual['tipo_pessoa'], margins=True)\n",
    "print(crosstab_bbm_pessoa)\n",
    "\n",
    "# Percentuais por tipo de pessoa\n",
    "print(f\"\\nPercentuais BBM por Tipo de Pessoa:\")\n",
    "for pessoa in ['PF', 'PJ']:\n",
    "    df_pessoa = df_atual[df_atual['tipo_pessoa'] == pessoa]\n",
    "    if not df_pessoa.empty:\n",
    "        bbm_count = (df_pessoa['classificacao_bbm'] == 'BBM').sum()\n",
    "        total_pessoa = len(df_pessoa)\n",
    "        perc_bbm = (bbm_count / total_pessoa) * 100\n",
    "        print(f\"  {pessoa}: {bbm_count:,}/{total_pessoa:,} são BBM ({perc_bbm:.1f}%)\")\n",
    "\n",
    "# Análise por faixa de principalidade\n",
    "if 'faixa_categoria' in df_atual.columns:\n",
    "    print(f\"\\nBBM por Faixa de Principalidade:\")\n",
    "    crosstab_bbm_faixa = pd.crosstab(df_atual['faixa_categoria'], df_atual['classificacao_bbm'], margins=True)\n",
    "    print(crosstab_bbm_faixa)\n",
    "\n",
    "# Resumo executivo BBM\n",
    "bbm_count = dist_bbm.get('BBM', 0)\n",
    "fora_bbm_count = dist_bbm.get('Fora_BBM', 0)\n",
    "\n",
    "resumo_bbm = {\n",
    "    'total_analisados': total_analisados,\n",
    "    'bbm_count': bbm_count,\n",
    "    'fora_bbm_count': fora_bbm_count,\n",
    "    'perc_bbm': (bbm_count / total_analisados) * 100,\n",
    "    'perc_fora_bbm': (fora_bbm_count / total_analisados) * 100\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 RESUMO EXECUTIVO - BBM\")\n",
    "print(f\"Dentro do filtro BBM: {resumo_bbm['bbm_count']:,} ({resumo_bbm['perc_bbm']:.1f}%)\")\n",
    "print(f\"Fora do filtro BBM: {resumo_bbm['fora_bbm_count']:,} ({resumo_bbm['perc_fora_bbm']:.1f}%)\")\n",
    "\n",
    "globals()['resumo_bbm'] = resumo_bbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23724f53",
   "metadata": {},
   "source": [
    "## 4. Análise de Principalidade: Subida e Bajada Últimos 3 Meses\n",
    "\n",
    "Análise da evolução da principalidade: contas que subiram e desceram nos últimos 3 meses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de Evolução da Principalidade\n",
    "print(\"📊 ANÁLISE DE EVOLUÇÃO DA PRINCIPALIDADE - ÚLTIMOS 3 MESES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Pegar últimos 3 meses disponíveis\n",
    "meses_disponiveis = sorted(df_base['ano_mes'].unique())\n",
    "ultimos_3_meses = meses_disponiveis[-3:] if len(meses_disponiveis) >= 3 else meses_disponiveis\n",
    "\n",
    "print(f\"Períodos analisados: {ultimos_3_meses}\")\n",
    "\n",
    "# Filtrar dados dos últimos 3 meses\n",
    "df_3meses = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].copy()\n",
    "\n",
    "# Análise por associado nos últimos 3 meses\n",
    "evolucao_princ = df_3meses.groupby('cpf_cnpj').agg({\n",
    "    'pontos_principalidade': ['first', 'last', 'min', 'max', 'std'],\n",
    "    'var_pontos': 'sum',\n",
    "    'queda_flag': 'sum',\n",
    "    'soma_quedas': 'max',\n",
    "    'tipo_pessoa': 'first',\n",
    "    'faixa_categoria': 'last'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "evolucao_princ.columns = ['_'.join(col).strip() for col in evolucao_princ.columns]\n",
    "evolucao_princ = evolucao_princ.reset_index()\n",
    "\n",
    "# Calcular variação total\n",
    "evolucao_princ['var_total'] = evolucao_princ['pontos_principalidade_last'] - evolucao_princ['pontos_principalidade_first']\n",
    "\n",
    "# Classificar evolução\n",
    "def classificar_evolucao(row):\n",
    "    if row['var_total'] > 0:\n",
    "        return 'SUBIU'\n",
    "    elif row['var_total'] < 0:\n",
    "        return 'DESCEU'\n",
    "    else:\n",
    "        return 'ESTÁVEL'\n",
    "\n",
    "evolucao_princ['classificacao_evolucao'] = evolucao_princ.apply(classificar_evolucao, axis=1)\n",
    "\n",
    "# Análise geral de evolução\n",
    "print(f\"\\nRESULTADOS GERAIS:\")\n",
    "dist_evolucao = evolucao_princ['classificacao_evolucao'].value_counts()\n",
    "total_associados = len(evolucao_princ)\n",
    "\n",
    "for classif, qtd in dist_evolucao.items():\n",
    "    perc = (qtd / total_associados) * 100\n",
    "    print(f\"  {classif}: {qtd:,} associados ({perc:.1f}%)\")\n",
    "\n",
    "# Estatísticas de variação\n",
    "print(f\"\\nESTATÍSTICAS DE VARIAÇÃO DOS PONTOS:\")\n",
    "print(f\"  Variação média: {evolucao_princ['var_total'].mean():.2f} pontos\")\n",
    "print(f\"  Variação mediana: {evolucao_princ['var_total'].median():.2f} pontos\")\n",
    "print(f\"  Maior subida: {evolucao_princ['var_total'].max():.2f} pontos\")\n",
    "print(f\"  Maior queda: {evolucao_princ['var_total'].min():.2f} pontos\")\n",
    "print(f\"  Desvio padrão: {evolucao_princ['var_total'].std():.2f} pontos\")\n",
    "\n",
    "# Evolução por tipo de pessoa\n",
    "print(f\"\\nEVOLUÇÃO POR TIPO DE PESSOA:\")\n",
    "for pessoa in ['PF', 'PJ']:\n",
    "    df_pessoa = evolucao_princ[evolucao_princ['tipo_pessoa_first'] == pessoa]\n",
    "    if not df_pessoa.empty:\n",
    "        dist_pessoa = df_pessoa['classificacao_evolucao'].value_counts()\n",
    "        total_pessoa = len(df_pessoa)\n",
    "        \n",
    "        print(f\"\\n  {pessoa} (Total: {total_pessoa:,}):\")\n",
    "        for classif, qtd in dist_pessoa.items():\n",
    "            perc = (qtd / total_pessoa) * 100\n",
    "            print(f\"    {classif}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Top 10 maiores subidas\n",
    "print(f\"\\nTOP 10 MAIORES SUBIDAS:\")\n",
    "top_subidas = evolucao_princ.nlargest(10, 'var_total')\n",
    "for i, row in enumerate(top_subidas.itertuples(), 1):\n",
    "    print(f\"  {i:2d}. CPF: {row.cpf_cnpj[-4:]}... | Subida: +{row.var_total:.1f} pontos | {row.tipo_pessoa_first}\")\n",
    "\n",
    "# Top 10 maiores quedas  \n",
    "print(f\"\\nTOP 10 MAIORES QUEDAS:\")\n",
    "top_quedas = evolucao_princ.nsmallest(10, 'var_total')\n",
    "for i, row in enumerate(top_quedas.itertuples(), 1):\n",
    "    print(f\"  {i:2d}. CPF: {row.cpf_cnpj[-4:]}... | Queda: {row.var_total:.1f} pontos | {row.tipo_pessoa_first}\")\n",
    "\n",
    "# Associados com 3 quedas consecutivas (conforme query original)\n",
    "associados_3_quedas = evolucao_princ[evolucao_princ['soma_quedas_max'] >= 3]\n",
    "print(f\"\\nASSOCIADOS COM 3 QUEDAS CONSECUTIVAS: {len(associados_3_quedas):,}\")\n",
    "\n",
    "if len(associados_3_quedas) > 0:\n",
    "    print(f\"  PF: {(associados_3_quedas['tipo_pessoa_first'] == 'PF').sum():,}\")\n",
    "    print(f\"  PJ: {(associados_3_quedas['tipo_pessoa_first'] == 'PJ').sum():,}\")\n",
    "    \n",
    "    # Exemplos de associados com 3 quedas\n",
    "    print(f\"\\n  Exemplos (5 primeiros):\")\n",
    "    for i, row in enumerate(associados_3_quedas.head().itertuples(), 1):\n",
    "        print(f\"    {i}. CPF: {row.cpf_cnpj[-4:]}... | Var Total: {row.var_total:.1f} | Quedas: {row.soma_quedas_max}\")\n",
    "\n",
    "# Resumo executivo\n",
    "resumo_evolucao = {\n",
    "    'total_analisados': total_associados,\n",
    "    'subiu': dist_evolucao.get('SUBIU', 0),\n",
    "    'desceu': dist_evolucao.get('DESCEU', 0), \n",
    "    'estavel': dist_evolucao.get('ESTÁVEL', 0),\n",
    "    'tres_quedas': len(associados_3_quedas),\n",
    "    'var_media': evolucao_princ['var_total'].mean()\n",
    "}\n",
    "\n",
    "print(f\"\\n📋 RESUMO EXECUTIVO - EVOLUÇÃO PRINCIPALIDADE\")\n",
    "print(f\"Analisados: {resumo_evolucao['total_analisados']:,} associados\")\n",
    "print(f\"Subiram: {resumo_evolucao['subiu']:,} ({resumo_evolucao['subiu']/total_associados*100:.1f}%)\")\n",
    "print(f\"Desceram: {resumo_evolucao['desceu']:,} ({resumo_evolucao['desceu']/total_associados*100:.1f}%)\")\n",
    "print(f\"3 quedas consecutivas: {resumo_evolucao['tres_quedas']:,}\")\n",
    "\n",
    "globals()['resumo_evolucao'] = resumo_evolucao\n",
    "globals()['evolucao_princ'] = evolucao_princ\n",
    "\n",
    "# Define as colunas específicas do 'assoc_dash' que queremos mesclar.\n",
    "# Adicionados: 'pix_trans_30d', 'ult_movimento'\n",
    "cols_dash_to_merge = [\n",
    "    'tipo_pessoa', 'segmento_cliente', 'perfil_investidor',\n",
    "    'data_abertura', 'data_encerramento', 'status_conta', 'nome_agencia',\n",
    "    'pix_trans_30d', 'ult_movimento'  # <- NOVOS CAMPOS\n",
    "]\n",
    "\n",
    "# Realiza o merge usando a função atualizada\n",
    "df_base = merge_and_coalesce(\n",
    "    df_left=df_principalidade,\n",
    "    df_right=assoc_dash,\n",
    "    on='cpf_cnpj',\n",
    "    cols_to_merge=cols_dash_to_merge,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verificação de valores nulos em 'tipo_pessoa' após o merge\n",
    "if 'tipo_pessoa' in df_base.columns and df_base['tipo_pessoa'].isnull().any():\n",
    "    print(\"\\nAtenção: Existem registros com 'tipo_pessoa' nulo após o merge com assoc_dash.\")\n",
    "    print(\"Isso indica que alguns CPFs/CNPJs de 'df_principalidade' não foram encontrados em 'assoc_dash'.\")\n",
    "\n",
    "# Garantir conversão de ult_movimento para data (para uso posterior)\n",
    "if 'ult_movimento' in df_base.columns:\n",
    "    df_base['ult_movimento'] = pd.to_datetime(df_base['ult_movimento'], errors='coerce')\n",
    "\n",
    "df_base.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ddf09",
   "metadata": {},
   "source": [
    "## 6. Diferença de ISA Últimos 3 Meses\n",
    "\n",
    "Análise das variações do ISA nos últimos 3 meses e insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de Diferença de ISA Últimos 3 Meses\n",
    "print(\"📊 ANÁLISE DE DIFERENÇA DE ISA - ÚLTIMOS 3 MESES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar disponibilidade de dados ISA\n",
    "if 'isa_media' in df_base.columns or 'isa_media_3m_calc' in df_base.columns:\n",
    "    \n",
    "    # Usar dados ISA disponíveis\n",
    "    df_isa = df_base.copy()\n",
    "    \n",
    "    # Preparar análise ISA\n",
    "    if 'isa_media_3m_calc' in df_isa.columns:\n",
    "        print(\"Usando ISA média 3M calculada\")\n",
    "        df_isa['isa_analise'] = df_isa['isa_media_3m_calc']\n",
    "    elif 'isa_media' in df_isa.columns:\n",
    "        print(\"Usando ISA média disponível\")\n",
    "        df_isa['isa_analise'] = df_isa['isa_media']\n",
    "    else:\n",
    "        print(\"⚠️  Dados de ISA não disponíveis para análise completa\")\n",
    "        df_isa['isa_analise'] = np.nan\n",
    "    \n",
    "    # Filtrar dados com ISA válido\n",
    "    df_isa_valido = df_isa[df_isa['isa_analise'].notna()].copy()\n",
    "    \n",
    "    if len(df_isa_valido) > 0:\n",
    "        print(f\"Registros com ISA válido: {len(df_isa_valido):,}\")\n",
    "        \n",
    "        # Estatísticas gerais de ISA\n",
    "        print(f\"\\nESTATÍSTICAS GERAIS DE ISA:\")\n",
    "        isa_stats = df_isa_valido['isa_analise'].describe()\n",
    "        print(f\"  Média: {isa_stats['mean']:.2f}\")\n",
    "        print(f\"  Mediana: {isa_stats['50%']:.2f}\")\n",
    "        print(f\"  Mínimo: {isa_stats['min']:.2f}\")\n",
    "        print(f\"  Máximo: {isa_stats['max']:.2f}\")\n",
    "        print(f\"  Desvio Padrão: {isa_stats['std']:.2f}\")\n",
    "        \n",
    "        # Distribuição por faixas de ISA\n",
    "        df_isa_valido['faixa_isa'] = pd.cut(\n",
    "            df_isa_valido['isa_analise'], \n",
    "            bins=[0, 2, 4, 6, 8, float('inf')], \n",
    "            labels=['0-2', '2-4', '4-6', '6-8', '8+']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDISTRIBUIÇÃO POR FAIXAS DE ISA:\")\n",
    "        dist_isa = df_isa_valido['faixa_isa'].value_counts().sort_index()\n",
    "        total_isa = len(df_isa_valido)\n",
    "        \n",
    "        for faixa, qtd in dist_isa.items():\n",
    "            perc = (qtd / total_isa) * 100\n",
    "            print(f\"  {faixa}: {qtd:,} ({perc:.1f}%)\")\n",
    "        \n",
    "        # ISA por tipo de pessoa\n",
    "        print(f\"\\nISA MÉDIO POR TIPO DE PESSOA:\")\n",
    "        isa_por_pessoa = df_isa_valido.groupby('tipo_pessoa')['isa_analise'].agg(['mean', 'count', 'std']).round(2)\n",
    "        \n",
    "        for pessoa in isa_por_pessoa.index:\n",
    "            media = isa_por_pessoa.loc[pessoa, 'mean']\n",
    "            count = isa_por_pessoa.loc[pessoa, 'count']\n",
    "            std = isa_por_pessoa.loc[pessoa, 'std']\n",
    "            print(f\"  {pessoa}: {media:.2f} (n={count:,}, std={std:.2f})\")\n",
    "        \n",
    "        # ISA vs Principalidade\n",
    "        if 'pontos_principalidade' in df_isa_valido.columns:\n",
    "            print(f\"\\nCORRELAÇÃO ISA vs PRINCIPALIDADE:\")\n",
    "            correlacao = df_isa_valido['isa_analise'].corr(df_isa_valido['pontos_principalidade'])\n",
    "            print(f\"  Correlação: {correlacao:.3f}\")\n",
    "            \n",
    "            # Análise por quartis de ISA\n",
    "            df_isa_valido['quartil_isa'] = pd.qcut(\n",
    "                df_isa_valido['isa_analise'], \n",
    "                q=4, \n",
    "                labels=['Q1_Baixo', 'Q2_MedioBaixo', 'Q3_MedioAlto', 'Q4_Alto']\n",
    "            )\n",
    "            \n",
    "            princ_por_quartil = df_isa_valido.groupby('quartil_isa')['pontos_principalidade'].agg(['mean', 'count']).round(1)\n",
    "            \n",
    "            print(f\"\\n  PRINCIPALIDADE MÉDIA POR QUARTIL DE ISA:\")\n",
    "            for quartil in princ_por_quartil.index:\n",
    "                media_princ = princ_por_quartil.loc[quartil, 'mean']\n",
    "                count = princ_por_quartil.loc[quartil, 'count']\n",
    "                print(f\"    {quartil}: {media_princ:.1f} pontos (n={count:,})\")\n",
    "        \n",
    "        # Top/Bottom ISA\n",
    "        print(f\"\\nTOP 10 MAIORES ISA:\")\n",
    "        top_isa = df_isa_valido.nlargest(10, 'isa_analise')[['cpf_cnpj', 'isa_analise', 'tipo_pessoa', 'pontos_principalidade']]\n",
    "        for i, row in enumerate(top_isa.itertuples(), 1):\n",
    "            cpf_mask = row.cpf_cnpj[-4:] if len(str(row.cpf_cnpj)) >= 4 else str(row.cpf_cnpj)\n",
    "            print(f\"  {i:2d}. CPF: ...{cpf_mask} | ISA: {row.isa_analise:.2f} | {row.tipo_pessoa} | Princ: {row.pontos_principalidade:.1f}\")\n",
    "        \n",
    "        print(f\"\\nTOP 10 MENORES ISA (acima de 0):\")\n",
    "        bottom_isa = df_isa_valido[df_isa_valido['isa_analise'] > 0].nsmallest(10, 'isa_analise')[['cpf_cnpj', 'isa_analise', 'tipo_pessoa', 'pontos_principalidade']]\n",
    "        for i, row in enumerate(bottom_isa.itertuples(), 1):\n",
    "            cpf_mask = row.cpf_cnpj[-4:] if len(str(row.cpf_cnpj)) >= 4 else str(row.cpf_cnpj)\n",
    "            print(f\"  {i:2d}. CPF: ...{cpf_mask} | ISA: {row.isa_analise:.2f} | {row.tipo_pessoa} | Princ: {row.pontos_principalidade:.1f}\")\n",
    "        \n",
    "        # Insights ISA\n",
    "        print(f\"\\n💡 INSIGHTS ISA:\")\n",
    "        \n",
    "        # ISA alto com baixa principalidade (oportunidades)\n",
    "        isa_alto_princ_baixo = df_isa_valido[\n",
    "            (df_isa_valido['isa_analise'] >= 6) & \n",
    "            (df_isa_valido['pontos_principalidade'] < df_isa_valido['pontos_principalidade'].median())\n",
    "        ]\n",
    "        \n",
    "        if len(isa_alto_princ_baixo) > 0:\n",
    "            print(f\"  • {len(isa_alto_princ_alto):,} associados com ISA alto (≥6) mas principalidade baixa\")\n",
    "            print(f\"    Oportunidade de cross-selling!\")\n",
    "        \n",
    "        # ISA baixo com alta principalidade (risco)\n",
    "        isa_baixo_princ_alto = df_isa_valido[\n",
    "            (df_isa_valido['isa_analise'] <= 2) & \n",
    "            (df_isa_valido['pontos_principalidade'] > df_isa_valido['pontos_principalidade'].quantile(0.75))\n",
    "        ]\n",
    "        \n",
    "        if len(isa_baixo_princ_alto) > 0:\n",
    "            print(f\"  • {len(isa_baixo_princ_alto):,} associados com ISA baixo (≤2) mas principalidade alta\")\n",
    "            print(f\"    Possível risco de churn!\")\n",
    "        \n",
    "        # Concentração ISA por faixa\n",
    "        faixa_2_4 = (dist_isa.get('2-4', 0) / total_isa) * 100\n",
    "        if faixa_2_4 > 30:\n",
    "            print(f\"  • {faixa_2_4:.1f}% dos associados na faixa ISA 2-4 (faixa crítica)\")\n",
    "        \n",
    "        # Resumo ISA\n",
    "        resumo_isa = {\n",
    "            'total_com_isa': len(df_isa_valido),\n",
    "            'isa_medio': df_isa_valido['isa_analise'].mean(),\n",
    "            'isa_mediano': df_isa_valido['isa_analise'].median(),\n",
    "            'acima_6': (df_isa_valido['isa_analise'] >= 6).sum(),\n",
    "            'abaixo_2': (df_isa_valido['isa_analise'] <= 2).sum(),\n",
    "            'correlacao_princ': correlacao if 'correlacao' in locals() else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n📋 RESUMO EXECUTIVO - ISA\")\n",
    "        print(f\"ISA médio: {resumo_isa['isa_medio']:.2f}\")\n",
    "        print(f\"Acima de 6: {resumo_isa['acima_6']:,} ({resumo_isa['acima_6']/total_isa*100:.1f}%)\")\n",
    "        print(f\"Abaixo de 2: {resumo_isa['abaixo_2']:,} ({resumo_isa['abaixo_2']/total_isa*100:.1f}%)\")\n",
    "        \n",
    "        globals()['resumo_isa'] = resumo_isa\n",
    "        globals()['df_isa_analise'] = df_isa_valido\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  Nenhum registro com ISA válido encontrado\")\n",
    "        resumo_isa = {'total_com_isa': 0, 'isa_medio': 0}\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  Colunas de ISA não encontradas nos dados\")\n",
    "    resumo_isa = {'total_com_isa': 0, 'isa_medio': 0}\n",
    "\n",
    "# Colunas do 'isa_historico' para fazer o merge\n",
    "cols_isa_to_merge = ['tipo_pessoa', 'segmento']\n",
    "\n",
    "# Realiza o merge usando a função atualizada\n",
    "df_base = merge_and_coalesce(\n",
    "    df_left=df_base,\n",
    "    df_right=isa_historico,\n",
    "    on='cpf_cnpj',\n",
    "    cols_to_merge=cols_isa_to_merge,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verificação de valores nulos em 'tipo_pessoa' após o merge final\n",
    "if df_base['tipo_pessoa'].isnull().any():\n",
    "    print(\"\\nAtenção: Existem registros com 'tipo_pessoa' nulo após o merge final.\")\n",
    "    print(\"Esses registros não foram encontrados nem em 'assoc_dash' nem em 'isa_historico'.\")\n",
    "\n",
    "df_base.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025af51",
   "metadata": {},
   "source": [
    "## 7. Chave PIX e Chave Forte Últimos 3 Períodos\n",
    "\n",
    "Análise do uso de chave PIX e chave forte nos últimos 3 períodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de Chave PIX e Chave Forte Últimos 3 Períodos\n",
    "print(\"📊 ANÁLISE CHAVE PIX E CHAVE FORTE - ÚLTIMOS 3 PERÍODOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Campos PIX disponíveis\n",
    "campos_pix = {\n",
    "    'PIX Cadastrado': 'cad_pix',\n",
    "    'PIX Ativo': 'cad_pix_ativo',\n",
    "    'Transações PIX 30d': 'pix_trans_30d'\n",
    "}\n",
    "\n",
    "# Dados dos últimos 3 períodos\n",
    "df_pix_3m = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].copy()\n",
    "\n",
    "print(f\"Períodos analisados: {ultimos_3_meses}\")\n",
    "print(f\"Total de registros: {len(df_pix_3m):,}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Normalização de campos PIX para evitar erro:\n",
    "# Algumas bases usam 'S'/'N', 'SIM'/'NAO' ou strings numéricas.\n",
    "# Convertemos para numérico (0/1) ou 0 quando não reconhecido.\n",
    "# ------------------------------------------------------------------\n",
    "mapeamentos_binarios = {\n",
    "    'S': 1, 'N': 0,\n",
    "    'SIM': 1, 'NAO': 0, 'NÃO': 0,\n",
    "    'Y': 1, 'NOK': 0, 'OK': 1,\n",
    "    True: 1, False: 0\n",
    "}\n",
    "\n",
    "for col in ['cad_pix_ativo', 'pix_trans_30d']:\n",
    "    if col in df_pix_3m.columns:\n",
    "        # Substitui valores mapeáveis\n",
    "        df_pix_3m[col] = df_pix_3m[col].replace(mapeamentos_binarios)\n",
    "        # Converte strings numéricas; valores não convertíveis viram NaN e depois 0\n",
    "        df_pix_3m[col] = pd.to_numeric(df_pix_3m[col], errors='coerce').fillna(0)\n",
    "        # Para coluna binária, força inteiro 0/1\n",
    "        if col == 'cad_pix_ativo':\n",
    "            df_pix_3m[col] = (df_pix_3m[col] > 0).astype(int)\n",
    "\n",
    "# Garantir mesma limpeza no último período (usado mais abaixo)\n",
    "df_ultimo_periodo = df_pix_3m[df_pix_3m['ano_mes'] == ultimos_3_meses[-1]].copy()\n",
    "if 'cad_pix_ativo' in df_ultimo_periodo.columns:\n",
    "    df_ultimo_periodo['cad_pix_ativo'] = df_ultimo_periodo['cad_pix_ativo'].replace(mapeamentos_binarios)\n",
    "    df_ultimo_periodo['cad_pix_ativo'] = pd.to_numeric(df_ultimo_periodo['cad_pix_ativo'], errors='coerce').fillna(0)\n",
    "    df_ultimo_periodo['cad_pix_ativo'] = (df_ultimo_periodo['cad_pix_ativo'] > 0).astype(int)\n",
    "if 'pix_trans_30d' in df_ultimo_periodo.columns:\n",
    "    df_ultimo_periodo['pix_trans_30d'] = df_ultimo_periodo['pix_trans_30d'].replace(mapeamentos_binarios)\n",
    "    df_ultimo_periodo['pix_trans_30d'] = pd.to_numeric(df_ultimo_periodo['pix_trans_30d'], errors='coerce').fillna(0)\n",
    "\n",
    "# Análise evolutiva PIX por período\n",
    "print(f\"\\nEVOLUÇÃO PIX POR PERÍODO:\")\n",
    "print(f\"{'Período':<10} {'PIX Ativo':<12} {'% Total':<10} {'Transações':<12} {'Variação':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "evolucao_pix = []\n",
    "for i, periodo in enumerate(ultimos_3_meses):\n",
    "    df_periodo = df_pix_3m[df_pix_3m['ano_mes'] == periodo]\n",
    "    total_periodo = len(df_periodo)\n",
    "    \n",
    "    # PIX Ativo\n",
    "    if 'cad_pix_ativo' in df_periodo.columns:\n",
    "        pix_ativo = df_periodo['cad_pix_ativo'].sum()\n",
    "        perc_pix_ativo = (pix_ativo / total_periodo) * 100 if total_periodo > 0 else 0\n",
    "    else:\n",
    "        pix_ativo = 0\n",
    "        perc_pix_ativo = 0\n",
    "    \n",
    "    # Transações PIX\n",
    "    if 'pix_trans_30d' in df_periodo.columns:\n",
    "        pix_transacoes = df_periodo['pix_trans_30d'].sum()\n",
    "    else:\n",
    "        pix_transacoes = 0\n",
    "    \n",
    "    # Variação em relação ao período anterior\n",
    "    if i > 0:\n",
    "        var_pix = pix_ativo - evolucao_pix[-1]['pix_ativo']\n",
    "        var_perc = ((pix_ativo / evolucao_pix[-1]['pix_ativo']) - 1) * 100 if evolucao_pix[-1]['pix_ativo'] > 0 else 0\n",
    "        variacao_str = f\"{var_pix:+,} ({var_perc:+.1f}%)\"\n",
    "    else:\n",
    "        variacao_str = \"Base\"\n",
    "    \n",
    "    \n",
    "    evolucao_pix.append({\n",
    "        'periodo': periodo,\n",
    "        'pix_ativo': pix_ativo,\n",
    "        'perc_pix_ativo': perc_pix_ativo,\n",
    "        'pix_transacoes': pix_transacoes,\n",
    "        'total_periodo': total_periodo\n",
    "    })\n",
    "\n",
    "# Análise PIX por tipo de pessoa\n",
    "print(f\"\\nPIX POR TIPO DE PESSOA (último período):\")\n",
    "df_ultimo_periodo = df_pix_3m[df_pix_3m['ano_mes'] == ultimos_3_meses[-1]]\n",
    "\n",
    "for pessoa in ['PF', 'PJ']:\n",
    "    df_pessoa = df_ultimo_periodo[df_ultimo_periodo['tipo_pessoa'] == pessoa]\n",
    "    if not df_pessoa.empty:\n",
    "        total_pessoa = len(df_pessoa)\n",
    "        \n",
    "        # PIX Ativo\n",
    "        if 'cad_pix_ativo' in df_pessoa.columns:\n",
    "            pix_ativo_pessoa = df_pessoa['cad_pix_ativo'].sum()\n",
    "            perc_pix_pessoa = (pix_ativo_pessoa / total_pessoa) * 100\n",
    "        else:\n",
    "            pix_ativo_pessoa = 0\n",
    "            perc_pix_pessoa = 0\n",
    "        \n",
    "        # Transações médias\n",
    "        if 'pix_trans_30d' in df_pessoa.columns:\n",
    "            # Se existir coluna de PIX ativo, calcula média apenas para ativos\n",
    "            if 'cad_pix_ativo' in df_pessoa.columns:\n",
    "                ativos_mask = df_pessoa['cad_pix_ativo'] == 1\n",
    "                if ativos_mask.any():\n",
    "                    trans_media = df_pessoa.loc[ativos_mask, 'pix_trans_30d'].mean()\n",
    "                else:\n",
    "                    trans_media = 0\n",
    "            else:\n",
    "                trans_media = df_pessoa['pix_trans_30d'].mean()\n",
    "            if pd.isna(trans_media):\n",
    "                trans_media = 0\n",
    "        else:\n",
    "            trans_media = 0\n",
    "        \n",
    "        print(f\"  {pessoa}: {pix_ativo_pessoa:,}/{total_pessoa:,} ({perc_pix_pessoa:.1f}%) | Trans/mês: {trans_media:.1f}\")\n",
    "\n",
    "# Chaves fortes (análise baseada em campos disponíveis)\n",
    "print(f\"\\nANÁLISE DE CHAVES FORTES:\")\n",
    "\n",
    "# Assumindo que chaves fortes são: CPF, CNPJ, telefone, email\n",
    "# Como não temos detalhamento, usamos PIX ativo como proxy para chaves fortes\n",
    "if 'cad_pix_ativo' in df_ultimo_periodo.columns:\n",
    "    chaves_fortes = df_ultimo_periodo['cad_pix_ativo'].sum()\n",
    "    total_ultimo = len(df_ultimo_periodo)\n",
    "    perc_fortes = (chaves_fortes / total_ultimo) * 100\n",
    "    \n",
    "    print(f\"  Associados com PIX ativo (proxy chaves fortes): {chaves_fortes:,} ({perc_fortes:.1f}%)\")\n",
    "    \n",
    "    # Por segmento\n",
    "    for pessoa in ['PF', 'PJ']:\n",
    "        df_pessoa = df_ultimo_periodo[df_ultimo_periodo['tipo_pessoa'] == pessoa]\n",
    "        if not df_pessoa.empty:\n",
    "            fortes_pessoa = df_pessoa['cad_pix_ativo'].sum()\n",
    "            total_pessoa = len(df_pessoa)\n",
    "            perc_pessoa = (fortes_pessoa / total_pessoa) * 100\n",
    "            print(f\"    {pessoa}: {fortes_pessoa:,}/{total_pessoa:,} ({perc_pessoa:.1f}%)\")\n",
    "\n",
    "# Análise de intensidade de uso PIX\n",
    "if 'pix_trans_30d' in df_ultimo_periodo.columns:\n",
    "    df_pix_users = df_ultimo_periodo[df_ultimo_periodo['cad_pix_ativo'] == 1].copy()\n",
    "    \n",
    "    if not df_pix_users.empty:\n",
    "        print(f\"\\nINTENSIDADE DE USO PIX (usuários ativos):\")\n",
    "        \n",
    "        # Classificar por intensidade\n",
    "        df_pix_users['intensidade_pix'] = pd.cut(\n",
    "            df_pix_users['pix_trans_30d'],\n",
    "            bins=[0, 1, 5, 15, float('inf')],\n",
    "            labels=['Muito_Baixo', 'Baixo', 'Médio', 'Alto']\n",
    "        )\n",
    "        \n",
    "        dist_intensidade = df_pix_users['intensidade_pix'].value_counts()\n",
    "        total_pix_users = len(df_pix_users)\n",
    "        \n",
    "        print(f\"  Total usuários PIX: {total_pix_users:,}\")\n",
    "        for intensidade, qtd in dist_intensidade.items():\n",
    "            perc = (qtd / total_pix_users) * 100\n",
    "            print(f\"    {intensidade}: {qtd:,} ({perc:.1f}%)\")\n",
    "        \n",
    "        # Média de transações\n",
    "        media_trans = df_pix_users['pix_trans_30d'].mean()\n",
    "        mediana_trans = df_pix_users['pix_trans_30d'].median()\n",
    "        print(f\"  Média transações/mês: {media_trans:.1f}\")\n",
    "        print(f\"  Mediana transações/mês: {mediana_trans:.1f}\")\n",
    "\n",
    "# PIX vs Principalidade\n",
    "if 'pontos_principalidade' in df_ultimo_periodo.columns and 'cad_pix_ativo' in df_ultimo_periodo.columns:\n",
    "    print(f\"\\nPIX vs PRINCIPALIDADE:\")\n",
    "    \n",
    "    # Média de principalidade com/sem PIX\n",
    "    princ_com_pix = df_ultimo_periodo[df_ultimo_periodo['cad_pix_ativo'] == 1]['pontos_principalidade'].mean()\n",
    "    princ_sem_pix = df_ultimo_periodo[df_ultimo_periodo['cad_pix_ativo'] == 0]['pontos_principalidade'].mean()\n",
    "    \n",
    "    print(f\"  Com PIX: {princ_com_pix:.1f} pontos de principalidade\")\n",
    "    print(f\"  Sem PIX: {princ_sem_pix:.1f} pontos de principalidade\")\n",
    "    print(f\"  Diferença: {princ_com_pix - princ_sem_pix:+.1f} pontos\")\n",
    "\n",
    "# Resumo PIX\n",
    "if evolucao_pix:\n",
    "    primeiro_periodo = evolucao_pix[0]\n",
    "    ultimo_periodo = evolucao_pix[-1]\n",
    "    \n",
    "    crescimento_pix = ultimo_periodo['pix_ativo'] - primeiro_periodo['pix_ativo']\n",
    "    crescimento_perc = ((ultimo_periodo['pix_ativo'] / primeiro_periodo['pix_ativo']) - 1) * 100 if primeiro_periodo['pix_ativo'] > 0 else 0\n",
    "    \n",
    "    resumo_pix = {\n",
    "        'pix_ativo_atual': ultimo_periodo['pix_ativo'],\n",
    "        'perc_penetracao': ultimo_periodo['perc_pix_ativo'],\n",
    "        'crescimento_absoluto': crescimento_pix,\n",
    "        'crescimento_percentual': crescimento_perc,\n",
    "        'transacoes_totais': ultimo_periodo['pix_transacoes']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n📋 RESUMO EXECUTIVO - PIX\")\n",
    "    print(f\"PIX ativo atual: {resumo_pix['pix_ativo_atual']:,} ({resumo_pix['perc_penetracao']:.1f}%)\")\n",
    "    print(f\"Crescimento 3M: {resumo_pix['crescimento_absoluto']:+,} ({resumo_pix['crescimento_percentual']:+.1f}%)\")\n",
    "    print(f\"Transações totais: {resumo_pix['transacoes_totais']:,}\")\n",
    "    \n",
    "    globals()['resumo_pix'] = resumo_pix\n",
    "    globals()['evolucao_pix'] = evolucao_pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise Cash-In vs Cash-Out Últimos 3 Meses\n",
    "print(\"📊 ANÁLISE CASH-IN vs CASH-OUT - ÚLTIMOS 3 MESES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar disponibilidade dos campos\n",
    "campos_cash = ['cash_in', 'cash_out', 'cash_total']\n",
    "campos_disponiveis = [c for c in campos_cash if c in df_base.columns]\n",
    "\n",
    "print(f\"Campos de cash disponíveis: {campos_disponiveis}\")\n",
    "\n",
    "if 'cash_in' in df_base.columns and 'cash_out' in df_base.columns:\n",
    "    \n",
    "    # Dados dos últimos 3 meses\n",
    "    df_cash_3m = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].copy()\n",
    "    \n",
    "    # Calcular diferença cash-in vs cash-out\n",
    "    df_cash_3m['saldo_cash'] = df_cash_3m['cash_in'] - df_cash_3m['cash_out']\n",
    "    df_cash_3m['cash_total_calc'] = df_cash_3m['cash_in'] + df_cash_3m['cash_out']\n",
    "    \n",
    "    print(f\"\\nANÁLISE POR PERÍODO:\")\n",
    "    print(f\"{'Período':<10} {'Cash-In':<15} {'Cash-Out':<15} {'Saldo':<15} {'Total Mov':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    analise_cash_periodo = []\n",
    "    \n",
    "    for periodo in ultimos_3_meses:\n",
    "        df_periodo = df_cash_3m[df_cash_3m['ano_mes'] == periodo]\n",
    "        \n",
    "        cash_in_total = df_periodo['cash_in'].sum()\n",
    "        cash_out_total = df_periodo['cash_out'].sum()\n",
    "        saldo_periodo = cash_in_total - cash_out_total\n",
    "        movimentacao_total = cash_in_total + cash_out_total\n",
    "        \n",
    "        print(f\"{periodo:<10} {cash_in_total:<15,.0f} {cash_out_total:<15,.0f} \"\n",
    "              f\"{saldo_periodo:<15,.0f} {movimentacao_total:<15,.0f}\")\n",
    "        \n",
    "        analise_cash_periodo.append({\n",
    "            'periodo': periodo,\n",
    "            'cash_in': cash_in_total,\n",
    "            'cash_out': cash_out_total,\n",
    "            'saldo': saldo_periodo,\n",
    "            'movimentacao_total': movimentacao_total,\n",
    "            'num_associados': len(df_periodo)\n",
    "        })\n",
    "    \n",
    "    # Estatísticas gerais (último período)\n",
    "    df_ultimo = df_cash_3m[df_cash_3m['ano_mes'] == ultimos_3_meses[-1]]\n",
    "    \n",
    "    print(f\"\\nESTATÍSTICAS ÚLTIMO PERÍODO ({ultimos_3_meses[-1]}):\")\n",
    "    print(f\"  Total associados: {len(df_ultimo):,}\")\n",
    "    \n",
    "    # Estatísticas cash-in\n",
    "    cash_in_stats = df_ultimo['cash_in'].describe()\n",
    "    print(f\"\\n  Cash-In:\")\n",
    "    print(f\"    Média por associado: R$ {cash_in_stats['mean']:,.2f}\")\n",
    "    print(f\"    Mediana: R$ {cash_in_stats['50%']:,.2f}\")\n",
    "    print(f\"    Total: R$ {cash_in_stats['count'] * cash_in_stats['mean']:,.0f}\")\n",
    "    \n",
    "    # Estatísticas cash-out\n",
    "    cash_out_stats = df_ultimo['cash_out'].describe()\n",
    "    print(f\"\\n  Cash-Out:\")\n",
    "    print(f\"    Média por associado: R$ {cash_out_stats['mean']:,.2f}\")\n",
    "    print(f\"    Mediana: R$ {cash_out_stats['50%']:,.2f}\")\n",
    "    print(f\"    Total: R$ {cash_out_stats['count'] * cash_out_stats['mean']:,.0f}\")\n",
    "    \n",
    "    # Saldo médio\n",
    "    saldo_medio = df_ultimo['saldo_cash'].mean()\n",
    "    saldo_mediano = df_ultimo['saldo_cash'].median()\n",
    "    print(f\"\\n  Saldo (Cash-In - Cash-Out):\")\n",
    "    print(f\"    Saldo médio: R$ {saldo_medio:,.2f}\")\n",
    "    print(f\"    Saldo mediano: R$ {saldo_mediano:,.2f}\")\n",
    "    \n",
    "    \n",
    "    # Classificação por tipo de fluxo\n",
    "    df_ultimo['tipo_fluxo'] = np.select([\n",
    "        df_ultimo['saldo_cash'] > 100,\n",
    "        df_ultimo['saldo_cash'] < -100,\n",
    "        True\n",
    "    ], [\n",
    "        'Entrada_Liquida',\n",
    "        'Saida_Liquida', \n",
    "        'Equilibrio'\n",
    "    ])\n",
    "    \n",
    "    dist_fluxo = df_ultimo['tipo_fluxo'].value_counts()\n",
    "    total_ultimo = len(df_ultimo)\n",
    "    \n",
    "    print(f\"\\nDISTRIBUIÇÃO POR TIPO DE FLUXO:\")\n",
    "    for tipo, qtd in dist_fluxo.items():\n",
    "        perc = (qtd / total_ultimo) * 100\n",
    "        print(f\"  {tipo.replace('_', ' ')}: {qtd:,} ({perc:.1f}%)\")\n",
    "    \n",
    "    # Análise por tipo de pessoa\n",
    "    print(f\"\\nCASH FLOW POR TIPO DE PESSOA:\")\n",
    "    \n",
    "    for pessoa in ['PF', 'PJ']:\n",
    "        df_pessoa = df_ultimo[df_ultimo['tipo_pessoa'] == pessoa]\n",
    "        if not df_pessoa.empty:\n",
    "            cash_in_pessoa = df_pessoa['cash_in'].mean()\n",
    "            cash_out_pessoa = df_pessoa['cash_out'].mean()\n",
    "            saldo_pessoa = df_pessoa['saldo_cash'].mean()\n",
    "            \n",
    "            print(f\"\\n  {pessoa} (n={len(df_pessoa):,}):\")\n",
    "            print(f\"    Cash-In médio: R$ {cash_in_pessoa:,.2f}\")\n",
    "            print(f\"    Cash-Out médio: R$ {cash_out_pessoa:,.2f}\")\n",
    "            print(f\"    Saldo médio: R$ {saldo_pessoa:,.2f}\")\n",
    "    \n",
    "    # Top movimentadores\n",
    "    print(f\"\\nTOP 10 MAIORES MOVIMENTAÇÕES TOTAIS:\")\n",
    "    top_movimentacao = df_ultimo.nlargest(10, 'cash_total_calc')\n",
    "    \n",
    "    for i, row in enumerate(top_movimentacao.itertuples(), 1):\n",
    "        cpf_mask = str(row.cpf_cnpj)[-4:] if len(str(row.cpf_cnpj)) >= 4 else str(row.cpf_cnpj)\n",
    "        print(f\"  {i:2d}. CPF: ...{cpf_mask} | {row.tipo_pessoa} | \"\n",
    "              f\"In: R${row.cash_in:,.0f} | Out: R${row.cash_out:,.0f} | \"\n",
    "              f\"Saldo: R${row.saldo_cash:+,.0f}\")\n",
    "    \n",
    "    # Correlação cash vs principalidade\n",
    "    if 'pontos_principalidade' in df_ultimo.columns:\n",
    "        print(f\"\\nCORRELAÇÃO CASH vs PRINCIPALIDADE:\")\n",
    "        \n",
    "        corr_cash_in = df_ultimo['cash_in'].corr(df_ultimo['pontos_principalidade'])\n",
    "        corr_cash_out = df_ultimo['cash_out'].corr(df_ultimo['pontos_principalidade'])\n",
    "        corr_saldo = df_ultimo['saldo_cash'].corr(df_ultimo['pontos_principalidade'])\n",
    "        \n",
    "        print(f\"  Cash-In vs Principalidade: {corr_cash_in:.3f}\")\n",
    "        print(f\"  Cash-Out vs Principalidade: {corr_cash_out:.3f}\")\n",
    "        print(f\"  Saldo vs Principalidade: {corr_saldo:.3f}\")\n",
    "    \n",
    "    # Tendência de evolução\n",
    "    if len(analise_cash_periodo) >= 2:\n",
    "        print(f\"\\nTENDÊNCIA DE EVOLUÇÃO:\")\n",
    "        \n",
    "        primeiro = analise_cash_periodo[0]\n",
    "        ultimo = analise_cash_periodo[-1]\n",
    "        \n",
    "        var_cash_in = ((ultimo['cash_in'] / primeiro['cash_in']) - 1) * 100 if primeiro['cash_in'] > 0 else 0\n",
    "        var_cash_out = ((ultimo['cash_out'] / primeiro['cash_out']) - 1) * 100 if primeiro['cash_out'] > 0 else 0\n",
    "        var_movimentacao = ((ultimo['movimentacao_total'] / primeiro['movimentacao_total']) - 1) * 100 if primeiro['movimentacao_total'] > 0 else 0\n",
    "        \n",
    "        print(f\"  Cash-In: {var_cash_in:+.1f}%\")\n",
    "        print(f\"  Cash-Out: {var_cash_out:+.1f}%\")\n",
    "        print(f\"  Movimentação Total: {var_movimentacao:+.1f}%\")\n",
    "    \n",
    "    # Resumo cash\n",
    "    resumo_cash = {\n",
    "        'cash_in_total': analise_cash_periodo[-1]['cash_in'],\n",
    "        'cash_out_total': analise_cash_periodo[-1]['cash_out'],\n",
    "        'saldo_total': analise_cash_periodo[-1]['saldo'],\n",
    "        'movimentacao_total': analise_cash_periodo[-1]['movimentacao_total'],\n",
    "        'saldo_medio_associado': saldo_medio,\n",
    "        'entrada_liquida': dist_fluxo.get('Entrada_Liquida', 0),\n",
    "        'saida_liquida': dist_fluxo.get('Saida_Liquida', 0)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n📋 RESUMO EXECUTIVO - CASH FLOW\")\n",
    "    print(f\"Movimentação total: R$ {resumo_cash['movimentacao_total']:,.0f}\")\n",
    "    print(f\"Saldo geral: R$ {resumo_cash['saldo_total']:+,.0f}\")\n",
    "    print(f\"Entrada líquida: {resumo_cash['entrada_liquida']:,} associados\")\n",
    "    print(f\"Saída líquida: {resumo_cash['saida_liquida']:,} associados\")\n",
    "    \n",
    "    globals()['resumo_cash'] = resumo_cash\n",
    "    globals()['analise_cash_periodo'] = analise_cash_periodo\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Campos de cash-in e cash-out não disponíveis para análise completa\")\n",
    "    resumo_cash = {'cash_in_total': 0, 'cash_out_total': 0, 'saldo_total': 0}\n",
    "\n",
    "# Normalizando os campos booleanos para True/False\n",
    "campos_bool = [\n",
    "    'tem_conta_corrente', 'tem_poupanca', 'tem_investimento', 'tem_previdencia', \n",
    "    'tem_seguro', 'tem_consorcio', 'tem_cartao_credito', 'tem_credito', \n",
    "    'tem_cambio', 'tem_cobranca'\n",
    "]\n",
    "\n",
    "for col in campos_bool:\n",
    "    if col in df_base.columns:\n",
    "        df_base[col] = df_base[col].fillna(False).astype(bool)\n",
    "\n",
    "# Convertendo colunas de data para datetime\n",
    "for col in ['data_abertura', 'data_encerramento']:\n",
    "    if col in df_base.columns:\n",
    "        df_base[col] = pd.to_datetime(df_base[col], errors='coerce')\n",
    "\n",
    "df_base.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56d03d",
   "metadata": {},
   "source": [
    "## 9. Productos Faltantes: Categoría 1 y Categoría 2\n",
    "\n",
    "Identificación de brechas de productos en dos categorías estratégicas y cálculo de cobertura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de Produtos Faltantes (Categoria 1 e 2)\n",
    "print(\"📊 ANÁLISE DE PRODUTOS FALTANTES - CATEGORIAS 1 E 2\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Definição das categorias (ajustar conforme necessidade)\n",
    "CATEGORIA_1 = {\n",
    "    'PIX Ativo': 'cad_pix_ativo',\n",
    "    'Cartão Débito': 'possui_cartao_debito',\n",
    "    'App/Canais Digitais': 'transacao_app',\n",
    "    'Débito Automático': 'debito_conta_ativo'\n",
    "}\n",
    "\n",
    "CATEGORIA_2 = {\n",
    "    'Cartão Crédito': 'possui_cartao_credito',\n",
    "    'Credenciamento': 'possui_adquirencia',\n",
    "    'Cobrança': 'possui_cobranca',\n",
    "    'Folha Pagamento': 'possui_folha_pagamento',\n",
    "    'Domicílio': 'possui_domicilio',\n",
    "    'Open Finance': 'ativou_open_finance'\n",
    "}\n",
    "\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_prod_atual = df_base[df_base['ano_mes'] == ultimo_mes].copy()\n",
    "\n",
    "# Garantir existência das colunas (criar se ausentes)\n",
    "for col in list(CATEGORIA_1.values()) + list(CATEGORIA_2.values()):\n",
    "    if col not in df_prod_atual.columns:\n",
    "        df_prod_atual[col] = 0\n",
    "\n",
    "# Calcular quantidade de produtos usados por categoria\n",
    "cat1_cols = list(CATEGORIA_1.values())\n",
    "cat2_cols = list(CATEGORIA_2.values())\n",
    "\n",
    "df_prod_atual['cat1_usados'] = df_prod_atual[cat1_cols].sum(axis=1)\n",
    "df_prod_atual['cat2_usados'] = df_prod_atual[cat2_cols].sum(axis=1)\n",
    "\n",
    "df_prod_atual['cat1_faltantes'] = len(cat1_cols) - df_prod_atual['cat1_usados']\n",
    "df_prod_atual['cat2_faltantes'] = len(cat2_cols) - df_prod_atual['cat2_usados']\n",
    "\n",
    "# Cobertura (percentual de produtos utilizados em cada categoria)\n",
    "df_prod_atual['cobertura_cat1'] = df_prod_atual['cat1_usados'] / len(cat1_cols)\n",
    "df_prod_atual['cobertura_cat2'] = df_prod_atual['cat2_usados'] / len(cat2_cols)\n",
    "\n",
    "# Distribuição de faltantes\n",
    "print(\"\\nDISTRIBUIÇÃO - CATEGORIA 1 (Produtos Transacionais Básicos):\")\n",
    "for falt in range(len(cat1_cols) + 1):\n",
    "    qtd = (df_prod_atual['cat1_faltantes'] == falt).sum()\n",
    "    perc = qtd / len(df_prod_atual) * 100\n",
    "    print(f\"  Faltam {falt} prod.: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "print(\"\\nDISTRIBUIÇÃO - CATEGORIA 2 (Cross-Sell / Relacionamento):\")\n",
    "for falt in range(len(cat2_cols) + 1):\n",
    "    qtd = (df_prod_atual['cat2_faltantes'] == falt).sum()\n",
    "    perc = qtd / len(df_prod_atual) * 100\n",
    "    print(f\"  Faltam {falt} prod.: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# Penetração por produto (ordenar)\n",
    "print(\"\\nPENETRAÇÃO POR PRODUTO (ÚLTIMO PERÍODO):\")\n",
    "penetracao = []\n",
    "for nome, col in {**CATEGORIA_1, **CATEGORIA_2}.items():\n",
    "    if col in df_prod_atual.columns:\n",
    "        usuarios = df_prod_atual[col].sum()\n",
    "        perc = usuarios / len(df_prod_atual) * 100 if len(df_prod_atual) > 0 else 0\n",
    "        penetracao.append({'produto': nome, 'usuarios': int(usuarios), 'perc': perc})\n",
    "\n",
    "df_penetracao = pd.DataFrame(penetracao).sort_values('perc', ascending=False)\n",
    "for row in df_penetracao.itertuples():\n",
    "    print(f\"  {row.produto:<18} {row.usuarios:>8,} ({row.perc:5.1f}%)\")\n",
    "\n",
    "# Identificar oportunidades (associados com baixa cobertura transacional e alta principalidade)\n",
    "if 'pontos_principalidade' in df_prod_atual.columns:\n",
    "    mediana_princ = df_prod_atual['pontos_principalidade'].median()\n",
    "    oportunidades_cat1 = df_prod_atual[(df_prod_atual['cobertura_cat1'] < 0.5) & (df_prod_atual['pontos_principalidade'] > mediana_princ)]\n",
    "    oportunidades_cat2 = df_prod_atual[(df_prod_atual['cobertura_cat2'] < 0.5) & (df_prod_atual['pontos_principalidade'] > mediana_princ)]\n",
    "    print(f\"\\nOPORTUNIDADES - Alta principalidade mas baixa cobertura transacional (<50%): {len(oportunidades_cat1):,}\")\n",
    "    print(f\"OPORTUNIDADES - Alta principalidade mas baixa cobertura cross-sell (<50%): {len(oportunidades_cat2):,}\")\n",
    "\n",
    "# Resumo executivo\n",
    "resumo_prod_faltantes = {\n",
    "    'total': len(df_prod_atual),\n",
    "    'cobertura_cat1_media': df_prod_atual['cobertura_cat1'].mean(),\n",
    "    'cobertura_cat2_media': df_prod_atual['cobertura_cat2'].mean(),\n",
    "    'sem_prod_cat1': (df_prod_atual['cat1_usados'] == 0).sum(),\n",
    "    'sem_prod_cat2': (df_prod_atual['cat2_usados'] == 0).sum()\n",
    "}\n",
    "\n",
    "print(\"\\n📋 RESUMO EXECUTIVO - PRODUTOS FALTANTES\")\n",
    "print(f\"Cobertura média Cat 1: {resumo_prod_faltantes['cobertura_cat1_media']*100:,.1f}%\")\n",
    "print(f\"Cobertura média Cat 2: {resumo_prod_faltantes['cobertura_cat2_media']*100:,.1f}%\")\n",
    "print(f\"Sem nenhum produto Cat1: {resumo_prod_faltantes['sem_prod_cat1']:,} ({resumo_prod_faltantes['sem_prod_cat1']/len(df_prod_atual)*100:.1f}%)\")\n",
    "print(f\"Sem nenhum produto Cat2: {resumo_prod_faltantes['sem_prod_cat2']:,} ({resumo_prod_faltantes['sem_prod_cat2']/len(df_prod_atual)*100:.1f}%)\")\n",
    "\n",
    "globals()['df_produtos_atual'] = df_prod_atual\n",
    "globals()['df_penetracao_produtos'] = df_penetracao\n",
    "globals()['resumo_prod_faltantes'] = resumo_prod_faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c8b99",
   "metadata": {},
   "source": [
    "## 10. SOW Sicredi vs Fora\n",
    "\n",
    "Análise do indicador de Share of Wallet (SOW) para identificar potencial de captura adicional de relacionamento entre associados dentro e fora da cooperativa. Caso a métrica `sow` não esteja disponível na base, criamos um proxy a partir de pontos de principalidade e uso de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise SOW Sicredi vs Fora\n",
    "print(\"📊 ANÁLISE SOW (SHARE OF WALLET) - PROXY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar existência ou criar proxy\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_sow = df_base[df_base['ano_mes'] == ultimo_mes].copy()\n",
    "\n",
    "if 'sow' not in df_sow.columns:\n",
    "    # Proxy simples: normalizar pontos de principalidade 0-1 e ponderar por cobertura de produtos transacionais\n",
    "    produtos_trans = [c for c in ['cad_pix_ativo','transacao_app','possui_cartao_debito','debito_conta_ativo'] if c in df_sow.columns]\n",
    "    if produtos_trans:\n",
    "        df_sow['cobertura_trans'] = df_sow[produtos_trans].mean(axis=1)\n",
    "    else:\n",
    "        df_sow['cobertura_trans'] = 0\n",
    "    princ_min = df_sow['pontos_principalidade'].min()\n",
    "    princ_max = df_sow['pontos_principalidade'].max()\n",
    "    escala = (df_sow['pontos_principalidade'] - princ_min) / (princ_max - princ_min) if princ_max > princ_min else 0\n",
    "    df_sow['sow_proxy'] = (0.6 * escala + 0.4 * df_sow['cobertura_trans']).clip(0,1)\n",
    "    sow_col = 'sow_proxy'\n",
    "else:\n",
    "    sow_col = 'sow'\n",
    "\n",
    "# Faixas de SOW\n",
    "bins = [0,0.25,0.5,0.75,1.01]\n",
    "labels = ['Baixo','Médio-Baixo','Médio-Alto','Alto']\n",
    "df_sow['faixa_sow'] = pd.cut(df_sow[sow_col], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Distribuição geral\n",
    "dist_sow = df_sow['faixa_sow'].value_counts().reindex(labels)\n",
    "print(\"\\nDISTRIBUIÇÃO GERAL SOW:\")\n",
    "for faixa, qtd in dist_sow.items():\n",
    "    perc = qtd / len(df_sow) * 100 if len(df_sow) else 0\n",
    "    print(f\"  {faixa:<11}: {qtd:,} ({perc:.1f}%)\")\n",
    "\n",
    "# SOW por tipo de pessoa\n",
    "print(\"\\nSOW MÉDIO POR TIPO DE PESSOA:\")\n",
    "for tp in ['PF','PJ']:\n",
    "    subset = df_sow[df_sow['tipo_pessoa']==tp]\n",
    "    if not subset.empty:\n",
    "        print(f\"  {tp}: {subset[sow_col].mean():.3f} (n={len(subset):,})\")\n",
    "\n",
    "# Relação SOW x Principalidade\n",
    "corr_sow_princ = df_sow[sow_col].corr(df_sow['pontos_principalidade']) if 'pontos_principalidade' in df_sow.columns else np.nan\n",
    "print(f\"\\nCorrelação SOW vs Principalidade: {corr_sow_princ:.3f}\")\n",
    "\n",
    "# Oportunidades: alta principalidade (>=P75) e SOW baixo/medio-baixo\n",
    "p75_princ = df_sow['pontos_principalidade'].quantile(0.75)\n",
    "oportunidades_sow = df_sow[(df_sow['pontos_principalidade']>=p75_princ) & (df_sow['faixa_sow'].isin(['Baixo','Médio-Baixo']))]\n",
    "print(f\"\\nOportunidades de expansão (alta principalidade & SOW<=0.5): {len(oportunidades_sow):,}\")\n",
    "\n",
    "# Resumo executivo\n",
    "resumo_sow = {\n",
    "    'total': len(df_sow),\n",
    "    'sow_medio': df_sow[sow_col].mean(),\n",
    "    'alta_participacao': (df_sow['faixa_sow']=='Alto').sum(),\n",
    "    'baixo_share': (df_sow['faixa_sow']=='Baixo').sum(),\n",
    "    'corr_princ': corr_sow_princ,\n",
    "    'oportunidades': len(oportunidades_sow)\n",
    "}\n",
    "\n",
    "print(\"\\n📋 RESUMO EXECUTIVO - SOW\")\n",
    "print(f\"SOW médio: {resumo_sow['sow_medio']:.3f}\")\n",
    "print(f\"Alto SOW: {resumo_sow['alta_participacao']:,} ({resumo_sow['alta_participacao']/len(df_sow)*100:.1f}%)\")\n",
    "print(f\"Baixo SOW: {resumo_sow['baixo_share']:,} ({resumo_sow['baixo_share']/len(df_sow)*100:.1f}%)\")\n",
    "print(f\"Correlação com Principalidade: {resumo_sow['corr_princ']:.3f}\")\n",
    "print(f\"Oportunidades expansão: {resumo_sow['oportunidades']:,}\")\n",
    "\n",
    "globals()['resumo_sow'] = resumo_sow\n",
    "globals()['df_sow'] = df_sow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bebe8",
   "metadata": {},
   "source": [
    "## 11. Análise PJ: Domicílio, Cobrança, PIX e Uso Transacional\n",
    "\n",
    "Foco em produtos críticos para relacionamento PJ: domicílio, cobrança, credenciamento, folha pagamento e adoção de PIX. Avaliação de penetração, gaps e relação com principalidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise PJ detalhada\n",
    "print(\"📊 ANÁLISE PJ - PRODUTOS E USO TRANSACIONAL\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_pj = df_base[(df_base['ano_mes']==ultimo_mes) & (df_base['tipo_pessoa']=='PJ')].copy()\n",
    "print(f\"Total PJ analisados: {len(df_pj):,}\")\n",
    "\n",
    "produtos_pj = {\n",
    "    'Domicílio': 'possui_domicilio',\n",
    "    'Cobrança': 'possui_cobranca',\n",
    "    'Credenciamento': 'possui_adquirencia',\n",
    "    'Folha Pagamento': 'possui_folha_pagamento',\n",
    "    'PIX Ativo': 'cad_pix_ativo'\n",
    "}\n",
    "\n",
    "# Garantir colunas\n",
    "for c in produtos_pj.values():\n",
    "    if c not in df_pj.columns:\n",
    "        df_pj[c] = 0\n",
    "\n",
    "# Penetração\n",
    "penetracao_pj = []\n",
    "for nome, col in produtos_pj.items():\n",
    "    usuarios = df_pj[col].sum()\n",
    "    perc = usuarios/len(df_pj)*100 if len(df_pj)>0 else 0\n",
    "    penetracao_pj.append({'produto': nome, 'usuarios': int(usuarios), 'perc': perc})\n",
    "\n",
    "df_pen_pj = pd.DataFrame(penetracao_pj).sort_values('perc', ascending=False)\n",
    "print(\"\\nPENETRAÇÃO DE PRODUTOS PJ:\")\n",
    "for r in df_pen_pj.itertuples():\n",
    "    print(f\"  {r.produto:<16} {r.usuarios:>7,} ({r.perc:5.1f}%)\")\n",
    "\n",
    "# Cobertura composta (quantos desses chave usa)\n",
    "cols_chave = list(produtos_pj.values())\n",
    "df_pj['prod_chave_usados'] = df_pj[cols_chave].sum(axis=1)\n",
    "print(\"\\nDistribuição nº de produtos chave usados:\")\n",
    "for k in range(len(cols_chave)+1):\n",
    "    qtd = (df_pj['prod_chave_usados']==k).sum()\n",
    "    print(f\"  Usa {k} prod.: {qtd:,} ({qtd/len(df_pj)*100:.1f}%)\")\n",
    "\n",
    "# PIX transações (intensidade) se existir\n",
    "if 'pix_trans_30d' in df_pj.columns and 'cad_pix_ativo' in df_pj.columns:\n",
    "    ativos_pix = df_pj[df_pj['cad_pix_ativo']==1]\n",
    "    if not ativos_pix.empty:\n",
    "        ativos_pix['intensidade_pix'] = pd.cut(\n",
    "            ativos_pix['pix_trans_30d'],\n",
    "            bins=[0,1,5,15,float('inf')],\n",
    "            labels=['Muito_Baixo','Baixo','Médio','Alto']\n",
    "        )\n",
    "        print(\"\\nINTENSIDADE PIX (PJ com PIX Ativo):\")\n",
    "        dist_int = ativos_pix['intensidade_pix'].value_counts()\n",
    "        for cat, qtd in dist_int.items():\n",
    "            print(f\"  {cat:<12}: {qtd:,} ({qtd/len(ativos_pix)*100:.1f}%)\")\n",
    "\n",
    "# Relação cada produto x principalidade média\n",
    "if 'pontos_principalidade' in df_pj.columns:\n",
    "    print(\"\\nPRINCIPALIDADE MÉDIA POR ADOÇÃO DE PRODUTO:\")\n",
    "    princ_med_base = df_pj['pontos_principalidade'].mean()\n",
    "    for nome, col in produtos_pj.items():\n",
    "        if col in df_pj.columns:\n",
    "            media_com = df_pj[df_pj[col]==1]['pontos_principalidade'].mean()\n",
    "            media_sem = df_pj[df_pj[col]==0]['pontos_principalidade'].mean()\n",
    "            uplift = media_com - media_sem\n",
    "            print(f\"  {nome:<16} Com: {media_com:7.2f} | Sem: {media_sem:7.2f} | Uplift: {uplift:+.2f}\")\n",
    "\n",
    "# Oportunidades: alta principalidade mas sem domicílio ou cobrança\n",
    "if 'pontos_principalidade' in df_pj.columns:\n",
    "    p75 = df_pj['pontos_principalidade'].quantile(0.75)\n",
    "    oportunidades_domicilio = df_pj[(df_pj['pontos_principalidade']>=p75) & (df_pj['possui_domicilio']==0)]\n",
    "    oportunidades_cobranca = df_pj[(df_pj['pontos_principalidade']>=p75) & (df_pj['possui_cobranca']==0)]\n",
    "    print(f\"\\nOportunidades Domicílio (alto princ, sem domicílio): {len(oportunidades_domicilio):,}\")\n",
    "    print(f\"Oportunidades Cobrança (alto princ, sem cobrança): {len(oportunidades_cobranca):,}\")\n",
    "\n",
    "resumo_pj = {\n",
    "    'total_pj': len(df_pj),\n",
    "    'pix_ativo': df_pj['cad_pix_ativo'].sum() if 'cad_pix_ativo' in df_pj.columns else 0,\n",
    "    'domicilio': df_pj['possui_domicilio'].sum() if 'possui_domicilio' in df_pj.columns else 0,\n",
    "    'cobranca': df_pj['possui_cobranca'].sum() if 'possui_cobranca' in df_pj.columns else 0\n",
    "}\n",
    "print(\"\\n📋 RESUMO EXECUTIVO - PJ\")\n",
    "print(f\"Total PJ: {resumo_pj['total_pj']:,}\")\n",
    "print(f\"Domicílio: {resumo_pj['domicilio']:,} ({resumo_pj['domicilio']/len(df_pj)*100:.1f}%)\")\n",
    "print(f\"Cobrança: {resumo_pj['cobranca']:,} ({resumo_pj['cobranca']/len(df_pj)*100:.1f}%)\")\n",
    "print(f\"PIX Ativo: {resumo_pj['pix_ativo']:,} ({resumo_pj['pix_ativo']/len(df_pj)*100:.1f}%)\")\n",
    "\n",
    "globals()['resumo_pj'] = resumo_pj\n",
    "globals()['df_pj_analise'] = df_pj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f9a09",
   "metadata": {},
   "source": [
    "## 12. Uso por Faixa de Dias sem Movimento\n",
    "\n",
    "Avalia adoção de produtos, principalidade e SOW conforme faixas de inatividade (dias sem movimento)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso por faixa de dias sem movimento\n",
    "print(\"📊 ANÁLISE POR FAIXA DE INATIVIDADE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_mov = df_base[df_base['ano_mes']==ultimo_mes].copy()\n",
    "\n",
    "# Cálculo direto de dias_sem_mov usando 'ult_movimento' (prioritário)\n",
    "if 'ult_movimento' in df_mov.columns:\n",
    "    df_mov['ult_movimento'] = pd.to_datetime(df_mov['ult_movimento'], errors='coerce')\n",
    "    hoje = pd.Timestamp.today().normalize()\n",
    "    df_mov['dias_sem_mov'] = (hoje - df_mov['ult_movimento']).dt.days\n",
    "else:\n",
    "    # Fallback legacy (mantido como contingência)\n",
    "    possiveis_col_datas = [\n",
    "        'ult_movimento_dt', 'dt_ult_movimento', 'data_ult_movimento',\n",
    "        'dt_ult_movto', 'dt_ult_mov', 'dat_ult_movimento'\n",
    "    ]\n",
    "    col_dt_encontrada = next((c for c in possiveis_col_datas if c in df_mov.columns), None)\n",
    "    if col_dt_encontrada:\n",
    "        df_mov[col_dt_encontrada] = pd.to_datetime(df_mov[col_dt_encontrada], errors='coerce')\n",
    "        hoje = pd.Timestamp.today().normalize()\n",
    "        df_mov['dias_sem_mov'] = (hoje - df_mov[col_dt_encontrada]).dt.days\n",
    "\n",
    "# Criar faixa_movimento (somente se dias_sem_mov disponível)\n",
    "if 'dias_sem_mov' in df_mov.columns:\n",
    "    df_mov['faixa_movimento'] = pd.cut(\n",
    "        df_mov['dias_sem_mov'],\n",
    "        bins=[-1,20,45,float('inf')],\n",
    "        labels=['Menos_20_dias','20_45_dias','Mais_45_dias']\n",
    "    )\n",
    "\n",
    "faixas = ['Menos_20_dias','20_45_dias','Mais_45_dias']\n",
    "produtos_chave = ['cad_pix_ativo','transacao_app','possui_cartao_debito','possui_cartao_credito']\n",
    "\n",
    "print(\"\\nMÉTRICAS POR FAIXA:\")\n",
    "print(f\"{'Faixa':<15} {'Qtde':>8} {'Princ Méd':>10} {'PIX%':>7} {'App%':>7} {'Déb%':>7} {'Créd%':>7}\")\n",
    "print('-'*70)\n",
    "resumo_faixas = []\n",
    "\n",
    "if 'faixa_movimento' in df_mov.columns:\n",
    "    for f in faixas:\n",
    "        subset = df_mov[df_mov['faixa_movimento'] == f]\n",
    "        if subset.empty: \n",
    "            continue\n",
    "        princ_med = subset['pontos_principalidade'].mean() if 'pontos_principalidade' in subset.columns else float('nan')\n",
    "        linha = {'faixa': f, 'qtde': len(subset), 'princ_media': princ_med}\n",
    "        valores_print = [f\"{f:<15} {len(subset):>8,} {princ_med:>10.1f}\"]\n",
    "        for prod in produtos_chave:\n",
    "            if prod in subset.columns:\n",
    "                perc = subset[prod].mean()*100\n",
    "                linha[prod+'_perc'] = perc\n",
    "                valores_print.append(f\"{perc:>6.1f}%\")\n",
    "            else:\n",
    "                valores_print.append(f\"{'-':>6}\")\n",
    "        print(' '.join(valores_print))\n",
    "        resumo_faixas.append(linha)\n",
    "else:\n",
    "    print(\"⚠️  Não foi possível criar 'faixa_movimento' (campo 'ult_movimento' ausente ou inválido).\")\n",
    "\n",
    "# Correlação dias vs principalidade\n",
    "if {'dias_sem_mov','pontos_principalidade'} <= set(df_mov.columns):\n",
    "    corr_mov_princ = df_mov['dias_sem_mov'].corr(df_mov['pontos_principalidade'])\n",
    "    print(f\"\\nCorrelação Dias Sem Mov. vs Principalidade: {corr_mov_princ:.3f}\")\n",
    "\n",
    "# Oportunidades: inativos + alta principalidade + sem PIX\n",
    "if {'dias_sem_mov','pontos_principalidade','cad_pix_ativo'} <= set(df_mov.columns):\n",
    "    p75 = df_mov['pontos_principalidade'].quantile(0.75)\n",
    "    oportunidades_inativos = df_mov[\n",
    "        (df_mov['dias_sem_mov']>45) &\n",
    "        (df_mov['pontos_principalidade']>=p75) &\n",
    "        (df_mov['cad_pix_ativo']==0)\n",
    "    ]\n",
    "    print(f\"Inativos com alta principalidade sem PIX: {len(oportunidades_inativos):,}\")\n",
    "\n",
    "# Resumo\n",
    "if 'faixa_movimento' in df_mov.columns:\n",
    "    resumo_movimento = {\n",
    "        'total': len(df_mov),\n",
    "        'menor_20': (df_mov['faixa_movimento']=='Menos_20_dias').sum(),\n",
    "        '20_45': (df_mov['faixa_movimento']=='20_45_dias').sum(),\n",
    "        'mais_45': (df_mov['faixa_movimento']=='Mais_45_dias').sum()\n",
    "    }\n",
    "else:\n",
    "    resumo_movimento = {'total': len(df_mov), 'menor_20': 0, '20_45': 0, 'mais_45': 0}\n",
    "\n",
    "print(\"\\n📋 RESUMO EXECUTIVO - FAIXAS INATIVIDADE\")\n",
    "print(f\"Menos 20 dias: {resumo_movimento['menor_20']:,}\")\n",
    "print(f\"20-45 dias: {resumo_movimento['20_45']:,}\")\n",
    "print(f\"Mais 45 dias: {resumo_movimento['mais_45']:,}\")\n",
    "\n",
    "globals()['resumo_movimento'] = resumo_movimento\n",
    "globals()['resumo_faixas_movimento'] = resumo_faixas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aa4a03",
   "metadata": {},
   "source": [
    "## 13. Visualizações e Gráficos Comparativos\n",
    "\n",
    "Gráficos para apresentação: distribuições, evoluções, heatmaps de correlação e penetração de produtos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizações chave para apresentação\n",
    "print(\"🎨 GERAÇÃO DE GRÁFICOS\")\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "figs_apresentacao = {}\n",
    "\n",
    "# 1. Distribuição Principalidade (último mês)\n",
    "ultimo_mes = df_base['ano_mes'].max()\n",
    "df_ult = df_base[df_base['ano_mes']==ultimo_mes]\n",
    "if 'pontos_principalidade' in df_ult.columns:\n",
    "    fig = px.histogram(df_ult, x='pontos_principalidade', nbins=40, title='Distribuição de Principalidade')\n",
    "    figs_apresentacao['dist_principalidade'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 2. Evolução média principalidade 3 últimos meses\n",
    "if 'pontos_principalidade' in df_base.columns:\n",
    "    evol_princ = df_base[df_base['ano_mes'].isin(ultimos_3_meses)].groupby('ano_mes')['pontos_principalidade'].mean().reset_index()\n",
    "    fig = px.line(evol_princ, x='ano_mes', y='pontos_principalidade', markers=True, title='Evolução Média Principalidade (3M)')\n",
    "    figs_apresentacao['evol_principalidade'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 3. Penetração produtos (bar)\n",
    "if 'df_penetracao_produtos' in globals():\n",
    "    fig = px.bar(df_penetracao_produtos.sort_values('perc'), x='perc', y='produto', orientation='h', title='Penetração de Produtos (%)')\n",
    "    figs_apresentacao['penet_produtos'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 4. Heatmap correlação (subset numérico)\n",
    "num_cols = []\n",
    "for c in ['pontos_principalidade','isa_analise','cad_pix_ativo','possui_cartao_credito','cash_in','cash_out']:\n",
    "    if c in df_ult.columns:\n",
    "        num_cols.append(c)\n",
    "if len(num_cols) >= 3:\n",
    "    corr = df_ult[num_cols].corr()\n",
    "    fig = px.imshow(corr, text_auto='.2f', title='Heatmap Correlação Indicadores')\n",
    "    figs_apresentacao['heatmap_corr'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 5. SOW Faixa x Principalidade média\n",
    "if 'df_sow' in globals() and 'faixa_sow' in df_sow.columns:\n",
    "    base_sow = df_sow.groupby('faixa_sow')['pontos_principalidade'].mean().reset_index()\n",
    "    fig = px.bar(base_sow, x='faixa_sow', y='pontos_principalidade', title='Principalidade Média por Faixa SOW')\n",
    "    figs_apresentacao['princ_por_sow'] = fig\n",
    "    fig.show()\n",
    "\n",
    "# 6. Inatividade x Adoção PIX\n",
    "if 'faixa_movimento' in df_ult.columns and 'cad_pix_ativo' in df_ult.columns:\n",
    "    inat_pix = df_ult.groupby('faixa_movimento')['cad_pix_ativo'].mean().reset_index()\n",
    "    fig = px.bar(inat_pix, x='faixa_movimento', y='cad_pix_ativo', title='Penetração PIX por Faixa de Inatividade', labels={'cad_pix_ativo':'PIX %'})\n",
    "    figs_apresentacao['pix_inatividade'] = fig\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\nTotal de gráficos gerados:\", len(figs_apresentacao))\n",
    "globals()['figs_apresentacao'] = figs_apresentacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Modelo de Pré-Churn (Machine Learning)\n",
    "\n",
    "Criação de features, definição de variável alvo (proxy churn = queda principalidade + inatividade + ausência de PIX), treino de modelo e identificação de associados com maior propensão a churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo de Pré-Churn\n",
    "print(\"🤖 MODELO PRÉ-CHURN - EXPERIMENTO INICIAL\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# Preparar base histórica mínima (últimos 4 registros por associado já carregados na query inicial)\n",
    "base_ml = df_base.copy()\n",
    "\n",
    "# Criar alvo churn_proxy: queda recente + inatividade + sem PIX\n",
    "# Condições: (queda_flag==1 no último mês) OU (soma_quedas>=2) E (dias_sem_mov>45) E (cad_pix_ativo==0)\n",
    "ultimo_mes = base_ml['ano_mes'].max()\n",
    "ult = base_ml[base_ml['ano_mes']==ultimo_mes].copy()\n",
    "\n",
    "# Garantir colunas\n",
    "for c in ['queda_flag','soma_quedas','dias_sem_mov','cad_pix_ativo']:\n",
    "    if c not in ult.columns:\n",
    "        ult[c] = 0\n",
    "\n",
    "ult['churn_proxy'] = np.where(\n",
    "    ( (ult['queda_flag']==1) | (ult['soma_quedas']>=2) ) &\n",
    "    (ult['dias_sem_mov']>45) &\n",
    "    (ult['cad_pix_ativo']==0),\n",
    "    1,0\n",
    ")\n",
    "\n",
    "print(f\"Taxa alvo churn proxy: {ult['churn_proxy'].mean()*100:.2f}%\")\n",
    "\n",
    "# Features selecionadas\n",
    "features = [c for c in [\n",
    "    'pontos_principalidade','cad_pix_ativo','transacao_app','possui_cartao_debito',\n",
    "    'possui_cartao_credito','possui_adquirencia','possui_cobranca','possui_domicilio',\n",
    "    'possui_folha_pagamento','ativou_open_finance','dias_sem_mov','cash_in','cash_out'\n",
    "] if c in ult.columns]\n",
    "\n",
    "df_model = ult[['cpf_cnpj','churn_proxy'] + features].dropna().copy()\n",
    "\n",
    "if df_model['churn_proxy'].nunique() >= 2 and len(df_model) > 100:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import roc_auc_score, classification_report\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    X = df_model[features]\n",
    "    y = df_model['churn_proxy']\n",
    "\n",
    "    # Balance simples se a taxa for muito baixa\n",
    "    if y.mean() < 0.05:\n",
    "        # Oversampling simples (duplicar positivos)\n",
    "        pos = df_model[df_model['churn_proxy']==1]\n",
    "        mult = int((len(df_model)-len(pos))/len(pos)*0.5)+1 if len(pos)>0 else 1\n",
    "        df_model_bal = pd.concat([df_model, pd.concat([pos]*mult)])\n",
    "        X = df_model_bal[features]\n",
    "        y = df_model_bal['churn_proxy']\n",
    "        print(f\"Aplicado oversampling simples. Nova taxa: {y.mean()*100:.2f}%\")\n",
    "\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced_subsample')\n",
    "    rf.fit(X_train,y_train)\n",
    "    probas = rf.predict_proba(X_test)[:,1]\n",
    "    preds = (probas>=0.5).astype(int)\n",
    "\n",
    "    auc = roc_auc_score(y_test, probas)\n",
    "    print(f\"ROC AUC: {auc:.3f}\")\n",
    "    print(\"\\nRelatório classificação:\")\n",
    "    print(classification_report(y_test,preds, digits=3))\n",
    "\n",
    "    # Importância features\n",
    "    imp = pd.DataFrame({'feature':features,'importance':rf.feature_importances_}).sort_values('importance', ascending=False)\n",
    "    print(\"\\nIMPORTÂNCIA DAS FEATURES:\")\n",
    "    for r in imp.itertuples():\n",
    "        print(f\"  {r.feature:<25} {r.importance:6.3f}\")\n",
    "\n",
    "    # Top risco (aplicar em todo dataset atual)\n",
    "    full_probas = rf.predict_proba(ult[features])[:,1]\n",
    "    ult['prob_churn'] = full_probas\n",
    "    top_risco = ult.nlargest(20,'prob_churn')[['cpf_cnpj','prob_churn','pontos_principalidade','dias_sem_mov']]\n",
    "    print(\"\\nTOP 20 ASSOCIADOS MAIOR RISCO (proxy):\")\n",
    "    for i,row in enumerate(top_risco.itertuples(),1):\n",
    "        print(f\"  {i:2d}. ...{str(row.cpf_cnpj)[-4:]} | Prob: {row.prob_churn:.2f} | Princ: {row.pontos_principalidade:.1f} | Dias SEM: {row.dias_sem_mov}\")\n",
    "\n",
    "    globals()['modelo_churn'] = rf\n",
    "    globals()['importancia_churn'] = imp\n",
    "    globals()['scoring_churn'] = ult[['cpf_cnpj','prob_churn','churn_proxy']]\n",
    "else:\n",
    "    print(\"⚠️ Base insuficiente ou alvo sem variabilidade para modelagem.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42a254",
   "metadata": {},
   "source": [
    "## 15. Geração de Relatório Final\n",
    "\n",
    "Conversão dos principais resumos, tabelas e métricas em um relatório executável (HTML/PDF) para apresentação. Inclui apêndice técnico e recomendações estratégicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de Relatório Final (HTML simplificado)\n",
    "print(\"📝 GERAÇÃO DE RELATÓRIO FINAL\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "resumos = {}\n",
    "for nome_var in [\n",
    "    'resumo_contas','resumo_bbm','resumo_evolucao','resumo_produtos','resumo_isa','resumo_pix',\n",
    "    'resumo_cash','resumo_prod_faltantes','resumo_sow','resumo_pj','resumo_movimento'\n",
    "]:\n",
    "    if nome_var in globals():\n",
    "        resumos[nome_var] = globals()[nome_var]\n",
    "\n",
    "html_parts = [\"<html><head><meta charset='utf-8'><title>Relatório Principalidade</title>\" \\\n",
    "              \"<style>body{font-family:Arial; margin:30px;} h2{border-bottom:1px solid #ccc;} table{border-collapse:collapse;} td,th{border:1px solid #ddd;padding:4px 8px;} .kpi{display:inline-block;margin:10px 20px;}</style></head><body>\"]\n",
    "html_parts.append(\"<h1>Relatório Executivo - Principalidade</h1>\")\n",
    "html_parts.append(f\"<p>Data geração: {datetime.now().strftime('%d/%m/%Y %H:%M')}</p>\")\n",
    "\n",
    "# KPIs principais (se existirem)\n",
    "if 'resumo_evolucao' in resumos and 'resumo_pix' in resumos:\n",
    "    kpis = [\n",
    "        (\"Associados Analisados\", resumos['resumo_evolucao'].get('total_analisados','-')),\n",
    "        (\"% Subiram\", f\"{resumos['resumo_evolucao'].get('subiu',0)/resumos['resumo_evolucao'].get('total_analisados',1)*100:.1f}%\"),\n",
    "        (\"PIX Penetração\", f\"{resumos['resumo_pix'].get('perc_penetracao',0):.1f}%\"),\n",
    "        (\"ISA Médio\", f\"{resumos.get('resumo_isa',{}).get('isa_medio',0):.2f}\"),\n",
    "        (\"SOW Médio\", f\"{resumos.get('resumo_sow',{}).get('sow_medio',0):.2f}\"),\n",
    "    ]\n",
    "    html_parts.append(\"<div>\")\n",
    "    for k,v in kpis:\n",
    "        html_parts.append(f\"<div class='kpi'><strong>{k}</strong><br>{v}</div>\")\n",
    "    html_parts.append(\"</div>\")\n",
    "\n",
    "# Tabelas de resumos\n",
    "for nome, dados in resumos.items():\n",
    "    html_parts.append(f\"<h2>{nome.replace('_',' ').title()}</h2>\")\n",
    "    html_parts.append(\"<table><tr><th>Métrica</th><th>Valor</th></tr>\")\n",
    "    for k,v in dados.items():\n",
    "        html_parts.append(f\"<tr><td>{k}</td><td>{v}</td></tr>\")\n",
    "    html_parts.append(\"</table>\")\n",
    "\n",
    "html_parts.append(\"<h2>Recomendações Estratégicas</h2>\")\n",
    "html_parts.append(\"<ul>\")\n",
    "html_parts.append(\"<li>Acelerar ativação PIX em faixas de alta principalidade e inatividade prolongada.</li>\")\n",
    "html_parts.append(\"<li>Priorizar cross-sell em contas com SOW baixo e principalidade elevada.</li>\")\n",
    "html_parts.append(\"<li>Atacar gaps de produtos transacionais básicos para elevar retenção.</li>\")\n",
    "html_parts.append(\"<li>Focar PJ sem domicílio/cobrança mas com relacionamento forte.</li>\")\n",
    "html_parts.append(\"<li>Acompanhar cohort de queda sucessiva (3 quedas) para prevenção churn.</li>\")\n",
    "html_parts.append(\"</ul>\")\n",
    "\n",
    "html_parts.append(\"<p><em>Relatório gerado automaticamente. Ajustes visuais podem ser aplicados em ferramenta de apresentação.</em></p>\")\n",
    "html_parts.append(\"</body></html>\")\n",
    "\n",
    "html_final = ''.join(html_parts)\n",
    "saida = Path('relatorio_principalidade.html')\n",
    "saida.write_text(html_final, encoding='utf-8')\n",
    "print(f\"Relatório salvo em: {saida.resolve()}\")\n",
    "\n",
    "globals()['relatorio_html'] = html_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validações finais e export opcional\n",
    "from IPython.display import display\n",
    "\n",
    "print('=== Validações Finais ===')\n",
    "if 'df_base' not in globals():\n",
    "    raise RuntimeError('df_base inexistente.')\n",
    "\n",
    "print(f\"Registros df_base: {len(df_base):,}\")\n",
    "print(f\"Colunas ({len(df_base.columns)}): {list(df_base.columns)[:25]} ...\")\n",
    "\n",
    "# 1. Checar duplicidade da chave principal (cpf_cnpj + ano_mes se existir)\n",
    "if 'cpf_cnpj' in df_base.columns and 'ano_mes' in df_base.columns:\n",
    "    dup = df_base.duplicated(subset=['cpf_cnpj','ano_mes']).sum()\n",
    "    print(f\"Duplicados cpf_cnpj+ano_mes: {dup}\")\n",
    "else:\n",
    "    print('Aviso: Não foi possível checar duplicidade (faltam colunas).')\n",
    "\n",
    "# 2. Percentual de tipo_pessoa preenchido\n",
    "if 'tipo_pessoa' in df_base.columns:\n",
    "    perc = df_base['tipo_pessoa'].notna().mean()*100\n",
    "    print(f\"tipo_pessoa preenchido: {perc:0.2f}%\")\n",
    "\n",
    "# 3. Amostra final\n",
    "print('\\nAmostra final:')\n",
    "cols_show = [c for c in ['cpf_cnpj','ano_mes','pontos_principalidade','tipo_pessoa','segmento'] if c in df_base.columns]\n",
    "print(df_base[cols_show].head())\n",
    "\n",
    "# 4. Export opcional\n",
    "EXPORTAR = False  # coloque True se quiser exportar\n",
    "CAMINHO_EXPORT = os.path.join(RUTAS['saida_dir'], 'principalidade_merge_bruto.parquet')\n",
    "if EXPORTAR:\n",
    "    df_base.to_parquet(CAMINHO_EXPORT, index=False)\n",
    "    print(f\"✓ Exportado para {CAMINHO_EXPORT}\")\n",
    "else:\n",
    "    print('Exportação desativada (EXPORTAR=False).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d0d95",
   "metadata": {},
   "source": [
    "# Conclusión y Validaciones Finales\n",
    "\n",
    "Se realizaron las cargas brutas, merges controlados y análisis inicial sin ninguna normalización automática de identificadores ni inferencia de tipo de persona.\n",
    "\n",
    "Próximos passos (opcionales):\n",
    "- Ajustar manualmente `cpf_cnpj` activando la celda opcional (solo si lo necesitas).\n",
    "- Agregar análisis adicionales en nuevas celdas sin modificar `df_base` original.\n",
    "- Exportar subconjuntos específicos si deseas trabajar en otro entorno.\n",
    "\n",
    "La siguiente celda ejecuta validaciones finales e opcionalmente guarda un parquet limpio. Ajusta rutas según necesidad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
